<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ai-and-Nlp on Blowfish</title><link>https://localhost:1313/categories/ai-and-nlp/</link><description>Recent content in Ai-and-Nlp on Blowfish</description><generator>Hugo</generator><language>en</language><managingEditor>nuno@n9o.xyz (Blowfish)</managingEditor><webMaster>nuno@n9o.xyz (Blowfish)</webMaster><lastBuildDate>Mon, 21 Apr 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://localhost:1313/categories/ai-and-nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>BitNet b1.58-2B4T: Revolutionary Binary Neural Network for Efficient AI</title><link>https://localhost:1313/dsblog/BitNet-b1-58-2B4T-for-efficient-ai-processing/</link><pubDate>Mon, 21 Apr 2025 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/BitNet-b1-58-2B4T-for-efficient-ai-processing/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6263-BitNet-b1.58-2B4T.jpg" alt="BitNet b1.58-2B4T">&lt;/p>
&lt;p>&lt;a href="https://arxiv.org/pdf/2504.12285">Archive Paper Link&lt;/a>&lt;/p>
&lt;h1 id="bitnet-b158-2b4t-the-future-of-efficient-ai-processing">BitNet b1.58-2B4T: The Future of Efficient AI Processing&lt;a class="td-heading-self-link" href="#bitnet-b158-2b4t-the-future-of-efficient-ai-processing" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="a-history-of-1-bit-transformer-model">A History of 1 bit Transformer Model&lt;a class="td-heading-self-link" href="#a-history-of-1-bit-transformer-model" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>A paper &amp;ldquo;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&amp;rdquo; was published by Stanford University, ETH ZÃ¼rich, and EPFL. It was published on October 2023 (published on arXiv on October 17, 2023). &lt;a href="https://arxiv.org/pdf/2310.11453">Standord Paper Link&lt;/a>. The core Concept of 1.58 bits per parameter, was introduced here. This demonstrated that LLMs could be effectively trained and operated with extremely low-bit representation while maintaining competitive performance&lt;/p></description></item><item><title>Retrieval-Augmented Generation with Conflicting Evidence</title><link>https://localhost:1313/dsblog/ps-Retrieval-Augmented-Generation-with-Conflicting-Evidence/</link><pubDate>Thu, 17 Apr 2025 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/ps-Retrieval-Augmented-Generation-with-Conflicting-Evidence/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6261-Retrieval-Augmented-Generation-with-Conflicting-Evidence.jpg" alt="">&lt;/p>
&lt;h1 id="paper-summary-retrieval-augmented-generation-with-conflicting-evidence">Paper Summary: Retrieval-Augmented Generation with Conflicting Evidence&lt;a class="td-heading-self-link" href="#paper-summary-retrieval-augmented-generation-with-conflicting-evidence" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>&lt;a href="https://arxiv.org/pdf/2504.13079">arXiv Paper&lt;/a>&lt;/p>
&lt;p>The hypothesis of this paper is that &lt;strong>real-world retrieval-augmented generation (RAG) systems must simultaneously handle various sources of conflicting information, including ambiguity in user queries and contradictory information arising from misinformation and noise in retrieved documents&lt;/strong>. The authors argue that prior work has largely addressed these challenges in isolation.&lt;/p></description></item><item><title>LLM Internal Encoding of Truthfulness and Hallucinations</title><link>https://localhost:1313/dsblog/ps-LLM-Internal-Encoding-of-Truthfulness-and-Hallucinations/</link><pubDate>Tue, 15 Apr 2025 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/ps-LLM-Internal-Encoding-of-Truthfulness-and-Hallucinations/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6260-LLM-Internal-Encoding-of-Truthfulness-and-Hallucinations.jpg" alt="LLM Internal Encoding of Truthfulness and Hallucinations">&lt;/p>
&lt;h1 id="paper-summary-llm-internal-encoding-of-truthfulness-and-hallucinations">Paper Summary: LLM Internal Encoding of Truthfulness and Hallucinations&lt;a class="td-heading-self-link" href="#paper-summary-llm-internal-encoding-of-truthfulness-and-hallucinations" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>The objective of this paper is to gain a deeper understanding of errors produced by large language models (LLMs) by examining their internal representations. The authors aim to reveal how information about the truthfulness of LLM outputs is encoded internally, going beyond extrinsic, behavioral analysis. They also seek to investigate the relationship between these internal representations and the external behavior of LLMs, including their tendency to produce inaccuracies or &amp;ldquo;hallucinations&amp;rdquo;. Furthermore, the paper intends to explore whether internal representations can be used to predict the types of errors LLMs make and to detect the correct answer even when the model generates an incorrect one.&lt;/p></description></item></channel></rss>