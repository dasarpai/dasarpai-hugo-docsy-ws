<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.147.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href="https://localhost:1313/favicons/favicon.ico?v=1"><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-16x16.png?v=1" sizes=16x16><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-32x32.png?v=1" sizes=32x32><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><title>How Much Memory Needed for LLM | Blowfish</title><meta property="og:url" content="https://localhost:1313/dsblog/How-Much-Memory-Needed-for-LLM/">
<meta property="og:site_name" content="Blowfish"><meta property="og:title" content="How Much Memory Needed for LLM"><meta property="og:description" content="How Much Memory Needed for LLM? What is LLM? LLM stands for Large Language Model. These are machine learning models that are trained on massive amounts of text data to understand, generate, and work with human language in a way that mimics natural language understanding. They are called “large” because of the significant number of parameters they contain, often numbering in the billions or even trillions."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="dsblog"><meta property="article:published_time" content="2024-08-05T00:00:00+00:00"><meta property="article:modified_time" content="2024-08-05T00:00:00+00:00"><meta property="article:tag" content="LLM Infrastructure"><meta property="article:tag" content="Hardware Requirements"><meta property="article:tag" content="GPU Computing"><meta property="article:tag" content="Memory Management"><meta property="article:tag" content="AI Infrastructure"><meta property="article:tag" content="Deep Learning"><meta itemprop=name content="How Much Memory Needed for LLM"><meta itemprop=description content="How Much Memory Needed for LLM? What is LLM? LLM stands for Large Language Model. These are machine learning models that are trained on massive amounts of text data to understand, generate, and work with human language in a way that mimics natural language understanding. They are called “large” because of the significant number of parameters they contain, often numbering in the billions or even trillions."><meta itemprop=datePublished content="2024-08-05T00:00:00+00:00"><meta itemprop=dateModified content="2024-08-05T00:00:00+00:00"><meta itemprop=wordCount content="2851"><meta itemprop=keywords content="LLM Memory Requirements,GPU Memory,Hardware Specifications,AI Computing Infrastructure,Deep Learning Hardware,Memory Management,Computing Resources,Model Deployment"><meta name=twitter:card content="summary"><meta name=twitter:title content="How Much Memory Needed for LLM"><meta name=twitter:description content="How Much Memory Needed for LLM? What is LLM? LLM stands for Large Language Model. These are machine learning models that are trained on massive amounts of text data to understand, generate, and work with human language in a way that mimics natural language understanding. They are called “large” because of the significant number of parameters they contain, often numbering in the billions or even trillions."><link rel=preload href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css as=style integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css rel=stylesheet integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link rel=stylesheet type=text/css href=https://localhost:1313/css/asciinema-player.css><script src=https://code.jquery.com/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><link rel=stylesheet href=https://localhost:1313/css/custom.css><script src=https://localhost:1313/js/lunr.js></script></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-light nav-shadow flex-column flex-md-row td-navbar"><a id=agones-top class=navbar-brand href=https://localhost:1313/><svg id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg> <span class="text-uppercase fw-bold">Blowfish</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=https://localhost:1313/><span class=active>Data Science</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Docs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/community/><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/shortcodes/><span>Short Codes</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/blog/><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://github.com/nunocoracao/blowfish><span></span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href>GitHub</a></li><li class="nav-item dropdown d-none d-lg-block"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Release</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://development.agones.dev>Development</a>
<a class=dropdown-item href=https://agones.dev></a><a class=dropdown-item href=https://1-47-0.agones.dev>1.47.0</a>
<a class=dropdown-item href=https://1-46-0.agones.dev>1.46.0</a>
<a class=dropdown-item href=https://1-45-0.agones.dev>1.45.0</a>
<a class=dropdown-item href=https://1-44-0.agones.dev>1.44.0</a>
<a class=dropdown-item href=https://1-43-0.agones.dev>1.43.0</a>
<a class=dropdown-item href=https://1-42-0.agones.dev>1.42.0</a>
<a class=dropdown-item href=https://1-41-0.agones.dev>1.41.0</a>
<a class=dropdown-item href=https://1-40-0.agones.dev>1.40.0</a>
<a class=dropdown-item href=https://1-39-0.agones.dev>1.39.0</a>
<a class=dropdown-item href=https://1-38-0.agones.dev>1.38.0</a>
<a class=dropdown-item href=https://1-37-0.agones.dev>1.37.0</a>
<a class=dropdown-item href=https://1-36-0.agones.dev>1.36.0</a>
<a class=dropdown-item href=https://1-35-0.agones.dev>1.35.0</a>
<a class=dropdown-item href=https://1-34-0.agones.dev>1.34.0</a>
<a class=dropdown-item href=https://1-33-0.agones.dev>1.33.0</a>
<a class=dropdown-item href=https://1-32-0.agones.dev>1.32.0</a>
<a class=dropdown-item href=https://1-31-0.agones.dev>1.31.0</a></div></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><ul class=dropdown-menu><li><a class=dropdown-item href=https://localhost:1313/it/>Italiano</a></li></ul></div></li></ul></div><div class="navbar-nav mx-lg-2 d-none d-lg-block"></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><div class=row><div class=col-md-2></div><div class=col-md-8><p><img src=https://localhost:1313/assets/images/dspost/dsp6133-How-Much-Memory-Needed-for-LLM.jpg alt=How-Much-Memory-Needed-for-LLM></p><h1 id=how-much-memory-needed-for-llm>How Much Memory Needed for LLM?<a class=td-heading-self-link href=#how-much-memory-needed-for-llm aria-label="Heading self-link"></a></h1><h2 id=what-is-llm>What is LLM?<a class=td-heading-self-link href=#what-is-llm aria-label="Heading self-link"></a></h2><p>LLM stands for <strong>Large Language Model</strong>. These are machine learning models that are trained on massive amounts of text data to understand, generate, and work with human language in a way that mimics natural language understanding. They are called &ldquo;large&rdquo; because of the significant number of parameters they contain, often numbering in the billions or even trillions.</p><h3 id=what-defines-a-large-language-model>What Defines a Large Language Model?<a class=td-heading-self-link href=#what-defines-a-large-language-model aria-label="Heading self-link"></a></h3><p>There is no strict or universally accepted benchmark to define what constitutes an LLM purely based on the number of parameters. The term &ldquo;large&rdquo; is relative and depends on the current state of technology and the size of models being developed. As technology progresses, what is considered &ldquo;large&rdquo; may continue to grow. However, some general guidelines have emerged:</p><ol><li><p><strong>Scale of Parameters</strong>:</p><ul><li>Models with hundreds of millions to billions of parameters are typically referred to as large language models.</li><li>For instance, OpenAI&rsquo;s GPT-2 has 1.5 billion parameters, and GPT-3 has 175 billion parameters. Models like these are typically categorized as LLMs.</li></ul></li><li><p><strong>Training Data</strong>:</p><ul><li>LLMs are trained on vast and diverse datasets, often encompassing hundreds of gigabytes or even terabytes of text data. This data can include books, websites, articles, and more, which helps the model learn a broad range of language patterns and knowledge.</li></ul></li><li><p><strong>Capabilities</strong>:</p><ul><li>LLMs are distinguished by their ability to perform a wide variety of tasks without task-specific training. They can generate coherent and contextually relevant text, translate languages, summarize documents, answer questions, and more, simply based on the data they were trained on.</li></ul></li></ol><h2 id=how-much-memory-is-required-to-host-a-llama-31-with-405-billion-parameters>How much memory is required to host a Llama 3.1 with 405 billion parameters?<a class=td-heading-self-link href=#how-much-memory-is-required-to-host-a-llama-31-with-405-billion-parameters aria-label="Heading self-link"></a></h2><p>Training or deploying large language models (LLMs) is a big challenge. The first question that comes to mind is how much memory is required to develop or use these models. How much memory is enough? Training and deploying these models is so expensive that small or mid-sized companies cannot afford to commit resources for the long term unless the value gained exceeds the costs incurred in developing, running, and maintaining these models. This article is not just about memory but overall what kind of hardware we may need to work with LLMs.</p><p>Deploying a large-scale model like Llama 3.1 with 405 billion parameters for inference purposes requires substantial hardware resources. Here are the main considerations:</p><h3 id=gpu-requirements>GPU Requirements<a class=td-heading-self-link href=#gpu-requirements aria-label="Heading self-link"></a></h3><ol><li><p><strong>Memory (VRAM)</strong>:</p><ul><li>Large models require significant GPU memory. A model with 405 billion parameters will need several high-memory GPUs. The exact amount of VRAM required will depend on the specific architecture and the optimizations available.</li><li>As a rough estimate, you might need multiple GPUs with at least 80 GB of VRAM each (like NVIDIA A100 80GB).</li></ul></li><li><p><strong>Compute Power</strong>:</p><ul><li>High-end GPUs like NVIDIA A100, NVIDIA V100, or the newer H100 are recommended due to their high compute power and memory bandwidth.</li></ul></li><li><p><strong>Multi-GPU Setup</strong>:</p><ul><li>Given the size of the model, you will likely need a multi-GPU setup. This could be achieved using NVIDIA&rsquo;s NVLink or NVSwitch to enable efficient communication between GPUs.</li><li>Distributed inference frameworks like NVIDIA Triton Inference Server, Hugging Face Accelerate, or DeepSpeed can help manage multi-GPU setups effectively.</li></ul></li></ol><h3 id=cpu-and-ram-requirements>CPU and RAM Requirements<a class=td-heading-self-link href=#cpu-and-ram-requirements aria-label="Heading self-link"></a></h3><ol><li><p><strong>CPU</strong>:</p><ul><li>A powerful multi-core CPU is recommended to handle data preprocessing, postprocessing, and feeding data to GPUs. CPUs with many cores and high clock speeds will be beneficial.</li><li>Examples: AMD EPYC or Intel Xeon processors with at least 32 cores.</li></ul></li><li><p><strong>RAM</strong>:</p><ul><li>Adequate system memory (RAM) is needed to support data handling and GPU operations. A system with at least 512 GB of RAM is recommended, but more might be necessary depending on the exact workload.</li></ul></li></ol><h3 id=storage>Storage<a class=td-heading-self-link href=#storage aria-label="Heading self-link"></a></h3><ol><li><strong>High-Speed Storage</strong>:<ul><li>NVMe SSDs are recommended for fast read/write operations, especially when dealing with large datasets.</li><li>Having several terabytes of storage is advisable to store model checkpoints, datasets, and intermediate outputs.</li></ul></li></ol><h3 id=networking>Networking<a class=td-heading-self-link href=#networking aria-label="Heading self-link"></a></h3><ol><li><strong>High-Speed Networking</strong>:<ul><li>If you are using a multi-node setup, high-speed networking (e.g., InfiniBand) is crucial for efficient data transfer between nodes.</li><li>Low-latency networking ensures that the GPUs can communicate quickly, reducing bottlenecks during inference.</li></ul></li></ol><h3 id=power-and-cooling>Power and Cooling<a class=td-heading-self-link href=#power-and-cooling aria-label="Heading self-link"></a></h3><ol><li><p><strong>Power Supply</strong>:</p><ul><li>Ensure that the power supply can handle the load from multiple high-end GPUs and CPUs.</li><li>Redundant power supplies are recommended for stability and reliability.</li></ul></li><li><p><strong>Cooling</strong>:</p><ul><li>Effective cooling solutions are necessary to maintain the optimal temperature of GPUs and CPUs.</li><li>Data center-grade cooling systems or liquid cooling solutions may be required.</li></ul></li></ol><h3 id=example-hardware-setup>Example Hardware Setup<a class=td-heading-self-link href=#example-hardware-setup aria-label="Heading self-link"></a></h3><p>Here is an example of a possible hardware setup:</p><ol><li><strong>GPUs</strong>: 8x NVIDIA A100 80GB</li><li><strong>CPU</strong>: Dual AMD EPYC 7742 (64 cores per CPU, 128 threads total)</li><li><strong>RAM</strong>: 1 TB DDR4</li><li><strong>Storage</strong>: 4x 2 TB NVMe SSDs</li><li><strong>Networking</strong>: InfiniBand networking with 100 Gbps bandwidth</li><li><strong>Cooling</strong>: Advanced liquid cooling system</li><li><strong>Power</strong>: Redundant power supplies with at least 3 kW capacity</li></ol><h3 id=cloud-solutions>Cloud Solutions<a class=td-heading-self-link href=#cloud-solutions aria-label="Heading self-link"></a></h3><p>If investing in physical hardware is not feasible, cloud service providers like AWS, Google Cloud, and Azure offer instances specifically designed for machine learning workloads. For instance:</p><ul><li><strong>AWS</strong>: p4d.24xlarge (8x NVIDIA A100 40GB) or p4de.24xlarge (8x NVIDIA A100 80GB)</li><li><strong>Google Cloud</strong>: A2 MegaMachine (up to 16x NVIDIA A100 40GB)</li><li><strong>Azure</strong>: NDm A100 v4 series (up to 8x NVIDIA A100 40GB)</li></ul><p>Using cloud instances can provide the necessary hardware resources on a pay-as-you-go basis, allowing for scalability and flexibility.</p><h3 id=optimizations>Optimizations<a class=td-heading-self-link href=#optimizations aria-label="Heading self-link"></a></h3><p>To make the most out of your hardware, consider the following optimizations:</p><ul><li><strong>Mixed Precision Inference</strong>: Use mixed precision (FP16) to reduce memory usage and improve performance.</li><li><strong>Model Parallelism</strong>: Split the model across multiple GPUs using libraries like Megatron-LM or Hugging Face Accelerate.</li><li><strong>Efficient Data Loading</strong>: Use efficient data loading and preprocessing techniques to keep the GPUs fed with data without bottlenecks.</li></ul><p>By carefully selecting and configuring your hardware and optimizing your inference pipeline, you can effectively deploy a large-scale model like Llama 3.1 405B for inference purposes.</p><h2 id=how-much-memory-is-required-to-host-a-llama-31-with-70-billion-parameters>How much memory is required to host a Llama 3.1 with 70 billion parameters?<a class=td-heading-self-link href=#how-much-memory-is-required-to-host-a-llama-31-with-70-billion-parameters aria-label="Heading self-link"></a></h2><p>Deploying the Llama 3.1 70B model for inference is more feasible compared to the 405B model but still requires substantial hardware resources. Here&rsquo;s a detailed breakdown of the hardware needed:</p><h3 id=gpu-requirements-1>GPU Requirements<a class=td-heading-self-link href=#gpu-requirements-1 aria-label="Heading self-link"></a></h3><ol><li><p><strong>Memory (VRAM)</strong>:</p><ul><li>The Llama 3.1 70B model requires significant GPU memory. You will need GPUs with large VRAM capacities. Typically, GPUs with 40-80 GB of VRAM are suitable.</li><li>For efficient inference, using GPUs like the NVIDIA A100 40GB or 80GB is recommended.</li></ul></li><li><p><strong>Compute Power</strong>:</p><ul><li>High-end GPUs are recommended due to their compute capabilities. NVIDIA A100 or newer H100 GPUs are well-suited for this task.</li></ul></li><li><p><strong>Multi-GPU Setup</strong>:</p><ul><li>Depending on the specific implementation and optimizations, you might need multiple GPUs. Using frameworks that support model parallelism (e.g., NVIDIA Triton Inference Server, Hugging Face Accelerate, or DeepSpeed) can help manage this setup efficiently.</li></ul></li></ol><h3 id=cpu-and-ram-requirements-1>CPU and RAM Requirements<a class=td-heading-self-link href=#cpu-and-ram-requirements-1 aria-label="Heading self-link"></a></h3><ol><li><p><strong>CPU</strong>:</p><ul><li>A powerful multi-core CPU is recommended to handle data preprocessing, postprocessing, and managing data transfer to GPUs. AMD EPYC or Intel Xeon processors with at least 16-32 cores are ideal.</li></ul></li><li><p><strong>RAM</strong>:</p><ul><li>Adequate system memory (RAM) is needed to support data handling and GPU operations. A system with at least 256 GB of RAM is recommended, though 512 GB or more would be better for more demanding workloads.</li></ul></li></ol><h3 id=storage-1>Storage<a class=td-heading-self-link href=#storage-1 aria-label="Heading self-link"></a></h3><ol><li><strong>High-Speed Storage</strong>:<ul><li>NVMe SSDs are recommended for fast read/write operations, especially when dealing with large datasets. Several terabytes of storage may be needed to store model checkpoints, datasets, and intermediate outputs.</li></ul></li></ol><h3 id=networking-1>Networking<a class=td-heading-self-link href=#networking-1 aria-label="Heading self-link"></a></h3><ol><li><strong>High-Speed Networking</strong>:<ul><li>For multi-node setups, high-speed networking (e.g., InfiniBand) is crucial for efficient data transfer between nodes. This ensures low latency and high bandwidth.</li></ul></li></ol><h3 id=power-and-cooling-1>Power and Cooling<a class=td-heading-self-link href=#power-and-cooling-1 aria-label="Heading self-link"></a></h3><ol><li><p><strong>Power Supply</strong>:</p><ul><li>Ensure that the power supply can handle the load from multiple high-end GPUs and CPUs. Redundant power supplies are recommended for stability and reliability.</li></ul></li><li><p><strong>Cooling</strong>:</p><ul><li>Effective cooling solutions are necessary to maintain optimal temperatures for GPUs and CPUs. Data center-grade cooling systems or advanced liquid cooling solutions may be required.</li></ul></li></ol><h3 id=example-hardware-setup-1>Example Hardware Setup<a class=td-heading-self-link href=#example-hardware-setup-1 aria-label="Heading self-link"></a></h3><p>Here is an example of a possible hardware setup for deploying the Llama 3.1 70B model:</p><ol><li><strong>GPUs</strong>: 4x NVIDIA A100 80GB</li><li><strong>CPU</strong>: Dual AMD EPYC 7742 (64 cores per CPU, 128 threads total)</li><li><strong>RAM</strong>: 512 GB DDR4</li><li><strong>Storage</strong>: 4x 2 TB NVMe SSDs</li><li><strong>Networking</strong>: InfiniBand networking with 100 Gbps bandwidth</li><li><strong>Cooling</strong>: Advanced liquid cooling system</li><li><strong>Power</strong>: Redundant power supplies with at least 3 kW capacity</li></ol><h3 id=cloud-solutions-1>Cloud Solutions<a class=td-heading-self-link href=#cloud-solutions-1 aria-label="Heading self-link"></a></h3><p>If investing in physical hardware is not feasible, consider using cloud service providers like AWS, Google Cloud, or Azure, which offer instances specifically designed for machine learning workloads. For instance:</p><ul><li><strong>AWS</strong>: p4d.24xlarge (8x NVIDIA A100 40GB) or p4de.24xlarge (8x NVIDIA A100 80GB)</li><li><strong>Google Cloud</strong>: A2 MegaMachine (up to 16x NVIDIA A100 40GB)</li><li><strong>Azure</strong>: NDm A100 v4 series (up to 8x NVIDIA A100 40GB)</li></ul><p>Using cloud instances provides the necessary hardware resources on a pay-as-you-go basis, allowing for scalability and flexibility.</p><h2 id=how-to-calculate-memory-requirement-of-llm-based-on-parameter-information>How to calculate Memory requirement of LLM based on parameter information?<a class=td-heading-self-link href=#how-to-calculate-memory-requirement-of-llm-based-on-parameter-information aria-label="Heading self-link"></a></h2><p>There is a rough calculation for estimating the memory needed based on the model size. The memory required for a language model primarily depends on the number of parameters, the precision used for storing those parameters, and additional memory needed for activations and intermediate computations during training or inference.</p><h3 id=estimating-memory-for-model-parameters-during-training>Estimating Memory for Model Parameters (during training)<a class=td-heading-self-link href=#estimating-memory-for-model-parameters-during-training aria-label="Heading self-link"></a></h3><ol><li><p><strong>Parameters</strong>:</p><ul><li>Each parameter in the model typically requires storage. For floating-point precision:<ul><li><strong>32-bit float (FP32)</strong>: 4 bytes per parameter</li><li><strong>16-bit float (FP16)</strong>: 2 bytes per parameter</li><li><strong>8-bit integer (INT8)</strong>: 1 byte per parameter (usually used for quantized models)</li></ul></li></ul></li><li><p><strong>Calculation</strong>:</p><ul><li><strong>Memory for parameters</strong> = Number of parameters × Size per parameter</li></ul></li></ol><h4 id=example-calculation>Example Calculation<a class=td-heading-self-link href=#example-calculation aria-label="Heading self-link"></a></h4><p>For a model with 70 billion parameters using FP32 precision:</p><ol><li><strong>Parameters</strong>: 70 billion</li><li><strong>Size per parameter (FP32)</strong>: 4 bytes</li></ol><p>$$ \text{Memory for parameters} = 70 \times 10^9 \text{ parameters} \times 4 \text{ bytes/parameter}$$
$$ \text{Memory for parameters} = 280 \text{ GB}$$</p><p>For FP16 precision:</p><ol><li><strong>Parameters</strong>: 70 billion</li><li><strong>Size per parameter (FP16)</strong>: 2 bytes</li></ol><p>$$ \text{Memory for parameters} = 70 \times 10^9 \text{ parameters} \times 2 \text{ bytes/parameter}$$
$$ \text{Memory for parameters} = 140 \text{ GB}$$</p><h4 id=additional-memory-requirements>Additional Memory Requirements<a class=td-heading-self-link href=#additional-memory-requirements aria-label="Heading self-link"></a></h4><ol><li><p><strong>Activations</strong>: During inference and especially during training, additional memory is needed for activations, which can be substantial. The memory required for activations depends on the batch size, sequence length, and model architecture.</p></li><li><p><strong>Gradient Storage (Training)</strong>: During training, you also need memory for gradients, which is roughly the same as the memory for the parameters, but this is not needed for inference.</p></li><li><p><strong>Optimizer States (Training)</strong>: Optimizers (like Adam) maintain additional states (momentum, variance) that require additional memory, often about the same as the model parameters.</p></li></ol><h3 id=memory-required-for-inference>Memory Required for Inference<a class=td-heading-self-link href=#memory-required-for-inference aria-label="Heading self-link"></a></h3><p>For inference, you can often reduce the memory footprint by:</p><ul><li><strong>Using FP16 precision</strong>: Reduces the memory requirement by half compared to FP32.</li><li><strong>Activations</strong>: Memory for activations depends on the model&rsquo;s architecture and sequence length.</li></ul><h4 id=example-memory-calculation-for-inference>Example Memory Calculation for Inference<a class=td-heading-self-link href=#example-memory-calculation-for-inference aria-label="Heading self-link"></a></h4><p>If using a model with 70 billion parameters and FP16 precision:</p><ol><li><strong>Memory for parameters</strong>: 140 GB</li><li><strong>Estimated activation memory</strong>: This can vary but might be around 10-30 GB depending on batch size and sequence length.</li></ol><h4 id=total-memory-estimate-for-inference>Total Memory Estimate for Inference<a class=td-heading-self-link href=#total-memory-estimate-for-inference aria-label="Heading self-link"></a></h4><ul><li><strong>Total Memory Estimate</strong>: Memory for parameters + Memory for activations</li><li>For a rough estimate: 140 GB (parameters) + 10-30 GB (activations) = <strong>150-170 GB</strong></li></ul><p>This is a rough estimate and can vary based on implementation specifics, additional overheads, and optimizations used. For accurate hardware requirements, you should consider using profiling tools specific to your deployment environment.</p><h2 id=how-to-calculate-memory-requirement-for-the-activation-function>How to calculate memory requirement for the Activation Function?<a class=td-heading-self-link href=#how-to-calculate-memory-requirement-for-the-activation-function aria-label="Heading self-link"></a></h2><p>Estimating the memory required for activation functions in a large language model like a 70 billion parameter model involves several factors, including the model architecture, batch size, sequence length, and the specific operations performed by the activation functions. Here’s a general approach to estimate the memory required:</p><h3 id=factors-affecting-activation-memory>Factors Affecting Activation Memory<a class=td-heading-self-link href=#factors-affecting-activation-memory aria-label="Heading self-link"></a></h3><ol><li><p><strong>Model Architecture</strong>:</p><ul><li>The architecture (e.g., Transformer) affects the number of intermediate activations. For instance, the attention mechanisms and feed-forward layers generate activations that need to be stored.</li></ul></li><li><p><strong>Batch Size</strong>:</p><ul><li>The batch size impacts the number of activations stored per forward pass. Larger batch sizes will require more memory.</li></ul></li><li><p><strong>Sequence Length</strong>:</p><ul><li>For sequence-based models (like Transformers), the sequence length affects the size of activations, especially in attention layers.</li></ul></li><li><p><strong>Number of Layers</strong>:</p><ul><li>More layers mean more intermediate activations need to be stored.</li></ul></li></ol><h3 id=general-calculation>General Calculation<a class=td-heading-self-link href=#general-calculation aria-label="Heading self-link"></a></h3><p>Let&rsquo;s outline a rough calculation for a Transformer-based model:</p><ol><li><p><strong>Layer Outputs</strong>:</p><ul><li>Each layer in a Transformer typically has an activation tensor with the shape <code>(batch_size, sequence_length, hidden_size)</code>.</li></ul></li><li><p><strong>Memory per Activation Tensor</strong>:</p><ul><li><strong>Memory per tensor</strong> = <code>batch_size × sequence_length × hidden_size × size_per_element</code></li><li>For FP32 (4 bytes per element) or FP16 (2 bytes per element).</li></ul></li><li><p><strong>Total Activations</strong>:</p><ul><li>The total memory for activations is the sum of activations across all layers.</li></ul></li></ol><h3 id=example-calculation-1>Example Calculation<a class=td-heading-self-link href=#example-calculation-1 aria-label="Heading self-link"></a></h3><p>Let’s assume a Transformer model with the following characteristics:</p><ul><li><strong>Hidden Size</strong>: 16,384 (for large models)</li><li><strong>Batch Size</strong>: 1</li><li><strong>Sequence Length</strong>: 2,048</li><li><strong>Number of Layers</strong>: 96 (typical for very large models)</li></ul><h4 id=memory-per-activation-tensor>Memory per Activation Tensor<a class=td-heading-self-link href=#memory-per-activation-tensor aria-label="Heading self-link"></a></h4><p>For FP32:</p><ul><li><strong>Memory per tensor</strong> = <code>1 × 2048 × 16384 × 4 bytes</code></li><li><strong>Memory per tensor</strong> = <code>134,217,728 bytes</code> ≈ 134.2 MB</li></ul><p>For FP16:</p><ul><li><strong>Memory per tensor</strong> = <code>1 × 2048 × 16384 × 2 bytes</code></li><li><strong>Memory per tensor</strong> = <code>67,108,864 bytes</code> ≈ 67.1 MB</li></ul><h4 id=total-activation-memory>Total Activation Memory<a class=td-heading-self-link href=#total-activation-memory aria-label="Heading self-link"></a></h4><p>Each Transformer layer typically generates activations for:</p><ul><li><strong>Self-attention</strong>: Depends on attention heads and sequence length.</li><li><strong>Feed-forward network</strong>: Additional activations.</li></ul><p>For simplicity, assume 2 main activation tensors per layer (one for attention and one for the feed-forward network):</p><ul><li><strong>Total memory for activations per layer</strong> (FP32) = <code>2 × 134.2 MB</code> = <code>268.4 MB</code></li><li><strong>Total memory for activations per layer</strong> (FP16) = <code>2 × 67.1 MB</code> = <code>134.2 MB</code></li></ul><p>For 96 layers:</p><ul><li><strong>Total activation memory (FP32)</strong> = <code>96 × 268.4 MB</code> ≈ <code>25.8 GB</code></li><li><strong>Total activation memory (FP16)</strong> = <code>96 × 134.2 MB</code> ≈ <code>12.9 GB</code></li></ul><h3 id=additional-considerations>Additional Considerations<a class=td-heading-self-link href=#additional-considerations aria-label="Heading self-link"></a></h3><ol><li><p><strong>Intermediate Activations</strong>: Other intermediate activations and temporary tensors during operations will add to the total memory requirement.</p></li><li><p><strong>Activation Checkpoints</strong>: Techniques like activation checkpointing can help reduce memory usage by storing only a subset of activations and recomputing others as needed.</p></li><li><p><strong>Optimizations</strong>: Advanced optimizations and frameworks can significantly impact actual memory usage.</p></li></ol><h3 id=summary>Summary<a class=td-heading-self-link href=#summary aria-label="Heading self-link"></a></h3><p>For a 70 billion parameter model, the activation memory required can range approximately between <strong>12.9 GB</strong> (using FP16 precision) and <strong>25.8 GB</strong> (using FP32 precision), depending on the specific model architecture and sequence length. This is a rough estimate and actual memory usage can vary based on implementation details and optimizations.</p><h2 id=popular-high-performance-large-language-models-llms-available-as-open-source-projects>Popular high-performance large language models (LLMs) available as open-source projects.<a class=td-heading-self-link href=#popular-high-performance-large-language-models-llms-available-as-open-source-projects aria-label="Heading self-link"></a></h2><h3 id=1-llama-large-language-model-meta-ai>1. <strong>LLaMA (Large Language Model Meta AI)</strong><a class=td-heading-self-link href=#1-llama-large-language-model-meta-ai aria-label="Heading self-link"></a></h3><ul><li><strong>Developed by</strong>: Meta AI (Facebook)</li><li><strong>Description</strong>: LLaMA is designed to provide efficient and scalable solutions for large language models.</li><li><strong>Key Features</strong>: High performance on various benchmarks, open-source availability, and multiple parameter sizes.</li><li><strong>Repository</strong>: <a href=https://github.com/facebookresearch/llama>GitHub</a></li></ul><h3 id=2-gpt-j>2. <strong>GPT-J</strong><a class=td-heading-self-link href=#2-gpt-j aria-label="Heading self-link"></a></h3><ul><li><strong>Developed by</strong>: EleutherAI</li><li><strong>Description</strong>: GPT-J is a 6 billion parameter model trained by EleutherAI. It aims to provide a large-scale, open-source alternative to proprietary models like GPT-3.</li><li><strong>Key Features</strong>: High performance on various NLP tasks, easy to deploy, and community-supported.</li><li><strong>Repository</strong>: <a href=https://github.com/kingoflolz/mesh-transformer-jax>GitHub</a></li></ul><h3 id=3-bloom>3. <strong>BLOOM</strong><a class=td-heading-self-link href=#3-bloom aria-label="Heading self-link"></a></h3><ul><li><strong>Developed by</strong>: BigScience</li><li><strong>Description</strong>: BLOOM is an open-source language model created through a large-scale collaboration involving over a thousand researchers. It aims to democratize access to large-scale language models.</li><li><strong>Key Features</strong>: Multilingual capabilities, various model sizes, and designed with ethical considerations in mind.</li><li><strong>Repository</strong>: <a href=https://huggingface.co/bigscience/bloom>Hugging Face</a></li></ul><h3 id=4-opt-open-pretrained-transformer>4. <strong>OPT (Open Pretrained Transformer)</strong><a class=td-heading-self-link href=#4-opt-open-pretrained-transformer aria-label="Heading self-link"></a></h3><ul><li><strong>Developed by</strong>: Meta AI (Facebook)</li><li><strong>Description</strong>: OPT is an open-source series of language models designed to provide transparency and reproducibility in training large-scale models.</li><li><strong>Key Features</strong>: Models up to 175 billion parameters, transparent documentation of training processes, and available checkpoints.</li><li><strong>Repository</strong>: <a href=https://github.com/facebookresearch/metaseq>GitHub</a></li></ul><h3 id=5-t5-text-to-text-transfer-transformer>5. <strong>T5 (Text-to-Text Transfer Transformer)</strong><a class=td-heading-self-link href=#5-t5-text-to-text-transfer-transformer aria-label="Heading self-link"></a></h3><ul><li><strong>Developed by</strong>: Google Research</li><li><strong>Description</strong>: T5 is a flexible and powerful model that frames all NLP tasks as text-to-text problems, allowing for a unified approach to a wide range of tasks.</li><li><strong>Key Features</strong>: Versatile across multiple NLP tasks, pre-trained on a diverse dataset, and various model sizes.</li><li><strong>Repository</strong>: <a href=https://github.com/google-research/text-to-text-transfer-transformer>GitHub</a></li></ul><h3 id=6-roberta>6. <strong>RoBERTa</strong><a class=td-heading-self-link href=#6-roberta aria-label="Heading self-link"></a></h3><ul><li><strong>Developed by</strong>: Facebook AI</li><li><strong>Description</strong>: RoBERTa (A Robustly Optimized BERT Pretraining Approach) is an optimized version of BERT, designed to maximize performance on NLP benchmarks.</li><li><strong>Key Features</strong>: Enhanced training techniques, improved performance over BERT, and widely used in the NLP community.</li><li><strong>Repository</strong>: <a href=https://github.com/pytorch/fairseq/tree/main/examples/roberta>GitHub</a></li></ul><h3 id=7-albert-a-lite-bert>7. <strong>ALBERT (A Lite BERT)</strong><a class=td-heading-self-link href=#7-albert-a-lite-bert aria-label="Heading self-link"></a></h3><ul><li><strong>Developed by</strong>: Google Research</li><li><strong>Description</strong>: ALBERT reduces the memory and computation requirements of BERT while maintaining similar performance levels.</li><li><strong>Key Features</strong>: Parameter-sharing across layers, factorized embedding parameterization, and efficient training.</li><li><strong>Repository</strong>: <a href=https://github.com/google-research/albert>GitHub</a></li></ul><h3 id=8-bert-bidirectional-encoder-representations-from-transformers>8. <strong>BERT (Bidirectional Encoder Representations from Transformers)</strong><a class=td-heading-self-link href=#8-bert-bidirectional-encoder-representations-from-transformers aria-label="Heading self-link"></a></h3><ul><li><strong>Developed by</strong>: Google AI</li><li><strong>Description</strong>: BERT is a groundbreaking model that introduced bidirectional training of transformers for NLP tasks.</li><li><strong>Key Features</strong>: Strong performance on a wide range of NLP tasks, pre-trained on large datasets, and the basis for many subsequent models.</li><li><strong>Repository</strong>: <a href=https://github.com/google-research/bert>GitHub</a></li></ul><h3 id=9-megatron-lm>9. <strong>Megatron-LM</strong><a class=td-heading-self-link href=#9-megatron-lm aria-label="Heading self-link"></a></h3><ul><li><strong>Developed by</strong>: NVIDIA</li><li><strong>Description</strong>: Megatron-LM is a framework for training large-scale transformer models with efficient model parallelism.</li><li><strong>Key Features</strong>: Scalable to very large models, efficient training on multi-GPU systems, and used for training models up to hundreds of billions of parameters.</li><li><strong>Repository</strong>: <a href=https://github.com/NVIDIA/Megatron-LM>GitHub</a></li></ul><h3 id=10-gpt-neox>10. <strong>GPT-NeoX</strong><a class=td-heading-self-link href=#10-gpt-neox aria-label="Heading self-link"></a></h3><ul><li><strong>Developed by</strong>: EleutherAI</li><li><strong>Description</strong>: GPT-NeoX is a large-scale implementation of GPT-3 architecture. It&rsquo;s designed to be a flexible and efficient framework for training and deploying large language models.</li><li><strong>Key Features</strong>: Supports models up to 20 billion parameters, distributed training, and inference capabilities.</li><li><strong>Repository</strong>: <a href=https://github.com/EleutherAI/gpt-neox>GitHub</a></li></ul><p>These models provide a range of capabilities and are backed by active research and community support. Depending on your specific use case, computational resources, and expertise, you can choose a model that best fits your needs.</p><p><strong>Author</strong><br>Dr Hari Thapliyaal<br>dasarpai.com<br>linkedin.com/in/harithapliyal</p><div class=category-section><h4 class=category-section__title>Categories:</h4><div class=category-badges><a href=https://localhost:1313/categories/dsblog class=category-badge>dsblog</a></div></div><div class=td-tags><h4 class=td-tags__title>Tags:</h4><div class=category-badges><a href=https://localhost:1313/tags/llm-infrastructure class=category-badge>LLM Infrastructure</a><a href=https://localhost:1313/tags/hardware-requirements class=category-badge>Hardware Requirements</a><a href=https://localhost:1313/tags/gpu-computing class=category-badge>GPU Computing</a><a href=https://localhost:1313/tags/memory-management class=category-badge>Memory Management</a><a href=https://localhost:1313/tags/ai-infrastructure class=category-badge>AI Infrastructure</a><a href=https://localhost:1313/tags/deep-learning class=category-badge>Deep Learning</a><a href=https://localhost:1313/tags/computing-resources class=category-badge>Computing Resources</a></div></div><div class=td-author-box><div class=td-author-box__info><h4 class=td-author-box__name>Blowfish</h4><p class=td-author-box__bio>A powerful, lightweight theme for Hugo.</p><div class=td-author-box__links><a href target=_blank rel=noopener aria-label></a><a href target=_blank rel=noopener aria-label></a></div></div></div><div class=td-social-share><h4 class=td-social-share__title>Share this article:</h4><ul class=td-social-share__list><div class=social-share><a href="https://twitter.com/intent/tweet?text=How%20Much%20Memory%20Needed%20for%20LLM&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fHow-Much-Memory-Needed-for-LLM%2f" target=_blank rel=noopener aria-label="Share on Twitter"><i class="fab fa-twitter"></i>
</a><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fHow-Much-Memory-Needed-for-LLM%2f" target=_blank rel=noopener aria-label="Share on Facebook"><i class="fab fa-facebook"></i>
</a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fHow-Much-Memory-Needed-for-LLM%2f&title=How%20Much%20Memory%20Needed%20for%20LLM" target=_blank rel=noopener aria-label="Share on LinkedIn"><i class="fab fa-linkedin"></i>
</a><a href="https://www.reddit.com/submit?url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fHow-Much-Memory-Needed-for-LLM%2f&title=How%20Much%20Memory%20Needed%20for%20LLM" target=_blank rel=noopener aria-label="Share on Reddit"><i class="fab fa-reddit"></i>
</a><a href="mailto:?subject=How%20Much%20Memory%20Needed%20for%20LLM&body=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fHow-Much-Memory-Needed-for-LLM%2f" aria-label="Share via Email"><i class="fas fa-envelope"></i></a></div></ul></div><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href=https://localhost:1313/dsblog/LLM-Architecture-and-Training/ aria-label="Previous page"><div class=td-pager__meta><i class="fa-solid fa-angle-left"></i>
<span class=td-pager__meta-label><b>Previous:</b></span>
<span class=td-pager__meta-title>LLM Architecture and Training</span></div></a><a class="td-pager__link td-pager__link--next" href=https://localhost:1313/dsblog/Open-Source-vs-Closed-Source-AI/ aria-label="Next page"><div class=td-pager__meta><span class=td-pager__meta-label><b>Next:</b></span>
<span class=td-pager__meta-title>Open Source vs Closed Source AI</span>
<i class="fa-solid fa-angle-right"></i></div></a></ul></div><div class=col-md-2></div></div></main><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class=col-2><a href=https://dasarpai.com target=_blank rel=noopener><img src=https://localhost:1313/assets/images/site-logo.png alt=dasarpAI width=100 style=border-radius:12px></a></div><div class=col-8><div class=row></div><div class=row></div></div><div class=col-2></div><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"></div></div></div></footer></div><script src=https://localhost:1313/js/main.min.7c31f98e62c36b0c9c834ce3f8260a0e21895dd6aa0773e81a64b104eae3b2e8.js integrity="sha256-fDH5jmLDawycg0zj+CYKDiGJXdaqB3PoGmSxBOrjsug=" crossorigin=anonymous></script><script defer src=https://localhost:1313/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=https://localhost:1313/js/tabpane-persist.js></script><script src=https://localhost:1313/js/asciinema-player.js></script><script>(function(){var e=document.querySelector("#td-section-nav");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></body></html>