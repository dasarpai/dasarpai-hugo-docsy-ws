<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.147.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href="https://localhost:1313/favicons/favicon.ico?v=1"><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-16x16.png?v=1" sizes=16x16><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-32x32.png?v=1" sizes=32x32><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><title>Exploring Tokenization and Embedding in NLP | Blowfish</title><meta property="og:url" content="https://localhost:1313/dsblog/exploring-tokenization-and-embedding-in-nlp/">
<meta property="og:site_name" content="Blowfish"><meta property="og:title" content="Exploring Tokenization and Embedding in NLP"><meta property="og:description" content="Exploring Tokenization and Embedding in NLP Tokenization and embedding are key components of natural language processing (NLP) models. Sometimes people misunderstand tokenization and embedding and this article is to address those issues. This is in the question answer format and addressing following questions."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="dsblog"><meta property="article:published_time" content="2025-01-31T00:00:00+00:00"><meta property="article:modified_time" content="2025-01-31T00:00:00+00:00"><meta property="article:tag" content="Tokenization"><meta property="article:tag" content="Embeddings"><meta property="article:tag" content="NLP"><meta itemprop=name content="Exploring Tokenization and Embedding in NLP"><meta itemprop=description content="Exploring Tokenization and Embedding in NLP Tokenization and embedding are key components of natural language processing (NLP) models. Sometimes people misunderstand tokenization and embedding and this article is to address those issues. This is in the question answer format and addressing following questions."><meta itemprop=datePublished content="2025-01-31T00:00:00+00:00"><meta itemprop=dateModified content="2025-01-31T00:00:00+00:00"><meta itemprop=wordCount content="2427"><meta itemprop=keywords content="NLP Tokenization Schemes,Tokenization in NLP,Language Model Tokenization,Embeddings in NLP,NLP Tokenization Methods,Tokenization Techniques NLP,NLP Tokenization Examples"><meta name=twitter:card content="summary"><meta name=twitter:title content="Exploring Tokenization and Embedding in NLP"><meta name=twitter:description content="Exploring Tokenization and Embedding in NLP Tokenization and embedding are key components of natural language processing (NLP) models. Sometimes people misunderstand tokenization and embedding and this article is to address those issues. This is in the question answer format and addressing following questions."><link rel=preload href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css as=style integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css rel=stylesheet integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link rel=stylesheet type=text/css href=https://localhost:1313/css/asciinema-player.css><script src=https://code.jquery.com/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><link rel=stylesheet href=https://localhost:1313/css/custom.css><script src=https://localhost:1313/js/lunr.js></script></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-light nav-shadow flex-column flex-md-row td-navbar"><a id=agones-top class=navbar-brand href=https://localhost:1313/><svg id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg> <span class="text-uppercase fw-bold">Blowfish</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=https://localhost:1313/><span class=active>Data Science</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Docs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/community/><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/shortcodes/><span>Short Codes</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/blog/><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://github.com/nunocoracao/blowfish><span></span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href>GitHub</a></li><li class="nav-item dropdown d-none d-lg-block"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Release</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://development.agones.dev>Development</a>
<a class=dropdown-item href=https://agones.dev></a><a class=dropdown-item href=https://1-47-0.agones.dev>1.47.0</a>
<a class=dropdown-item href=https://1-46-0.agones.dev>1.46.0</a>
<a class=dropdown-item href=https://1-45-0.agones.dev>1.45.0</a>
<a class=dropdown-item href=https://1-44-0.agones.dev>1.44.0</a>
<a class=dropdown-item href=https://1-43-0.agones.dev>1.43.0</a>
<a class=dropdown-item href=https://1-42-0.agones.dev>1.42.0</a>
<a class=dropdown-item href=https://1-41-0.agones.dev>1.41.0</a>
<a class=dropdown-item href=https://1-40-0.agones.dev>1.40.0</a>
<a class=dropdown-item href=https://1-39-0.agones.dev>1.39.0</a>
<a class=dropdown-item href=https://1-38-0.agones.dev>1.38.0</a>
<a class=dropdown-item href=https://1-37-0.agones.dev>1.37.0</a>
<a class=dropdown-item href=https://1-36-0.agones.dev>1.36.0</a>
<a class=dropdown-item href=https://1-35-0.agones.dev>1.35.0</a>
<a class=dropdown-item href=https://1-34-0.agones.dev>1.34.0</a>
<a class=dropdown-item href=https://1-33-0.agones.dev>1.33.0</a>
<a class=dropdown-item href=https://1-32-0.agones.dev>1.32.0</a>
<a class=dropdown-item href=https://1-31-0.agones.dev>1.31.0</a></div></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><ul class=dropdown-menu><li><a class=dropdown-item href=https://localhost:1313/it/>Italiano</a></li></ul></div></li></ul></div><div class="navbar-nav mx-lg-2 d-none d-lg-block"></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><div class=row><div class=col-md-2></div><div class=col-md-8><p><img src=https://localhost:1313/assets/images/dspost/dsp6215-Exploring-Tokenization-in-AI.jpg alt="Exploring Tokenization and Embedding in NLP"></p><h1 id=exploring-tokenization-and-embedding-in-nlp>Exploring Tokenization and Embedding in NLP<a class=td-heading-self-link href=#exploring-tokenization-and-embedding-in-nlp aria-label="Heading self-link"></a></h1><p>Tokenization and embedding are key components of natural language processing (NLP) models. Sometimes people misunderstand tokenization and embedding and this article is to address those issues. This is in the question answer format and addressing following questions.</p><ol><li>What is tokenization?</li><li>What are different Tokenzation schemes?</li><li>What is OOV (Out-of-Vocabulary) in Tokenization?</li><li>If a word does not exist in embedding model&rsquo;s vocabulary, then how tokenization and embedding is done?</li><li>What is criteria of splitting a word?</li><li>What is Subword Tokenization?</li><li>How FastText Tokenization works?</li><li>What is role of [CLS] token?</li><li>What is WordPiece and how it works?</li><li>What is BPE (Byte Pair Encoding), and how it works?</li><li>What is SentencePiece and how it works?</li><li>For Indian languages what tokenization schemes is the best?</li></ol><h2 id=what-is-tokenization>What is tokenization?<a class=td-heading-self-link href=#what-is-tokenization aria-label="Heading self-link"></a></h2><p>Tokenization is the process of breaking text into smaller units (tokens), such as words, subwords, or characters, for NLP tasks.</p><p>Purpose:
‚úî Converts raw text into a structured format for machine learning models.
‚úî Handles out-of-vocabulary (OOV) words and improves text processing.
‚úî Enables models to understand and generate human language efficiently.</p><h2 id=what-are-different-tokenzation-schemes>What are different Tokenzation schemes?<a class=td-heading-self-link href=#what-are-different-tokenzation-schemes aria-label="Heading self-link"></a></h2><p>The most popular and widely used in NLP today. Each scheme is optimized for different tasks, languages, and domains.</p><ol><li><strong>Word-Based</strong> (Simple, but suffers from OOV issues)</li><li><strong>Whitespace & Rule-Based</strong> (NLTK, spaCy, useful for preprocessing)</li><li><strong>Subword-Based</strong> (WordPiece, BPE, Unigram‚Äîused in BERT, GPT, T5)</li><li><strong>Character-Level</strong> (Good for misspellings, used in FastText, OCR)</li><li><strong>Byte-Level</strong> (GPT-4, T5, handles emojis and special characters)</li><li><strong>Morpheme-Based</strong> (Japanese, Korean, Chinese NLP)</li><li><strong>Sentence-Level</strong> (Text summarization, document-level models)</li><li><strong>Hybrid Tokenization</strong> (Combining multiple methods‚ÄîXLNet, ALBERT)</li><li><strong>Augmented BPE</strong> (Multilingual NLP‚ÄîmBERT, XLM-R)</li><li><strong>Meta-Corpus Tokenization</strong> (Self-learning tokenization‚ÄîDeepMind RETRO)</li><li><strong>Hierarchical Tokenization</strong> (Sentence ‚Üí Word ‚Üí Subword ‚Üí Character)</li><li><strong>Phoneme-Based</strong> (Speech recognition‚ÄîWhisper, wav2vec)</li><li><strong>Radical-Based</strong> (For Chinese/Japanese‚ÄîMeCab, KoNLPy)</li><li><strong>Graph-Based</strong> (For knowledge graphs‚ÄîKG-BERT)</li><li><strong>Punctuation-Aware</strong> (Legal, TTS, sentiment analysis)</li><li><strong>Code-Specific</strong> (For programming languages‚ÄîCodeBERT, Codex)</li></ol><h2 id=what-is-oov-out-of-vocabulary-in-tokenization>What is OOV (Out-of-Vocabulary) in Tokenization?<a class=td-heading-self-link href=#what-is-oov-out-of-vocabulary-in-tokenization aria-label="Heading self-link"></a></h2><p>OOV (Out-of-Vocabulary) refers to words that are not present in a model‚Äôs vocabulary and cannot be directly recognized or processed.</p><p>Why Does OOV Occur?</p><ul><li>Limited vocabulary size in NLP models.</li><li>Rare or new words, such as slang or technical terms.</li><li>Misspellings or variations of words.</li><li>Morphologically complex words, especially in rich languages like Hindi or Finnish.</li></ul><h2 id=if-a-word-does-not-exist-in-embedding-models-vocabulary-then-how-tokenization-and-embedding-is-done>If a word does not exist in embedding model&rsquo;s vocabulary, then how tokenization and embedding is done?<a class=td-heading-self-link href=#if-a-word-does-not-exist-in-embedding-models-vocabulary-then-how-tokenization-and-embedding-is-done aria-label="Heading self-link"></a></h2><p>For example we have word <em>simultaneously</em> which is not part of vocbulary. In that case it is tokenized into multiple tokens in a transformer-based model (e.g., BERT, GPT), each token gets its own embedding. To generate an embedding for the complete word, there are several strategies:</p><ol><li><p><strong>Taking the First Token&rsquo;s Embedding</strong></p><ul><li>Many transformer models (e.g., BERT) use <strong>wordpiece</strong> tokenization, where the first subword token often carries more meaning. Using the embedding of the first token can be a simple and efficient choice.</li></ul></li><li><p><strong>Averaging the Token Embeddings</strong></p><ul><li>Compute the <strong>mean</strong> of all subword embeddings:
$$ E_{word} = \frac{1}{N} \sum_{i=1}^{N} E_{token_i}$$</li><li>This smooths out variations and gives a balanced representation.</li></ul></li><li><p><strong>Weighted Average (Attention-Based)</strong></p><ul><li>Some methods use attention weights to give more importance to certain subwords, especially in contextual embeddings.</li></ul></li><li><p><strong>Using the Last Token&rsquo;s Embedding</strong></p><ul><li>If the last token carries a suffix that changes the meaning (e.g., &ldquo;-ing&rdquo;, &ldquo;-ly&rdquo;), it might be useful to consider its embedding.</li></ul></li><li><p><strong>Concatenation of Embeddings</strong></p><ul><li>Instead of averaging, concatenate the embeddings of all tokens, creating a longer but richer word representation.</li></ul></li><li><p><strong>Fine-tuning with a Context-Aware Model</strong></p><ul><li>If the application requires better word-level embeddings, a downstream model can be trained to aggregate subword embeddings effectively.</li></ul></li></ol><p><strong>Best Practice</strong>
For general NLP tasks, <strong>averaging subword embeddings</strong> is a common and effective strategy. However, in tasks like machine translation or sentiment analysis, an attention-weighted or task-specific approach may work better.</p><h2 id=what-is-criteria-of-splitting-a-word>What is criteria of splitting a word?<a class=td-heading-self-link href=#what-is-criteria-of-splitting-a-word aria-label="Heading self-link"></a></h2><p>The criteria for splitting a word into multiple tokens in BERT (or similar models) depend on the tokenization algorithm. BERT uses WordPiece tokenization, which follows these rules:</p><p>simultaneously ‚Üí [&lsquo;simult&rsquo;, &lsquo;##aneously&rsquo;]
transformers ‚Üí [&rsquo;transform&rsquo;, &lsquo;##ers&rsquo;] # Splitting Follows the &ldquo;Longest Match First&rdquo; Rule<br>Subword Tokens Start with ##</p><h2 id=what-is-subword-tokenization>What is Subword Tokenization?<a class=td-heading-self-link href=#what-is-subword-tokenization aria-label="Heading self-link"></a></h2><p><strong>Subword Tokenization</strong> is a technique that breaks words into smaller, meaningful units (subwords) rather than whole words or individual characters. This helps handle <strong>out-of-vocabulary (OOV) words</strong>, <strong>morphologically rich languages</strong>, and <strong>rare words</strong> efficiently.</p><p><strong>How It Works:</strong></p><ol><li><p><strong>Training Phase</strong>:</p><ul><li>A vocabulary is built using <strong>frequent words</strong> and <strong>subwords</strong> from a corpus.</li><li>Common sequences of characters are merged iteratively.</li></ul></li><li><p><strong>Tokenization Phase</strong> (at inference):</p><ul><li>Words are split into <strong>subwords</strong> based on the trained vocabulary.</li><li>Frequent words remain whole, while rare words are broken into subwords.</li></ul></li></ol><p><strong>Example (Using BPE/WordPiece)</strong></p><ul><li><strong>Input:</strong> <code>"unhappiness"</code></li><li><strong>Tokenized as:</strong> <code>["un", "happiness"]</code><br>(if &ldquo;happiness&rdquo; is common)</li><li><strong>If not in vocabulary:</strong> <code>["un", "happi", "ness"]</code></li></ul><p><strong>Popular Subword Tokenization Methods</strong></p><ul><li><strong>Byte Pair Encoding (BPE)</strong> ‚Äì Used in GPT models.</li><li><strong>WordPiece</strong> ‚Äì Used in BERT.</li><li><strong>SentencePiece (Unigram)</strong> ‚Äì Used in T5, mBERT.</li><li><strong>FastText</strong></li></ul><p><strong>Key Benefits</strong>
‚úîÔ∏è Handles <strong>rare words</strong><br>‚úîÔ∏è Reduces <strong>vocabulary size</strong><br>‚úîÔ∏è Works well for <strong>multilingual models</strong></p><h2 id=how-fasttext-tokenization-works>How FastText Tokenization works?<a class=td-heading-self-link href=#how-fasttext-tokenization-works aria-label="Heading self-link"></a></h2><p><strong>FastText</strong> uses <strong>subword tokenization</strong>, but in a different way compared to <strong>BPE, WordPiece, or SentencePiece</strong>.</p><p><strong>How FastText Uses Subwords</strong><br>Instead of breaking words into learned <strong>subword units</strong>, FastText represents words using <strong>character n-grams</strong> (continuous sequences of <code>n</code> characters).</p><p><strong>How It Works:</strong></p><ol><li>Each word is split into overlapping <strong>character n-grams</strong> (default: <strong>3-6 characters</strong>).</li><li>The word embedding is computed as the sum of its <strong>subword embeddings</strong>.</li><li>This helps handle <strong>OOV words</strong>, <strong>misspellings</strong>, and <strong>morphological variations</strong> better than traditional word embeddings.</li></ol><p><strong>Example:</strong><br>For the word <strong>&ldquo;apple&rdquo;</strong>, with <code>n=3</code>:</p><ul><li>Subwords: <code>["&lt;ap", "app", "ppl", "ple", "le>"]</code></li><li>Word embedding = sum of all subword embeddings</li></ul><h2 id=what-is-role-of-cls-token>What is role of [CLS] token?<a class=td-heading-self-link href=#what-is-role-of-cls-token aria-label="Heading self-link"></a></h2><p>The [CLS] token embedding is used when you want a context-aware representation of a word within a sentence, rather than just its isolated meaning. In BERT, the first token of every input is [CLS], which learns a summary representation of the entire sequence. This is often useful for sentence-level tasks like classification, but it can also be used as a context-aware word embedding when working with entire sentences.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>sentence</span> <span class=o>=</span> <span class=s2>&#34;I saw an elephant in the zoo.&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Tokenize the sentence</span>
</span></span><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sentence</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Get embeddings</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Convert sentence to token list</span>
</span></span><span class=line><span class=cl><span class=n>token_list</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>tokenize</span><span class=p>(</span><span class=n>sentence</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>token_list</span><span class=p>)</span>  <span class=c1># Example output: [&#39;i&#39;, &#39;saw&#39;, &#39;an&#39;, &#39;elephant&#39;, &#39;in&#39;, &#39;the&#39;, &#39;zoo&#39;, &#39;.&#39;]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Find index of &#34;elephant&#34;</span>
</span></span><span class=line><span class=cl><span class=n>elephant_index</span> <span class=o>=</span> <span class=n>token_list</span><span class=o>.</span><span class=n>index</span><span class=p>(</span><span class=s2>&#34;elephant&#34;</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span>  <span class=c1># +1 because BERT adds [CLS] at position 0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Extract &#34;elephant&#34; embedding</span>
</span></span><span class=line><span class=cl><span class=n>elephant_embedding</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>last_hidden_state</span><span class=p>[:,</span> <span class=n>elephant_index</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Elephant Embedding Shape:&#34;</span><span class=p>,</span> <span class=n>elephant_embedding</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>  <span class=c1># Expected: (1, 768)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Extract embedding for the entire sentence</span>
</span></span><span class=line><span class=cl><span class=n>sentence_embedding</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>last_hidden_state</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Sentence Embedding Shape:&#34;</span><span class=p>,</span> <span class=n>sentence_embedding</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>  <span class=c1># Expected: (1, 768)</span>
</span></span></code></pre></div><h2 id=what-is-wordpiece-and-how-it-works>What is WordPiece and how it works?<a class=td-heading-self-link href=#what-is-wordpiece-and-how-it-works aria-label="Heading self-link"></a></h2><p><strong>WordPiece</strong> is a <strong>subword tokenization algorithm</strong> used in <strong>BERT, mBERT, and ALBERT</strong>. It helps handle <strong>OOV words</strong>, reduces vocabulary size, and improves efficiency in NLP models.</p><p><strong>How WordPiece Works:</strong></p><ol><li><p><strong>Build Vocabulary:</strong></p><ul><li>Start with a base vocabulary (single characters).</li><li>Iteratively merge the most <strong>frequent</strong> adjacent character sequences to form subwords.</li><li>Stop when the vocabulary reaches a predefined size (e.g., 30,000 for BERT).</li></ul></li><li><p><strong>Tokenization Process:</strong></p><ul><li>Split words into subwords based on the trained vocabulary.</li><li>If a word is not in the vocabulary, it is <strong>broken into known subwords</strong>, prefixed with <code>##</code> (to indicate continuation).</li></ul></li></ol><p><strong>Example:</strong></p><ul><li><strong>Input:</strong> <code>"unhappiness"</code></li><li><strong>Tokenized as:</strong> <code>["un", "##happi", "##ness"]</code><br>(if &ldquo;happiness&rdquo; isn&rsquo;t frequent, it&rsquo;s split further)</li></ul><h2 id=what-is-bpe-byte-pair-encoding-and-how-it-works>What is BPE (Byte Pair Encoding), and how it works?<a class=td-heading-self-link href=#what-is-bpe-byte-pair-encoding-and-how-it-works aria-label="Heading self-link"></a></h2><p><strong>BPE (Byte Pair Encoding)</strong> is a <strong>subword tokenization algorithm</strong> that replaces the most frequent character pairs with new tokens iteratively. It is widely used in <strong>GPT models, MarianMT, and SentencePiece</strong> to handle <strong>rare words, reduce vocabulary size, and improve text compression</strong>.</p><hr><p><strong>Step 1. Training Phase (Vocabulary Learning)</strong></p><ol><li><strong>Start with individual characters</strong> as the base vocabulary.</li><li><strong>Count frequent adjacent character pairs</strong> in the corpus.</li><li><strong>Merge the most frequent pair</strong> into a new token.</li><li><strong>Repeat steps 2-3</strong> until a predefined vocabulary size is reached.</li></ol><hr><p><strong>Step 2. Tokenization Phase (Applying Learned Rules)</strong></p><ol><li>Break words into <strong>subwords</strong> using the trained vocabulary.</li><li>If a word is not found in the vocabulary, break it into the longest matching subwords.</li></ol><hr><p><strong>Example:</strong><br><strong>Corpus:</strong> <code>"low", "lowest", "lower"</code></p><ol><li>Initial tokens: <code>["l", "o", "w", "e", "s", "t", "r"]</code></li><li>Merge frequent pairs:<ul><li><code>"l o"</code> ‚Üí <code>"lo"</code></li><li><code>"lo w"</code> ‚Üí <code>"low"</code></li><li><code>"low e"</code> ‚Üí <code>"lowe"</code></li><li><code>"lowe s"</code> ‚Üí <code>"lowes"</code></li><li><code>"lowes t"</code> ‚Üí <code>"lowest"</code></li></ul></li></ol><p><strong>Final tokenized words:</strong></p><ul><li><code>"low"</code> ‚Üí <code>["low"]</code></li><li><code>"lowest"</code> ‚Üí <code>["low","est"]</code></li><li><code>"lower"</code> ‚Üí <code>["low", "er"]</code></li></ul><h2 id=what-is-sentencepiece-and-how-it-works>What is SentencePiece and how it works?<a class=td-heading-self-link href=#what-is-sentencepiece-and-how-it-works aria-label="Heading self-link"></a></h2><p><strong>What is SentencePiece?</strong><br><strong>SentencePiece</strong> is a data-driven <strong>subword tokenization algorithm</strong> used in <strong>T5, mBERT, XLNet, and MarianMT</strong>. Unlike <strong>BPE</strong> or <strong>WordPiece</strong>, it treats text as a <strong>stream of raw bytes</strong> instead of splitting on whitespace or relying on pre-tokenized words.</p><hr><p><strong>How SentencePiece Works?</strong></p><ol><li><p><strong>Preprocesses Text Without Whitespace Assumptions</strong></p><ul><li>Treats input text as a <strong>continuous stream of characters</strong> (no need for space-based tokenization).</li><li>Works well for languages <strong>without clear word boundaries</strong> (e.g., Chinese, Japanese, Thai).</li></ul></li><li><p><strong>Builds a Token Vocabulary Using One of Two Methods:</strong></p><ul><li><strong>Byte Pair Encoding (BPE)</strong> ‚Üí Merges frequent character pairs iteratively.</li><li><strong>Unigram Model</strong> ‚Üí Uses a <strong>probabilistic approach</strong>, where subword units are scored based on likelihood.</li></ul></li><li><p><strong>Tokenizes New Text Based on Learned Vocabulary</strong></p><ul><li>Splits words into <strong>subwords</strong> based on the highest-probability segmentations.</li></ul></li></ol><hr><p><strong>Example:</strong><br><strong>Input:</strong> <code>"unhappiness"</code></p><p>If using <strong>BPE-based SentencePiece:</strong><br>üîπ <code>["un", "happiness"]</code> (if &ldquo;happiness&rdquo; is frequent)<br>üîπ <code>["un", "happi", "ness"]</code> (if &ldquo;happiness&rdquo; isn&rsquo;t frequent)</p><p>If using <strong>Unigram-based SentencePiece:</strong><br>üîπ Multiple segmentations are possible, and the most probable one is selected.</p><hr><p><strong>Key Advantages of SentencePiece:</strong><br>‚úî <strong>Supports raw text</strong> (doesn‚Äôt need pre-tokenization)<br>‚úî <strong>Works for multiple languages</strong> (even ones without spaces)<br>‚úî <strong>Can use both BPE and Unigram approaches</strong><br>‚úî <strong>Highly efficient for Neural Machine Translation (NMT)</strong></p><h2 id=for-indian-languages-what-tokenization-schemes-is-the-best->For Indian languages what tokenization schemes is the best ?<a class=td-heading-self-link href=#for-indian-languages-what-tokenization-schemes-is-the-best- aria-label="Heading self-link"></a></h2><p>For <strong>Indian languages</strong>, tokenization can be challenging due to their diverse scripts, rich morphology, and the fact that many languages have complex compound words, words with complex characters, and inflections. Below are the <strong>best tokenization schemes</strong> and techniques specifically suited for Indian languages:</p><hr><p><strong>1. Morpheme-Based Tokenization</strong></p><ul><li>Indian languages like <strong>Hindi, Bengali, Tamil, Kannada</strong>, etc., often have complex morphology, and <strong>morpheme-based tokenization</strong> is highly effective here.</li><li>This approach breaks down words into smaller meaningful units (morphemes), allowing for better handling of compound words and inflections.</li><li>Indian languages often have <strong>rich inflection</strong> (gender, tense, case), and tokenizing at the morpheme level preserves these nuances.</li></ul><p><strong>Example:</strong></p><ul><li><strong>Hindi:</strong> &ldquo;‡§µ‡§π ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à&rdquo; (&ldquo;He goes to school&rdquo;)<br>Tokenized as: [&ldquo;‡§µ‡§π&rdquo;, &ldquo;‡§∏‡•ç‡§ï‡•Ç‡§≤&rdquo;, &ldquo;‡§ú‡§æ‡§§‡§æ&rdquo;, &ldquo;‡§π‡•à&rdquo;]<br>Using morpheme-based tokenization, one could further break down the word &ldquo;‡§ú‡§æ‡§§‡§æ&rdquo; (goes) into &ldquo;‡§ú‡§æ&rdquo; (go) + &ldquo;‡§§‡§æ&rdquo; (tense suffix).</li></ul><p><strong>Used in Libraries:</strong></p><ul><li><strong>Indic NLP Library</strong></li><li><strong>KoNLPy</strong> (though more popular for Korean, adaptations exist for Indian languages)</li></ul><hr><p><strong>2. Subword Tokenization</strong>
<strong>Multilingual models</strong> like <strong>mBERT</strong> support Indian languages, and SentencePiece tokenization is often used in these models. These models break down words into <strong>subwords</strong>, enabling better handling of complex and compound words.</p><ul><li>Models like WordPiece, BPE, SentencePiece break down words into <strong>subwords</strong>, enabling better handling of complex and compound words.</li><li><strong>WordPiece, BPE, or SentencePiece</strong> models are popular choices for tokenizing Indian languages, particularly for <strong>Neural Machine Translation (NMT)</strong> or <strong>multilingual models</strong> like <strong>mBERT</strong> and <strong>XLM-R</strong>.</li><li>These methods are effective for handling <strong>out-of-vocabulary (OOV)</strong> words in low-resource Indian languages.</li></ul><p><strong>Example:</strong></p><ul><li><strong>Hindi:</strong> &ldquo;‡§∏‡•ç‡§µ‡§§‡§Ç‡§§‡•ç‡§∞‡§§‡§æ ‡§∏‡§Ç‡§ó‡•ç‡§∞‡§æ‡§Æ ‡§∏‡•á‡§®‡§æ‡§®‡•Ä&rdquo; (&ldquo;Freedom Fighter&rdquo;)<br>Tokenized into subwords: [&ldquo;‡§∏‡•ç‡§µ‡§§&rdquo;, &ldquo;‡§Ç‡§§‡•ç‡§∞&rdquo;, &ldquo;‡§§‡§æ&rdquo;, &ldquo;‡§∏‡§Ç&rdquo;, &ldquo;‡§ó‡•ç‡§∞‡§æ‡§Æ&rdquo;, &ldquo;‡§∏‡•á&rdquo;, &ldquo;‡§®‡§æ‡§®‡•Ä&rdquo;]</li></ul><p><strong>Used in Libraries:</strong></p><ul><li><strong>mBERT</strong>, <strong>XLM-R</strong>, <strong>IndicBERT</strong>, <strong>T5</strong>, <strong>XLM</strong></li></ul><hr><p><strong>3. Hybrid Tokenization (Combining WordPiece + Character-Level)</strong>
This approach combines the best of <strong>word-level tokenization</strong> and <strong>character-level tokenization</strong>. It‚Äôs especially useful for <strong>Indian languages</strong> because they have a combination of regular words and highly agglutinative or inflected words. Hybrid tokenization helps model both <strong>frequent words</strong> and <strong>rare or unknown words</strong> by combining word-level tokenization with the ability to handle characters.</p><p><strong>Example:</strong></p><ul><li><strong>Tamil:</strong> &ldquo;‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æ®‡Ææ‡Æü‡ØÅ&rdquo; (&ldquo;Tamil Nadu&rdquo;)<ul><li>Could be tokenized as [&ldquo;‡Æ§&rdquo;, &ldquo;‡ÆÆ&rdquo;, &ldquo;‡Æø‡Æ¥&rdquo;, &ldquo;‡Æ®‡Ææ‡Æü‡ØÅ&rdquo;] (character-level) or as one word in <strong>pre-trained multilingual models</strong>.</li></ul></li></ul><p><strong>Used in Libraries:</strong></p><ul><li><strong>IndicBERT</strong>, <strong>XLM-R</strong> (combines character-level and subword-level tokenization).</li></ul><hr><p><strong>4. Sentence-Level Tokenization</strong>
For tasks like <strong>translation</strong>, <strong>summarization</strong>, or <strong>document-level tasks</strong> in <strong>Indian languages</strong>, sentence-level tokenization is often used. <strong>Sentence segmentation</strong> helps in <strong>splitting long texts</strong> into meaningful sentences, which can then be processed by NLP models. Indian languages have complex sentence structures, and sentence segmentation ensures that each meaningful unit is tokenized appropriately.</p><p><strong>Example:</strong></p><ul><li><strong>Hindi:</strong> &ldquo;‡§Æ‡•à‡§Ç ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ú‡§æ‡§§‡§æ ‡§π‡•Ç‡§Å‡•§&rdquo; ‚Üí [&ldquo;‡§Æ‡•à‡§Ç ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ú‡§æ‡§§‡§æ ‡§π‡•Ç‡§Å‡•§&rdquo;] - a complete sentence.</li><li><strong>Tamil:</strong> &ldquo;‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø‡Æï‡Øç‡Æï‡ØÅ ‡Æ™‡Øã‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç.&rdquo; ‚Üí [&ldquo;‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø‡Æï‡Øç‡Æï‡ØÅ ‡Æ™‡Øã‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç.&rdquo;]</li></ul><p><strong>Used in Libraries:</strong></p><ul><li><strong>IndicNLP</strong> for segmentation.</li><li><strong>spaCy</strong> with custom models.</li></ul><hr><p><strong>5. Grapheme-Based Tokenization (Script-Specific)</strong>
<strong>Grapheme-based tokenization</strong> is used for handling text at the character level. Especially useful for handling <strong>script-specific challenges</strong> (e.g., <strong>ligatures</strong> or <strong>complex characters</strong>). It works at the <strong>character level</strong>, breaking down words into individual characters, making it easier to handle scripts with rich diacritics and combinations.</p><p><strong>Example:</strong></p><ul><li><strong>Kannada:</strong> &ldquo;‡≤¨‡≥Ü‡≤Ç‡≤ó‡≤≥‡≥Ç‡≤∞‡≥Å&rdquo; (&ldquo;Bengaluru&rdquo;)<br>Tokenized at the grapheme level: [&ldquo;‡≤¨‡≥Ü&rdquo;, &ldquo;‡≤Ç&rdquo;, &ldquo;‡≤ó&rdquo;, &ldquo;‡≤≤&rdquo;, &ldquo;‡≥Ç&rdquo;, &ldquo;‡≤∞‡≥Å&rdquo;]</li></ul><p><strong>Used in Libraries:</strong></p><ul><li><strong>Indic NLP Library</strong> for preprocessing. <strong>ICU-based tokenizers</strong> (Unicode handling).</li></ul><hr><p><strong>6. Character n-grams</strong>
<strong>FastText</strong> is a <strong>character-level model</strong> that uses <strong>n-grams</strong> (subsequences of n characters) to generate word embeddings, and it works well for <strong>Indian languages</strong>, especially for <strong>low-resource languages</strong>. It works well for low-resource languages. Useful for text classification, <strong>word embeddings</strong>, and <strong>language identification</strong> in Indian languages. FastText works well with <strong>misspelled words</strong>, <strong>out-of-vocabulary terms</strong>, and <strong>morphologically rich languages</strong> like Hindi, Tamil, or Bengali.</p><p><strong>Example:</strong></p><ul><li><strong>Hindi:</strong> &ldquo;‡§∏‡§Æ‡§æ‡§ú‡§µ‡§æ‡§¶‡•Ä&rdquo; (&ldquo;Socialist&rdquo;)<br>Tokenized as character n-grams like [&ldquo;‡§∏‡§Æ&rdquo;, &ldquo;‡§Æ‡§æ&rdquo;, &ldquo;‡§æ‡§ú&rdquo;, &ldquo;‡§µ‡§æ‡§¶&rdquo;, &ldquo;‡§µ‡•Ä&rdquo;].</li></ul><p><strong>Used in Libraries:</strong></p><ul><li><strong>FastText model</strong> for Indian language embeddings.</li></ul><hr><p><strong>7. Punctuation-Aware Tokenization</strong>
For <strong>sentiment analysis</strong>, <strong>legal text</strong>, and <strong>social media data</strong>, tokenizers need to handle <strong>punctuation</strong> and <strong>emojis</strong> effectively. <strong>Indian languages</strong> with a mix of <strong>Hindi, English (Hinglish)</strong>, and emojis require punctuation-aware tokenization. Keeps punctuation as separate tokens, ensuring that models correctly interpret sentiments or legal structures.</p><p><strong>Example:</strong></p><ul><li><strong>Hindi-English:</strong> &ldquo;‡§Æ‡•Å‡§ù‡•á food ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§≤‡§ó‡§æ!&rdquo; (&ldquo;I liked food!&rdquo;)<br>Tokenized as: [&ldquo;‡§Æ‡•Å‡§ù‡•á&rdquo;, &ldquo;food&rdquo;, &ldquo;‡§Ö‡§ö‡•ç‡§õ‡§æ&rdquo;, &ldquo;‡§≤‡§ó‡§æ&rdquo;, &ldquo;!&rdquo;]</li></ul><p><strong>Used in Libraries:</strong></p><ul><li><strong>BERT-based</strong> models like <strong>Hinglish-BERT</strong>.</li><li><strong>Or other Multi-lingual or code-mixed NLP models</strong>.</li></ul><hr><p><strong>8. Indic-Specific Models (mBERT, IndicBERT)</strong>
Pre-trained models like <strong>mBERT</strong> or <strong>IndicBERT</strong> are trained on <strong>multiple Indian languages</strong>. They rely on <strong>subword tokenization</strong> (BPE/WordPiece) for efficient handling of Indian languages. <strong>IndicBERT</strong> is specifically optimized for <strong>Indian languages</strong> and trained on <strong>11 official languages</strong> of India. These models are designed with <strong>Indian linguistic features</strong> in mind, and they support multiple <strong>scripts</strong> and <strong>languages</strong> without requiring language-specific tokenization.</p><p><strong>Example:</strong></p><ul><li><strong>Hindi:</strong> &ldquo;‡§≠‡§æ‡§∞‡§§&rdquo; (&ldquo;India&rdquo;)<br>Tokenized as: [&ldquo;‡§≠‡§æ‡§∞‡§§&rdquo;]</li></ul><p><strong>Used in Libraries:</strong></p><ul><li><strong>IndicBERT</strong>, <strong>mBERT</strong>, <strong>XLM-R</strong> for multilingual tasks.</li></ul><hr><p><strong>Which Tokenization Scheme is Best for Indian Languages?</strong></p><ul><li><strong>Morpheme-Based</strong>: Best for <strong>richly inflected languages</strong> (e.g., Hindi, Tamil, Kannada).</li><li><strong>BPE/WordPiece/SentencePiece</strong>: Best for <strong>multilingual models</strong> (e.g., <strong>mBERT</strong>, <strong>IndicBERT</strong>) and <strong>low-resource languages</strong>.</li><li><strong>Hybrid Tokenization</strong>: Effective for <strong>handling both frequent and rare words</strong> (e.g., <strong>IndicBERT</strong>, <strong>XLM-R</strong>).</li><li><strong>FastText</strong>: Best for <strong>word embeddings</strong>, <strong>language identification</strong>, and <strong>low-resource languages</strong>.</li><li><strong>Punctuation-Aware</strong>: Best for <strong>sentiment analysis</strong>, <strong>legal text</strong> or <strong>social media</strong>.</li></ul><hr><div class=category-section><h4 class=category-section__title>Categories:</h4><div class=category-badges><a href=https://localhost:1313/categories/dsblog class=category-badge>dsblog</a></div></div><div class=td-tags><h4 class=td-tags__title>Tags:</h4><div class=category-badges><a href=https://localhost:1313/tags/tokenization class=category-badge>Tokenization</a><a href=https://localhost:1313/tags/embeddings class=category-badge>Embeddings</a><a href=https://localhost:1313/tags/nlp class=category-badge>NLP</a></div></div><div class=td-author-box><div class=td-author-box__info><h4 class=td-author-box__name>Blowfish</h4><p class=td-author-box__bio>A powerful, lightweight theme for Hugo.</p><div class=td-author-box__links><a href target=_blank rel=noopener aria-label></a><a href target=_blank rel=noopener aria-label></a></div></div></div><div class=td-social-share><h4 class=td-social-share__title>Share this article:</h4><ul class=td-social-share__list><div class=social-share><a href="https://twitter.com/intent/tweet?text=Exploring%20Tokenization%20and%20Embedding%20in%20NLP&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fexploring-tokenization-and-embedding-in-nlp%2f" target=_blank rel=noopener aria-label="Share on Twitter"><i class="fab fa-twitter"></i>
</a><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fexploring-tokenization-and-embedding-in-nlp%2f" target=_blank rel=noopener aria-label="Share on Facebook"><i class="fab fa-facebook"></i>
</a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fexploring-tokenization-and-embedding-in-nlp%2f&title=Exploring%20Tokenization%20and%20Embedding%20in%20NLP" target=_blank rel=noopener aria-label="Share on LinkedIn"><i class="fab fa-linkedin"></i>
</a><a href="https://www.reddit.com/submit?url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fexploring-tokenization-and-embedding-in-nlp%2f&title=Exploring%20Tokenization%20and%20Embedding%20in%20NLP" target=_blank rel=noopener aria-label="Share on Reddit"><i class="fab fa-reddit"></i>
</a><a href="mailto:?subject=Exploring%20Tokenization%20and%20Embedding%20in%20NLP&body=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fexploring-tokenization-and-embedding-in-nlp%2f" aria-label="Share via Email"><i class="fas fa-envelope"></i></a></div></ul></div><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href=https://localhost:1313/dsblog/understanding-contextual-embedding-in-transformers/ aria-label="Previous page"><div class=td-pager__meta><i class="fa-solid fa-angle-left"></i>
<span class=td-pager__meta-label><b>Previous:</b></span>
<span class=td-pager__meta-title>Understanding Contextual Embedding in Transformers</span></div></a><a class="td-pager__link td-pager__link--next" href=https://localhost:1313/dsblog/demystify-nvidia-gpus/ aria-label="Next page"><div class=td-pager__meta><span class=td-pager__meta-label><b>Next:</b></span>
<span class=td-pager__meta-title>Demystifying NVIDIA GPUs</span>
<i class="fa-solid fa-angle-right"></i></div></a></ul></div><div class=col-md-2></div></div></main><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class=col-2><a href=https://dasarpai.com target=_blank rel=noopener><img src=https://localhost:1313/assets/images/site-logo.png alt=dasarpAI width=100 style=border-radius:12px></a></div><div class=col-8><div class=row></div><div class=row></div></div><div class=col-2></div><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"></div></div></div></footer></div><script src=https://localhost:1313/js/main.min.7c31f98e62c36b0c9c834ce3f8260a0e21895dd6aa0773e81a64b104eae3b2e8.js integrity="sha256-fDH5jmLDawycg0zj+CYKDiGJXdaqB3PoGmSxBOrjsug=" crossorigin=anonymous></script><script defer src=https://localhost:1313/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=https://localhost:1313/js/tabpane-persist.js></script><script src=https://localhost:1313/js/asciinema-player.js></script><script>(function(){var e=document.querySelector("#td-section-nav");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></body></html>