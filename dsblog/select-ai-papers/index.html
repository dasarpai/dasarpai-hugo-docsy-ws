<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.147.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href="https://localhost:1313/favicons/favicon.ico?v=1"><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-16x16.png?v=1" sizes=16x16><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-32x32.png?v=1" sizes=32x32><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><title>Important AI Paper List | Blowfish</title><meta property="og:url" content="https://localhost:1313/dsblog/select-ai-papers/">
<meta property="og:site_name" content="Blowfish"><meta property="og:title" content="Important AI Paper List"><meta property="og:description" content="Important AI Paper List Introduciton In almost all citations it becomes very difficult to read the title of research papers. Why? Because the contributors’ information is first and most of the time, it is difficult to read the name other than native people. For example, if an Indian find a native name like “Vivek Ramaswami, Kartikeyan Karunanidhi” it is easy for them to read the name but the same name becomes difficult to read for non-Indian people, and vice-versa. Giving respect to the creator is very important but more than we need to know what have they done. I know from my experience, for almost every researcher, it becomes very difficult to track good AI research papers. For me, it is more difficult because I need to maintain this blog and I want to give references to the work across different webpages. Therefore I am creating a citation key, which includes the Last name of the first researcher + year of presenting that paper. Along with this, I am describing the title of the paper and where it was presented. If you find a particular title interesting for your work you can search that paper on “google scholar”, Mendeley, sci-hub or other places with which you are familiar and comfortable. Post that you can download and read that paper at your leisure. Hope you find this list of some use for your work."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="dsblog"><meta property="article:published_time" content="2023-08-22T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-22T00:00:00+00:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="Encoder"><meta property="article:tag" content="Decoder"><meta property="article:tag" content="Research Papers"><meta property="article:tag" content="AI Research"><meta itemprop=name content="Important AI Paper List"><meta itemprop=description content="Important AI Paper List Introduciton In almost all citations it becomes very difficult to read the title of research papers. Why? Because the contributors’ information is first and most of the time, it is difficult to read the name other than native people. For example, if an Indian find a native name like “Vivek Ramaswami, Kartikeyan Karunanidhi” it is easy for them to read the name but the same name becomes difficult to read for non-Indian people, and vice-versa. Giving respect to the creator is very important but more than we need to know what have they done. I know from my experience, for almost every researcher, it becomes very difficult to track good AI research papers. For me, it is more difficult because I need to maintain this blog and I want to give references to the work across different webpages. Therefore I am creating a citation key, which includes the Last name of the first researcher + year of presenting that paper. Along with this, I am describing the title of the paper and where it was presented. If you find a particular title interesting for your work you can search that paper on “google scholar”, Mendeley, sci-hub or other places with which you are familiar and comfortable. Post that you can download and read that paper at your leisure. Hope you find this list of some use for your work."><meta itemprop=datePublished content="2023-08-22T00:00:00+00:00"><meta itemprop=dateModified content="2023-08-22T00:00:00+00:00"><meta itemprop=wordCount content="5065"><meta itemprop=keywords content="AI Research Papers,Machine Learning Papers,Deep Learning Research,AI Academic Resources,Research Collection,ML Publications,AI Development Papers,Scientific Literature"><meta name=twitter:card content="summary"><meta name=twitter:title content="Important AI Paper List"><meta name=twitter:description content="Important AI Paper List Introduciton In almost all citations it becomes very difficult to read the title of research papers. Why? Because the contributors’ information is first and most of the time, it is difficult to read the name other than native people. For example, if an Indian find a native name like “Vivek Ramaswami, Kartikeyan Karunanidhi” it is easy for them to read the name but the same name becomes difficult to read for non-Indian people, and vice-versa. Giving respect to the creator is very important but more than we need to know what have they done. I know from my experience, for almost every researcher, it becomes very difficult to track good AI research papers. For me, it is more difficult because I need to maintain this blog and I want to give references to the work across different webpages. Therefore I am creating a citation key, which includes the Last name of the first researcher + year of presenting that paper. Along with this, I am describing the title of the paper and where it was presented. If you find a particular title interesting for your work you can search that paper on “google scholar”, Mendeley, sci-hub or other places with which you are familiar and comfortable. Post that you can download and read that paper at your leisure. Hope you find this list of some use for your work."><link rel=preload href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css as=style integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css rel=stylesheet integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link rel=stylesheet type=text/css href=https://localhost:1313/css/asciinema-player.css><script src=https://code.jquery.com/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><link rel=stylesheet href=https://localhost:1313/css/custom.css><script src=https://localhost:1313/js/lunr.js></script></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-light nav-shadow flex-column flex-md-row td-navbar"><a id=agones-top class=navbar-brand href=https://localhost:1313/><svg id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg> <span class="text-uppercase fw-bold">Blowfish</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=https://localhost:1313/><span class=active>Data Science</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Docs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/community/><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/shortcodes/><span>Short Codes</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/blog/><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://github.com/nunocoracao/blowfish><span></span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href>GitHub</a></li><li class="nav-item dropdown d-none d-lg-block"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Release</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://development.agones.dev>Development</a>
<a class=dropdown-item href=https://agones.dev></a><a class=dropdown-item href=https://1-47-0.agones.dev>1.47.0</a>
<a class=dropdown-item href=https://1-46-0.agones.dev>1.46.0</a>
<a class=dropdown-item href=https://1-45-0.agones.dev>1.45.0</a>
<a class=dropdown-item href=https://1-44-0.agones.dev>1.44.0</a>
<a class=dropdown-item href=https://1-43-0.agones.dev>1.43.0</a>
<a class=dropdown-item href=https://1-42-0.agones.dev>1.42.0</a>
<a class=dropdown-item href=https://1-41-0.agones.dev>1.41.0</a>
<a class=dropdown-item href=https://1-40-0.agones.dev>1.40.0</a>
<a class=dropdown-item href=https://1-39-0.agones.dev>1.39.0</a>
<a class=dropdown-item href=https://1-38-0.agones.dev>1.38.0</a>
<a class=dropdown-item href=https://1-37-0.agones.dev>1.37.0</a>
<a class=dropdown-item href=https://1-36-0.agones.dev>1.36.0</a>
<a class=dropdown-item href=https://1-35-0.agones.dev>1.35.0</a>
<a class=dropdown-item href=https://1-34-0.agones.dev>1.34.0</a>
<a class=dropdown-item href=https://1-33-0.agones.dev>1.33.0</a>
<a class=dropdown-item href=https://1-32-0.agones.dev>1.32.0</a>
<a class=dropdown-item href=https://1-31-0.agones.dev>1.31.0</a></div></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><ul class=dropdown-menu><li><a class=dropdown-item href=https://localhost:1313/it/>Italiano</a></li></ul></div></li></ul></div><div class="navbar-nav mx-lg-2 d-none d-lg-block"></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><div class=row><div class=col-md-2></div><div class=col-md-8><p><img src=https://localhost:1313/assets/images/dspost/dsp6090-rps-Important-AI-Paper-List.jpg alt="Important AI Paper List"></p><h1 id=important-ai-paper-list>Important AI Paper List<a class=td-heading-self-link href=#important-ai-paper-list aria-label="Heading self-link"></a></h1><h2 id=introduciton>Introduciton<a class=td-heading-self-link href=#introduciton aria-label="Heading self-link"></a></h2><p>In almost all citations it becomes very difficult to read the title of research papers. Why? Because the contributors&rsquo; information is first and most of the time, it is difficult to read the name other than native people. For example, if an Indian find a native name like &ldquo;Vivek Ramaswami, Kartikeyan Karunanidhi&rdquo; it is easy for them to read the name but the same name becomes difficult to read for non-Indian people, and vice-versa. Giving respect to the creator is very important but more than we need to know what have they done. I know from my experience, for almost every researcher, it becomes very difficult to track good AI research papers. For me, it is more difficult because I need to maintain this blog and I want to give references to the work across different webpages. Therefore I am creating a citation key, which includes the Last name of the first researcher + year of presenting that paper. Along with this, I am describing the title of the paper and where it was presented. If you find a particular title interesting for your work you can search that paper on &ldquo;google scholar&rdquo;, Mendeley, sci-hub or other places with which you are familiar and comfortable. Post that you can download and read that paper at your leisure. Hope you find this list of some use for your work.</p><h2 id=citations>Citations<a class=td-heading-self-link href=#citations aria-label="Heading self-link"></a></h2><blockquote><p><strong>Pretrained Language Models for Text Generation: A Survey</strong></p></blockquote><h3 id=bahdanau2015>[Bahdanau2015]<a class=td-heading-self-link href=#bahdanau2015 aria-label="Heading self-link"></a></h3><p>Neural machine translation by jointly learning to align and translate. In ICLR, 2015.</p><h3 id=bao2020>[Bao2020]<a class=td-heading-self-link href=#bao2020 aria-label="Heading self-link"></a></h3><p>PLATO-2: towards building an open- domain chatbot via curriculum learning. arXiv preprint arXiv:2006.16779, 2020.</p><h3 id=brown2020>[Brown2020]<a class=td-heading-self-link href=#brown2020 aria-label="Heading self-link"></a></h3><p>Language models are few-shot learners. In NeurIPS, 2020.</p><h3 id=chen2020a>[Chen2020a]<a class=td-heading-self-link href=#chen2020a aria-label="Heading self-link"></a></h3><p>Distilling knowledge learned in BERT for text generation. In ACL, 2020.</p><h3 id=chen2020b>[Chen2020b]<a class=td-heading-self-link href=#chen2020b aria-label="Heading self-link"></a></h3><p>Few-shot NLG with pre-trained language model. In ACL, 2020.</p><h3 id=conneau2019>[Conneau2019]<a class=td-heading-self-link href=#conneau2019 aria-label="Heading self-link"></a></h3><p>Cross-lingual language model pretraining. In NeurIPS, 2019.</p><h3 id=devlin2019>[Devlin2019]<a class=td-heading-self-link href=#devlin2019 aria-label="Heading self-link"></a></h3><p>BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 2019.</p><h3 id=dong2019>[Dong2019]<a class=td-heading-self-link href=#dong2019 aria-label="Heading self-link"></a></h3><p>Unified language model pretraining for natural language understanding and generation. In NeurIPS, 2019.</p><h3 id=fan2019>[Fan2019]<a class=td-heading-self-link href=#fan2019 aria-label="Heading self-link"></a></h3><p>Unsupervised pre-training for sequence to sequence speech recognition. CoRR, arXiv preprint arXiv:1910.12418, 2019.</p><h3 id=gehring2017>[Gehring2017]<a class=td-heading-self-link href=#gehring2017 aria-label="Heading self-link"></a></h3><p>Convolutional sequence to sequence learning. In ICML, 2017.</p><h3 id=gong2020>[Gong2020]<a class=td-heading-self-link href=#gong2020 aria-label="Heading self-link"></a></h3><p>Tablegpt: Few-shot table-to-text generation with table structure reconstruction and content matching. In COLING, 2020.</p><h3 id=gu2020>[Gu2020]<a class=td-heading-self-link href=#gu2020 aria-label="Heading self-link"></a></h3><p>A tailored pre-training model for task-oriented dialog generation. arXiv preprint arXiv:2004.13835, 2020.</p><h3 id=guan2020>[Guan2020]<a class=td-heading-self-link href=#guan2020 aria-label="Heading self-link"></a></h3><p>Survey on automatic text summarization and transformer models applicability. In CCRIS, 2020.</p><h3 id=hendrycks2020>[Hendrycks2020]<a class=td-heading-self-link href=#hendrycks2020 aria-label="Heading self-link"></a></h3><p>Pretrained transformers improve out-of- distribution robustness. In ACL, 2020.</p><h3 id=keskar2019>[Keskar2019]<a class=td-heading-self-link href=#keskar2019 aria-label="Heading self-link"></a></h3><p>CTRL: A conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858, 2019.</p><h3 id=kryscinski2018>[Kryscinski2018]<a class=td-heading-self-link href=#kryscinski2018 aria-label="Heading self-link"></a></h3><p>Improving abstraction in text summarization. In EMNLP, 2018.</p><h3 id=lan2020>[Lan2020]<a class=td-heading-self-link href=#lan2020 aria-label="Heading self-link"></a></h3><p>ALBERT: A lite BERT for self-supervised learning of language representations. In ICLR, 2020.</p><h3 id=lewis2020>[Lewis2020]<a class=td-heading-self-link href=#lewis2020 aria-label="Heading self-link"></a></h3><p>BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In ACL, 2020.</p><h3 id=li2019>[Li2019]<a class=td-heading-self-link href=#li2019 aria-label="Heading self-link"></a></h3><p>Generating long and informative reviews with aspect-aware coarse-to-fine decoding. In ACL, pages 1969–1979, 2019.</p><h3 id=li2020>[Li2020]<a class=td-heading-self-link href=#li2020 aria-label="Heading self-link"></a></h3><p>Knowledge-enhanced personalized review generation with capsule graph neural network. In CIKM, pages 735–744, 2020.</p><h3 id=li2021a>[Li2021a]<a class=td-heading-self-link href=#li2021a aria-label="Heading self-link"></a></h3><p>TextBox: A unified, modularized, and extensible framework for text generation. In ACL, 2021.</p><h3 id=li2021b>[Li2021b]<a class=td-heading-self-link href=#li2021b aria-label="Heading self-link"></a></h3><p>Few-shot knowledge graph-to-text generation with pretrained language models. In Findings of ACL, 2021.</p><h3 id=li2021c>[Li2021c]<a class=td-heading-self-link href=#li2021c aria-label="Heading self-link"></a></h3><p>Knowledge-based review generation by coherence enhanced text planning. In SIGIR, 2021.</p><h3 id=lin2020>[Lin2020]<a class=td-heading-self-link href=#lin2020 aria-label="Heading self-link"></a></h3><p>Pretraining multilingual neural machine translation by leveraging alignment information. In EMNLP, 2020.</p><h3 id=liu2019>[Liu2019]<a class=td-heading-self-link href=#liu2019 aria-label="Heading self-link"></a></h3><p>Text summarization with pretrained encoders. In EMNLP, 2019.</p><h3 id=mager2020>[Mager2020]<a class=td-heading-self-link href=#mager2020 aria-label="Heading self-link"></a></h3><p>GPT-too: A language-model-first approach for AMR-to-text generation. In ACL, 2020.</p><h3 id=peters2018>[Peters2018]<a class=td-heading-self-link href=#peters2018 aria-label="Heading self-link"></a></h3><p>Deep contextualized word representations. In NAACL-HLT, 2018.</p><h3 id=qiu2020>[Qiu2020]<a class=td-heading-self-link href=#qiu2020 aria-label="Heading self-link"></a></h3><p>Pre-trained models for natural language processing: A survey. arXiv preprint arXiv:2003.08271, 2020.</p><h3 id=radford2019>[Radford2019]<a class=td-heading-self-link href=#radford2019 aria-label="Heading self-link"></a></h3><p>Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p><h3 id=raffel2020>[Raffel2020]<a class=td-heading-self-link href=#raffel2020 aria-label="Heading self-link"></a></h3><p>Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 2020.</p><h3 id=ribeiro2020>[Ribeiro2020]<a class=td-heading-self-link href=#ribeiro2020 aria-label="Heading self-link"></a></h3><p>Investigating pretrained language models for graph-to-text generation. arXiv preprint arXiv:2007.08426, 2020.</p><h3 id=ross-2012>[Ross, 2012]<a class=td-heading-self-link href=#ross-2012 aria-label="Heading self-link"></a></h3><p>Guide for conducting risk assessments. In NIST Special Publication, 2012.</p><h3 id=rothe2020>[Rothe2020]<a class=td-heading-self-link href=#rothe2020 aria-label="Heading self-link"></a></h3><p>Leveraging pre-trained checkpoints for sequence generation tasks. TACL, 2020.</p><h3 id=sanh2019>[Sanh2019]<a class=td-heading-self-link href=#sanh2019 aria-label="Heading self-link"></a></h3><p>Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.</p><h3 id=see2017>[See2017]<a class=td-heading-self-link href=#see2017 aria-label="Heading self-link"></a></h3><p>Get to the point: Summarization with pointer-generator networks. In ACL, 2017.</p><h3 id=song2019>[Song2019]<a class=td-heading-self-link href=#song2019 aria-label="Heading self-link"></a></h3><p>MASS: masked sequence to sequence pre-training for language generation. In ICML, 2019.</p><h3 id=sun2019a>[Sun2019a]<a class=td-heading-self-link href=#sun2019a aria-label="Heading self-link"></a></h3><p>Contrastive bidirectional transformer for temporal representation learning. arXiv preprint arXiv:1906.05743, 2019.</p><h3 id=sun2019b>[Sun2019b]<a class=td-heading-self-link href=#sun2019b aria-label="Heading self-link"></a></h3><p>Videobert: A joint model for video and language representation learning. In ICCV, 2019.</p><h3 id=vaswani2017>[Vaswani2017]<a class=td-heading-self-link href=#vaswani2017 aria-label="Heading self-link"></a></h3><p>Attention is all you need. In NIPS, 2017.</p><h3 id=wada2018>[Wada2018]<a class=td-heading-self-link href=#wada2018 aria-label="Heading self-link"></a></h3><p>Unsupervised cross-lingual word embedding by multilingual neural language models. arXiv preprint arXiv:1809.02306, 2018.</p><h3 id=wolf2019>[Wolf2019]<a class=td-heading-self-link href=#wolf2019 aria-label="Heading self-link"></a></h3><p>Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv preprint arXiv:1901.08149, 2019.</p><h3 id=xia2020>[Xia2020]<a class=td-heading-self-link href=#xia2020 aria-label="Heading self-link"></a></h3><p>XGPT: cross-modal generative pre-training for image captioning. arXiv preprint arXiv:2003.01473, 2020.</p><h3 id=xu2020a>[Xu2020a]<a class=td-heading-self-link href=#xu2020a aria-label="Heading self-link"></a></h3><p>Discourse-aware neural extractive text summarization. In ACL, 2020.</p><h3 id=xu2020b>[Xu2020b]<a class=td-heading-self-link href=#xu2020b aria-label="Heading self-link"></a></h3><p>Unsupervised extractive summarization by pre-training hierarchical transformers. In EMNLP, 2020.</p><h3 id=yang2020a>[Yang2020a]<a class=td-heading-self-link href=#yang2020a aria-label="Heading self-link"></a></h3><p>CSP: code-switching pre-training for neural machine translation. In EMNLP, 2020.</p><h3 id=yang2020b>[Yang2020b]<a class=td-heading-self-link href=#yang2020b aria-label="Heading self-link"></a></h3><p>TED: A pretrained unsupervised summarization model with theme modeling and denoising. In EMNLP (Findings), 2020.</p><h3 id=zaib2020>[Zaib2020]<a class=td-heading-self-link href=#zaib2020 aria-label="Heading self-link"></a></h3><p>A short survey of pre-trained language models for conversational AI-A new age in NLP. In ACSW, 2020.</p><h3 id=zeng2020>[Zeng2020]<a class=td-heading-self-link href=#zeng2020 aria-label="Heading self-link"></a></h3><p>Generalized conditioned dialogue generation based on pre-trained language model. arXiv preprint arXiv:2010.11140, 2020.</p><h3 id=zhang2019a>[Zhang2019a]<a class=td-heading-self-link href=#zhang2019a aria-label="Heading self-link"></a></h3><p>Pretraining-based natural language generation for text summarization. In CoNLL, 2019.</p><h3 id=zhang2019b>[Zhang2019b]<a class=td-heading-self-link href=#zhang2019b aria-label="Heading self-link"></a></h3><p>HIBERT: document level pre-training of hierarchical bidirectional transformers for document summarization. In ACL, 2019.</p><h3 id=zhang2019c>[Zhang2019c]<a class=td-heading-self-link href=#zhang2019c aria-label="Heading self-link"></a></h3><p>ERNIE: enhanced language representation with informative entities. In ACL, 2019.</p><h3 id=zhang2020>[Zhang2020]<a class=td-heading-self-link href=#zhang2020 aria-label="Heading self-link"></a></h3><p>DIALOGPT : Largescale generative pre-training for conversational response generation. In ACL, 2020.</p><h3 id=zhao2020>[Zhao2020]<a class=td-heading-self-link href=#zhao2020 aria-label="Heading self-link"></a></h3><p>Knowledge-grounded dialogue generation with pretrained language models. In EMNLP, 2020.</p><h3 id=zheng2019>[Zheng2019]<a class=td-heading-self-link href=#zheng2019 aria-label="Heading self-link"></a></h3><p>Sentence centrality revisited for unsupervised summarization. In ACL, 2019.</p><h3 id=zhou2020>[Zhou2020]<a class=td-heading-self-link href=#zhou2020 aria-label="Heading self-link"></a></h3><p>Unified vision-language pre-training for image captioning and VQA. In AAAI, 2020</p><blockquote><p><strong>Survey on Automatic Text Summarization and Transformer Models Applicability</strong></p></blockquote><h3 id=cohana2018>[CohanA2018]<a class=td-heading-self-link href=#cohana2018 aria-label="Heading self-link"></a></h3><p>A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents. In Proceedings of the 2018 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies. 615–621.</p><h3 id=nenkovaa2007>[NenkovaA2007]<a class=td-heading-self-link href=#nenkovaa2007 aria-label="Heading self-link"></a></h3><p>The pyramid method: Incorporating human content selection variation in summarization evaluation. ACM Transactions on Speech and Language Processing 4, 2 (2007).</p><h3 id=radforda>[RadfordA]<a class=td-heading-self-link href=#radforda aria-label="Heading self-link"></a></h3><p>Improving language understanding by generative pre-training. <a href=https://www.cs.ubc.ca/~amuham01/LING530/>www.cs.ubc.ca/~amuham01/LING530/</a> papers/radford2018improving.pdf</p><h3 id=rasimma2013>[RasimMA2013]<a class=td-heading-self-link href=#rasimma2013 aria-label="Heading self-link"></a></h3><p>Multiple documents summarization based on evolutionary optimization algorithm. Expert Systems with Applications 40, 5 (2013), 1675–1689.</p><h3 id=rasimma>[RasimMA]<a class=td-heading-self-link href=#rasimma aria-label="Heading self-link"></a></h3><p>MCMR: Maximum coverage and minimum redundant text summarization model. Expert Systems with Applications 38, 12 (2011), 14514–14522.</p><h3 id=vaswania2017>[VaswaniA2017]<a class=td-heading-self-link href=#vaswania2017 aria-label="Heading self-link"></a></h3><p>Attention is all you need. Advances in neural information processing systems (2017), 5998–6008.</p><h3 id=raffelc2019>[RaffelC2019]<a class=td-heading-self-link href=#raffelc2019 aria-label="Heading self-link"></a></h3><p>Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv:1910.10683 (2019).</p><h3 id=bahdanaud2014>[BahdanauD2014]<a class=td-heading-self-link href=#bahdanaud2014 aria-label="Heading self-link"></a></h3><p>Neural machine translation by jointly learning to align and translate. arXiv:1409.0473 (2014).</p><h3 id=gunese2004>[GunesE2004]<a class=td-heading-self-link href=#gunese2004 aria-label="Heading self-link"></a></h3><p>LexRank: Graph-based lexical centrality as salience in text summarization. Journal ofArtificial Intelligence 20, 1 (2004), 457–479.</p><h3 id=zhangh>[ZhangH]<a class=td-heading-self-link href=#zhangh aria-label="Heading self-link"></a></h3><p>Pretraining-Based Natural Language Generation for Text Summarization. In Proceedings ofthe 23rd Conference on Computational Natural Language Learning (CoNLL). 789–797.</p><h3 id=devlinj2019>[DevlinJ2019]<a class=td-heading-self-link href=#devlinj2019 aria-label="Heading self-link"></a></h3><p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings ofthe 2019Conference ofthe NorthAmerican ChapteroftheAssociation forComputational Linguistics: Human Language Technologies. 4171–4186.</p><h3 id=howardj>[HowardJ]<a class=td-heading-self-link href=#howardj aria-label="Heading self-link"></a></h3><p>Universal Language Model Fine-tuning for Text Classification. In Proceedings ofthe 56th Annual Meeting ofthe Association for Computational Linguistics. 328–339.</p><h3 id=zhangj2019>[ZhangJ2019]<a class=td-heading-self-link href=#zhangj2019 aria-label="Heading self-link"></a></h3><p>PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization. arXiv:1912.08777 (2019).</p><h3 id=kaikhahk>[KaikhahK]<a class=td-heading-self-link href=#kaikhahk aria-label="Heading self-link"></a></h3><p>Text summarization using neural networks. In Proceeding of second conference on intelligent system. 40–44.</p><h3 id=xuk>[XuK]<a class=td-heading-self-link href=#xuk aria-label="Heading self-link"></a></h3><p>Show, attend and tell: Neural image caption generation with visual attention. In Proceedings ofthe International conference on machine learning. 2048–2057.</p><h3 id=chin-yewl>[Chin-YewL]<a class=td-heading-self-link href=#chin-yewl aria-label="Heading self-link"></a></h3><p>ROUGE: A package for automatic evaluation of summaries. In Proceedings ofACL Workshop “Text Summarization Branches Out”. 8.</p><h3 id=m2019>[M2019]<a class=td-heading-self-link href=#m2019 aria-label="Heading self-link"></a></h3><p>BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv:1910.13461 (2019).</p><h3 id=ch2011>[Ch2011]<a class=td-heading-self-link href=#ch2011 aria-label="Heading self-link"></a></h3><p>A statistical approach for automatic text summarization by extraction. In Proceedings of2011 International Conference on Communication Systems and Network Technologies. 268–271.</p><h3 id=conroyjm>[ConroyJM]<a class=td-heading-self-link href=#conroyjm aria-label="Heading self-link"></a></h3><p>Text Summarization via Hidden Markov Models. In Proceedings ofthe 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. 406–407.</p><h3 id=petersm2018>[PetersM2018]<a class=td-heading-self-link href=#petersm2018 aria-label="Heading self-link"></a></h3><p>Deep Contextualized Word Representations. In Proceedings ofthe 2018 Conference ofthe North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies. 2227–2237.</p><h3 id=rusham2015>[RushAM2015]<a class=td-heading-self-link href=#rusham2015 aria-label="Heading self-link"></a></h3><p>A neural attention model for abstractive sentence summarization. arXiv:1509.00685 (2015).</p><h3 id=vinyalso2015>[VinyalsO2015]<a class=td-heading-self-link href=#vinyalso2015 aria-label="Heading self-link"></a></h3><p>Pointer networks. Advances in neural information processing systems (2015), 2692–2700.</p><h3 id=dragomirrr2004>[DragomirRR2004]<a class=td-heading-self-link href=#dragomirrr2004 aria-label="Heading self-link"></a></h3><p>Centroid-based summarization of multiple documents. Information Processing & Management 40, 6 (2004), 919–938.</p><h3 id=mihalcear2004>[MihalceaR2004]<a class=td-heading-self-link href=#mihalcear2004 aria-label="Heading self-link"></a></h3><p>Textrank: Bringing order into text. In Proceedings of the 2004 conference on empirical methods in natural language processing. 404–411.</p><h3 id=nallapatir2016>[NallapatiR2016]<a class=td-heading-self-link href=#nallapatir2016 aria-label="Heading self-link"></a></h3><p>Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv:1602.06023 (2016).</p><h3 id=oakr2015>[OakR2015]<a class=td-heading-self-link href=#oakr2015 aria-label="Heading self-link"></a></h3><p>Extractive techniques for automatic document summarization: a survey. International Journal of Innovative Research in Computer and Communication Engineering 4, 3 (2016), 4158–4164.</p><h3 id=parkerr2011>[ParkerR2011]<a class=td-heading-self-link href=#parkerr2011 aria-label="Heading self-link"></a></h3><p>English Gigaword. <a href=https://catalog.ldc.upenn.edu/LDC2011T07>https://catalog.ldc.upenn.edu/LDC2011T07</a></p><h3 id=chopras2016>[ChopraS2016]<a class=td-heading-self-link href=#chopras2016 aria-label="Heading self-link"></a></h3><p>Abstractive sentence summarization with attentive recurrent neural networks. In Proceedings ofthe 2016 Conference ofthe North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies. 93–98.</p><h3 id=evans2008>[EvanS2008]<a class=td-heading-self-link href=#evans2008 aria-label="Heading self-link"></a></h3><p>The New York Times Annotated Corpus. <a href=https://catalog.ldc.upenn>https://catalog.ldc.upenn</a>. edu/LDC2008T19</p><h3 id=edunovs2019>[EdunovS2019]<a class=td-heading-self-link href=#edunovs2019 aria-label="Heading self-link"></a></h3><p>Pre-trained language model representations for language generation. In Proceedings ofthe 2019 Conference ofthe North American Chapter ofthe Association for Computational Linguistics. 4052–4059.</p><h3 id=narayans2018>[NarayanS2018]<a class=td-heading-self-link href=#narayans2018 aria-label="Heading self-link"></a></h3><p>Pretraining-Based Natural Language Generation for Text Summarization. In Proceedings ofthe 2018 Conference on Empirical Methods in Natural Language Processing. 1797–1807.</p><h3 id=peterj2017>[PeterJ2017]<a class=td-heading-self-link href=#peterj2017 aria-label="Heading self-link"></a></h3><p>Get to the point: Summarization with pointer-generator networks. arXiv:1704.04368 (2017).</p><h3 id=guptav2010>[GuptaV2010]<a class=td-heading-self-link href=#guptav2010 aria-label="Heading self-link"></a></h3><p>A Survey of Text Summarization Extractive Techniques. Journal ofEmerging Technologies in Web Intelligence 2, 3 (2010), 258–268.</p><h3 id=sanhv2019>[SanhV2019]<a class=td-heading-self-link href=#sanhv2019 aria-label="Heading self-link"></a></h3><p>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arxiv.org/pdf/1910.01108 (2019).</p><h3 id=liuy2019>[LiuY2019]<a class=td-heading-self-link href=#liuy2019 aria-label="Heading self-link"></a></h3><p>Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692 (2019).</p><h3 id=yany2020>[YanY2020]<a class=td-heading-self-link href=#yany2020 aria-label="Heading self-link"></a></h3><p>ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training. arXiv:2001.04063 (2020).</p><h3 id=daiz>[DaiZ]<a class=td-heading-self-link href=#daiz aria-label="Heading self-link"></a></h3><p>Transformer- XL: Attentive Language Models beyond a Fixed-Length Context. In Proceedings ofthe 57th Annual Meeting ofthe Association for Computational Linguistics. 2978–2988.</p><h3 id=lanz2019>[LanZ2019]<a class=td-heading-self-link href=#lanz2019 aria-label="Heading self-link"></a></h3><p>Albert: A lite bert for self-supervised learning of language representations. arXiv:1909.11942 (2019).</p><h3 id=yangz2019>[YangZ2019]<a class=td-heading-self-link href=#yangz2019 aria-label="Heading self-link"></a></h3><p>Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems (2019), 5754–5764.</p><blockquote><p><strong>CTRL: A Conditional Transformer Language Model For Controllable Generation</strong></p></blockquote><h3 id=mart2016>[Mart2016]<a class=td-heading-self-link href=#mart2016 aria-label="Heading self-link"></a></h3><p>Tensorflow: A system for large-scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and Imple-mentation ({OSDI} 16), pp. 265–283, 2016.</p><h3 id=rohan2019>[Rohan2019]<a class=td-heading-self-link href=#rohan2019 aria-label="Heading self-link"></a></h3><p>Memory-efficient adaptive optimiza-tion for large-scale learning. arXiv preprint arXiv:1901.11150, 2019.</p><h3 id=martin2017>[Martin2017]<a class=td-heading-self-link href=#martin2017 aria-label="Heading self-link"></a></h3><p>Wasserstein generative adversarial networks. ´In International conference on machine learning, pp. 214–223, 2017.</p><h3 id=matthew2017>[Matthew2017]<a class=td-heading-self-link href=#matthew2017 aria-label="Heading self-link"></a></h3><p>Factsheets: Increasing trust in AI servicesthrough supplier’s declarations of conformity, August 2018. arXiv:1808.07261 [cs.CY].Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural machinetranslation. arXiv preprint arXiv:1710.11041, 2017.</p><h3 id=jimmy2016>[Jimmy2016]<a class=td-heading-self-link href=#jimmy2016 aria-label="Heading self-link"></a></h3><p>Layer normalization. CoRR, abs/1607.06450,2016.</p><h3 id=lo2019>[Lo2019]<a class=td-heading-self-link href=#lo2019 aria-label="Heading self-link"></a></h3><p>Findings of the2019 conference on machine translation (wmt19). In Proceedings of the Fourth Conference onMachine Translation (Volume 2: Shared Task Papers, Day 1), pp. 1–61, 2019.</p><h3 id=yoshua2003>[Yoshua2003]<a class=td-heading-self-link href=#yoshua2003 aria-label="Heading self-link"></a></h3><p>A neural probabilistic ´language model. Journal of machine learning research, 3(Feb):1137–1155, 2003.</p><h3 id=thorsten2007>[Thorsten2007]<a class=td-heading-self-link href=#thorsten2007 aria-label="Heading self-link"></a></h3><p>Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods inNatural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),pp. 858–867, 2007.</p><h3 id=miles2016>[Miles2016]<a class=td-heading-self-link href=#miles2016 aria-label="Heading self-link"></a></h3><p>Artificial intelligence and responsible innovation. In Vincent C. Muller (ed.), ¨Fundamental Issues of Artificial Intelligence, pp. 543–554. Springer, 2016.</p><h3 id=miles2019>[Miles2019]<a class=td-heading-self-link href=#miles2019 aria-label="Heading self-link"></a></h3><p>The malicious use of artificial intelligence: Forecasting,prevention, and mitigation, February 2019. arXiv:1802.07228 [cs.AI].Isaac Caswell, Ciprian Chelba, and David Grangier. Tagged back-translation. arXiv preprintarXiv:1906.06442, 2019.</p><h3 id=xi2016>[Xi2016]<a class=td-heading-self-link href=#xi2016 aria-label="Heading self-link"></a></h3><p>Infogan:Interpretable representation learning by information maximizing generative adversarial nets. InAdvances in neural information processing systems, pp. 2172–2180, 2016.</p><h3 id=rewon2019>[Rewon2019]<a class=td-heading-self-link href=#rewon2019 aria-label="Heading self-link"></a></h3><p>Generating long sequences with sparsetransformers. arXiv preprint arXiv:1904.10509, 2019.</p><h3 id=ronan2008>[Ronan2008]<a class=td-heading-self-link href=#ronan2008 aria-label="Heading self-link"></a></h3><p>A unified architecture for natural language processing: Deepneural networks with multitask learning. In Proceedings of the 25th international conference onMachine learning, pp. 160–167. ACM, 2008.</p><h3 id=ronan2011>[Ronan2011]<a class=td-heading-self-link href=#ronan2011 aria-label="Heading self-link"></a></h3><p>Natural language processing (almost) from scratch. Journal of machine learning research,12(Aug):2493–2537, 2011.</p><h3 id=ruth1987>[Ruth1987]<a class=td-heading-self-link href=#ruth1987 aria-label="Heading self-link"></a></h3><p>The consumption junction: A proposal for research strategies in the sociol-ogy of technology. In Wiebe E. Bijker, Thomas P. Hughes, and Trevor J. Pinch (eds.), The SocialConstruction of Technological Systems, pp. 261–280. MIT Press, Cambridge, MA, USA, 1987.</p><h3 id=andrew2015>[Andrew2015]<a class=td-heading-self-link href=#andrew2015 aria-label="Heading self-link"></a></h3><p>Semi-supervised sequence learning. In Advances in neural infor-mation processing systems, pp. 3079–3087, 2015.</p><h3 id=zihang2019>[Zihang2019]<a class=td-heading-self-link href=#zihang2019 aria-label="Heading self-link"></a></h3><p>Transformer-xl: Attentive language models beyond a fixed-length context. arXivpreprint arXiv:1901.02860, 2019.</p><h3 id=jacob2018>[Jacob2018]<a class=td-heading-self-link href=#jacob2018 aria-label="Heading self-link"></a></h3><p>Bert: Pre-training of deepbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p><h3 id=john2011>[John2011]<a class=td-heading-self-link href=#john2011 aria-label="Heading self-link"></a></h3><p>Adaptive subgradient methods for online learning andstochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.</p><h3 id=matthew2017-1>[Matthew2017]<a class=td-heading-self-link href=#matthew2017-1 aria-label="Heading self-link"></a></h3><p>Searchqa: A new q&amp;a dataset augmented with context from a search engine. arXiv preprintarXiv:1704.05179, 2017.</p><h3 id=angela2018>[Angela2018]<a class=td-heading-self-link href=#angela2018 aria-label="Heading self-link"></a></h3><p>Hierarchical neural story generation. arXiv preprintarXiv:1805.04833, 2018.</p><h3 id=angela2019>[Angela2019]<a class=td-heading-self-link href=#angela2019 aria-label="Heading self-link"></a></h3><p>Eli5:Long form question answering. arXiv preprint arXiv:1907.09190, 2019.</p><h3 id=boris2019>[Boris2019]<a class=td-heading-self-link href=#boris2019 aria-label="Heading self-link"></a></h3><p>Stochastic gradient methods with layer-wise adaptive moments for training of deep networks. arXiv preprint arXiv:1905.11286, 2019.</p><h3 id=ian2014>[Ian2014]<a class=td-heading-self-link href=#ian2014 aria-label="Heading self-link"></a></h3><p>Generative adversarial nets. In Advances in neural infor-mation processing systems, pp. 2672–2680, 2014.</p><h3 id=max2016>[Max2016]<a class=td-heading-self-link href=#max2016 aria-label="Heading self-link"></a></h3><p>Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of the 2018 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies, pp.708–719, New Orleans, Louisiana, June 2018.</p><h3 id=kaiming2016>[Kaiming2016]<a class=td-heading-self-link href=#kaiming2016 aria-label="Heading self-link"></a></h3><p>Deep residual learning for image recog-nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.770–778, 2016.</p><h3 id=karl2015>[Karl2015]<a class=td-heading-self-link href=#karl2015 aria-label="Heading self-link"></a></h3><p>Teaching machines to read and comprehend. In Advances inneural information processing systems, pp. 1693–1701, 2015.</p><h3 id=ari2019>[Ari2019]<a class=td-heading-self-link href=#ari2019 aria-label="Heading self-link"></a></h3><p>The curious case of neural text degener-ation. arXiv preprint arXiv:1904.09751, 2019.</p><h3 id=jeremy2018>[Jeremy2018]<a class=td-heading-self-link href=#jeremy2018 aria-label="Heading self-link"></a></h3><p>Universal language model fine-tuning for text classification.arXiv preprint arXiv:1801.06146, 2018.</p><h3 id=hakan2016>[Hakan2016]<a class=td-heading-self-link href=#hakan2016 aria-label="Heading self-link"></a></h3><p>Tying word vectors and word classifiers: Aloss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016.</p><h3 id=melvin2017>[Melvin2017]<a class=td-heading-self-link href=#melvin2017 aria-label="Heading self-link"></a></h3><p>Googles multilingual neural ´machine translation system: Enabling zero-shot translation. Transactions of the Association forComputational Linguistics, 5:339–351, 2017.</p><h3 id=mandar2017>[Mandar2017]<a class=td-heading-self-link href=#mandar2017 aria-label="Heading self-link"></a></h3><p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.</p><h3 id=david2017>[David2017]<a class=td-heading-self-link href=#david2017 aria-label="Heading self-link"></a></h3><p>Self-censorship is not enough. Nature, 492(7429):345–347,December 2012. doi: 10.1038/492345a.</p><h3 id=lukasz2017>[Lukasz2017]<a class=td-heading-self-link href=#lukasz2017 aria-label="Heading self-link"></a></h3><p>One model to learn them all. arXiv preprint arXiv:1706.05137, 2017.</p><h3 id=łukasz2018>[Łukasz2018]<a class=td-heading-self-link href=#%c5%82ukasz2018 aria-label="Heading self-link"></a></h3><p>Fast decoding in sequence models using discrete latent variables. arXiv preprintarXiv:1803.03382, 2018.</p><h3 id=nitish2019>[Nitish2019]<a class=td-heading-self-link href=#nitish2019 aria-label="Heading self-link"></a></h3><p>Unifying questionanswering and text classification via span extraction. arXiv preprint arXiv:1904.09286, 2019.</p><h3 id=diederik2014>[Diederik2014]<a class=td-heading-self-link href=#diederik2014 aria-label="Heading self-link"></a></h3><p>Adam: A method for stochastic optimization. arXiv preprintarXiv:1412.6980, 2014.</p><h3 id=diederik2013>[Diederik2013]<a class=td-heading-self-link href=#diederik2013 aria-label="Heading self-link"></a></h3><p>Auto-encoding variational bayes. arXiv preprintarXiv:1312.6114, 2013.</p><h3 id=ryan2015>[Ryan2015]<a class=td-heading-self-link href=#ryan2015 aria-label="Heading self-link"></a></h3><p>Skip-thought vectors. In Advances in neural information processingsystems, pp. 3294–3302, 2015.</p><h3 id=catherine2016>[Catherine2016]<a class=td-heading-self-link href=#catherine2016 aria-label="Heading self-link"></a></h3><p>Senellart. Domain control for neural machine translation.arXiv preprint arXiv:1612.06140, 2016.</p><h3 id=wojciech2019>[Wojciech2019]<a class=td-heading-self-link href=#wojciech2019 aria-label="Heading self-link"></a></h3><p>Neural text summarization: A critical evaluation. arXiv preprint arXiv:1908.08960, 2019.</p><h3 id=tom2019>[Tom2019]<a class=td-heading-self-link href=#tom2019 aria-label="Heading self-link"></a></h3><p>Natural questions: abenchmark for question answering research. Transactions of the Association for ComputationalLinguistics, 7:453–466, 2019.</p><h3 id=guillaume2019>[Guillaume2019]<a class=td-heading-self-link href=#guillaume2019 aria-label="Heading self-link"></a></h3><p>Cross-lingual language model pretraining. arXiv preprintarXiv:1901.07291, 2019.</p><h3 id=guillaume2019-1>[Guillaume2019]<a class=td-heading-self-link href=#guillaume2019-1 aria-label="Heading self-link"></a></h3><p>Large memory layers with product keys. ´ arXiv preprint arXiv:1907.05242, 2019.</p><h3 id=hector2012>[Hector2012]<a class=td-heading-self-link href=#hector2012 aria-label="Heading self-link"></a></h3><p>The winograd schema challenge. In Thir-teenth International Conference on the Principles of Knowledge Representation and Reasoning,2012.</p><h3 id=patrick2019>[Patrick2019]<a class=td-heading-self-link href=#patrick2019 aria-label="Heading self-link"></a></h3><p>Unsupervised question answering by clozetranslation. arXiv preprint arXiv:1906.04980, 2019.</p><h3 id=minh-thang2015>[Minh-Thang2015]<a class=td-heading-self-link href=#minh-thang2015 aria-label="Heading self-link"></a></h3><p>Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.</p><h3 id=julian2015>[Julian2015]<a class=td-heading-self-link href=#julian2015 aria-label="Heading self-link"></a></h3><p>Image-based rec-ommendations on styles and substitutes. In Proceedings of the 38th International ACM SIGIRConference on Research and Development in Information Retrieval, pp. 43–52. ACM, 2015.</p><h3 id=bryan6294>[Bryan6294]<a class=td-heading-self-link href=#bryan6294 aria-label="Heading self-link"></a></h3><p>Learned in translation:Contextualized word vectors. In Advances in Neural Information Processing Systems, pp. 6294.</p><h3 id=bryan2018>[Bryan2018]<a class=td-heading-self-link href=#bryan2018 aria-label="Heading self-link"></a></h3><p>The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018.</p><h3 id=15stephen2017>[15Stephen2017]<a class=td-heading-self-link href=#15stephen2017 aria-label="Heading self-link"></a></h3><p>Regularizing and optimizing lstm lan-guage models. arXiv preprint arXiv:1708.02182, 2017.</p><h3 id=tomas2013>[Tomas2013]<a class=td-heading-self-link href=#tomas2013 aria-label="Heading self-link"></a></h3><p>Distributed represen-tations of words and phrases and their compositionality. In Advances in neural information pro-cessing systems, pp. 3111–3119, 2013.</p><h3 id=margaret7596>[Margaret7596]<a class=td-heading-self-link href=#margaret7596 aria-label="Heading self-link"></a></h3><p>Model cards for model reporting. InProceedings of the Conference on Fairness, Accountability, and Transparency (FAT* ’19), Jan-uary 2019. doi: 10.1145/3287560.3287596.</p><h3 id=amit2019>[Amit2019]<a class=td-heading-self-link href=#amit2019 aria-label="Heading self-link"></a></h3><p>Filling gender & number gaps in neural ma-chine translation with black-box context injection. arXiv preprint arXiv:1903.03467, 2019.</p><h3 id=vinod2010>[Vinod2010]<a class=td-heading-self-link href=#vinod2010 aria-label="Heading self-link"></a></h3><p>Rectified linear units improve restricted boltzmann machines. InProceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 807–814,2010.</p><h3 id=ramesh2016>[Ramesh2016]<a class=td-heading-self-link href=#ramesh2016 aria-label="Heading self-link"></a></h3><p>Abstractive text summarizationusing sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023, 2016.</p><h3 id=matthew2018>[Matthew2018]<a class=td-heading-self-link href=#matthew2018 aria-label="Heading self-link"></a></h3><p>Deep contextualized word representations. arXiv preprint arXiv:1802.05365,2018.</p><h3 id=carol1979>[Carol1979]<a class=td-heading-self-link href=#carol1979 aria-label="Heading self-link"></a></h3><p>Constraints on language mixing: intra sentential code-switching and borrowing inspanish/english. Language, pp. 291–318, 1979.</p><h3 id=shana1980>[Shana1980]<a class=td-heading-self-link href=#shana1980 aria-label="Heading self-link"></a></h3><p>Sometimes ill start a sentence in spanish y termino en espanol: toward a typologyof code-switching1. Linguistics, 18(7-8):581–618, 1980.</p><h3 id=ofir2016>[Ofir2016]<a class=td-heading-self-link href=#ofir2016 aria-label="Heading self-link"></a></h3><p>Using the output embedding to improve language models. arXiv preprintarXiv:1608.05859, 2016.</p><h3 id=alec2018>[Alec2018]<a class=td-heading-self-link href=#alec2018 aria-label="Heading self-link"></a></h3><p>Improving language under-standing by generative pre-training. URL <a href=https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/languageunsupervised/language>https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/languageunsupervised/language</a> understanding paper.pdf, 2018.</p><h3 id=alec2019>[Alec2019]<a class=td-heading-self-link href=#alec2019 aria-label="Heading self-link"></a></h3><p>Language models are unsupervised multitask learners. URLhttps://d4mucfpksywv.cloudfront.net/better-language-models/language models are unsupervised multitask learners.pdf, 2019.</p><h3 id=nazneen2019>[Nazneen2019]<a class=td-heading-self-link href=#nazneen2019 aria-label="Heading self-link"></a></h3><p>Explain yourself! leveraging language models for commonsense reasoning. arXiv preprint arXiv:1906.02361, 2019.</p><h3 id=pranav2016>[Pranav2016]<a class=td-heading-self-link href=#pranav2016 aria-label="Heading self-link"></a></h3><p>Squad: 100,000+ questionsfor machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.</p><h3 id=alexander2015>[Alexander2015]<a class=td-heading-self-link href=#alexander2015 aria-label="Heading self-link"></a></h3><p>A neural attention model for abstractivesentence summarization. arXiv preprint arXiv:1509.00685, 2015.</p><h3 id=evan2008>[Evan2008]<a class=td-heading-self-link href=#evan2008 aria-label="Heading self-link"></a></h3><p>The new york times annotated corpus. Linguistic Data Consortium, Philadelphia,6(12):e26752, 2008.</p><h3 id=thomas2019>[Thomas2019]<a class=td-heading-self-link href=#thomas2019 aria-label="Heading self-link"></a></h3><p>Answers unite!unsupervised metrics for reinforced summarization models. arXiv preprint arXiv:1909.01610,2019.</p><h3 id=abigail2017>[Abigail2017]<a class=td-heading-self-link href=#abigail2017 aria-label="Heading self-link"></a></h3><p>Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers), volume 1, pp. 1073–1083, 2017.</p><h3 id=rico2015>[Rico2015]<a class=td-heading-self-link href=#rico2015 aria-label="Heading self-link"></a></h3><p>Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.</p><h3 id=noam2018>[Noam2018]<a class=td-heading-self-link href=#noam2018 aria-label="Heading self-link"></a></h3><p>Adafactor: Adaptive learning rates with sublinear memory cost.arXiv preprint arXiv:1804.04235, 2018.</p><h3 id=jack008>[Jack008]<a class=td-heading-self-link href=#jack008 aria-label="Heading self-link"></a></h3><p>Developing a framework for responsible inno-vation. Research Policy, 42(9):1568–1580, November 2013. doi: 10.1016/j.respol.2013.05.008.</p><h3 id=ilya2014>[Ilya2014]<a class=td-heading-self-link href=#ilya2014 aria-label="Heading self-link"></a></h3><p>Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104–3112, 2014.</p><h3 id=trieu2018>[Trieu2018]<a class=td-heading-self-link href=#trieu2018 aria-label="Heading self-link"></a></h3><p>A simple method for commonsense reasoning. arXiv preprintarXiv:1806.02847, 2018.</p><h3 id=adam2016>[Adam2016]<a class=td-heading-self-link href=#adam2016 aria-label="Heading self-link"></a></h3><p>A machine comprehension dataset. arXiv preprint arXiv:1611.09830,2016.</p><h3 id=lav6008>[Lav6008]<a class=td-heading-self-link href=#lav6008 aria-label="Heading self-link"></a></h3><p>Pretrained AI models: Performativity,mobility, and change, September 2019. arXiv:1909.03290 [cs.CY].</p><h3 id=curran2018>[Curran2018]<a class=td-heading-self-link href=#curran2018 aria-label="Heading self-link"></a></h3><p>Glue:A multi-task benchmark and analysis platform for natural language understanding. arXiv preprintarXiv:1804.07461, 2018.</p><h3 id=sean2019>[Sean2019]<a class=td-heading-self-link href=#sean2019 aria-label="Heading self-link"></a></h3><p>Neural text generation with unlikelihood training. arXiv preprint arXiv:1908.04319, 2019.</p><h3 id=yonghui2016>[Yonghui2016]<a class=td-heading-self-link href=#yonghui2016 aria-label="Heading self-link"></a></h3><p>Google’s neural machine trans-lation system: Bridging the gap between human and machine translation. arXiv preprintarXiv:1609.08144, 2016.</p><h3 id=stratos2019>[Stratos2019]<a class=td-heading-self-link href=#stratos2019 aria-label="Heading self-link"></a></h3><p>Sumqe: a bert-based summary quality estimation model. arXiv preprint arXiv:1909.00578, 2019.</p><h3 id=zhilin2018>[Zhilin2018]<a class=td-heading-self-link href=#zhilin2018 aria-label="Heading self-link"></a></h3><p>Hotpotqa: A dataset for diverse, explainable multi-hop questionanswering. arXiv preprint arXiv:1809.09600, 2018.</p><h3 id=rowan-2019>[Rowan 2019]<a class=td-heading-self-link href=#rowan-2019 aria-label="Heading self-link"></a></h3><p>Defending against neural fake news. arXiv preprint arXiv:1905.12616, 2019</p><h3 id=fangxiaoyu2022>[Fangxiaoyu2022]<a class=td-heading-self-link href=#fangxiaoyu2022 aria-label="Heading self-link"></a></h3><p><a href=https://arxiv.org/abs/2007.01852>Language-agnostic BERT Sentence Embedding - LaBSE</a></p><ul><li>BERT is an effective method for learning monolingual sentence embeddings for semantic similarity and embedding based transfer learning</li><li>BERT based cross-lingual sentence embeddings is explored in this paper.</li><li>It explored combining the best methods for learning monolingual and cross-lingual representations including: masked language modeling (MLM), translation language modeling (TLM)</li><li>Introducing a pre-trained multilingual language model dramatically reduces the amount of parallel training data required to achieve good performance</li><li>It produces a model that achieves high bi-text retrieval accuracy over 112 languages</li></ul><h2 id=nlp-papers-available-on-my-google-drive>NLP Papers Available on my Google Drive<a class=td-heading-self-link href=#nlp-papers-available-on-my-google-drive aria-label="Heading self-link"></a></h2><p>You can download these papers from <a href="https://drive.google.com/drive/folders/1588WNZVCTRbOnsrSVKousaky7A59mfl9?usp=sharing">link</a></p><ol><li>A brief introduction to boosting.pdf</li><li>A Closer Look at Fermentors and Bioreactors.pdf</li><li>A Comprehensive Survey on Graph Neural Networks.pdf</li><li>A Corpus of English-Hindi Code-Mixed Tweets for Sarcasm Detection.pdf</li><li>A dataset for detecting irony in Hindi-english code-mixed social media text.pdf</li><li>A Framework for Document Specific Error Detection and Corrections in Indic OCR.pdf</li><li>A lexicon-based approach for hate speech detection.pdf</li><li>A method for multi-class sentiment classification based on an improved one-vs-one (OVO) strategy and the support vector machine (.pdf</li><li>A novel automatic satire and irony detection using ensembled feature selection and data mining.pdf</li><li>A Pragmatic Analysis Of Humor In Modern Family.pdf</li><li>A Selective Overview of Deep Learning.pdf</li><li>A Sentiment Analyzer for Hindi Using Hindi Senti Lexicon.pdf</li><li>A Survey of Code-switched Speech and Language Processing.pdf</li><li>A Survey of the State of Explainable AI for Natural Language Processing.pdf</li><li>A Survey on Explainable Artificial Intelligence (XAI) Toward Medical XAI.pdf</li><li>A TENGRAM method based part-of-speech tagging of multi-category words in Hindi language.pdf</li><li>A transformer-based approach to irony and sarcasm detection.pdf</li><li>A2Text-net A novel deep neural network for sarcasm detection.pdf</li><li>Adaptive glove and fasttext model for Hindi word embeddings.pdf</li><li>AI and Ethics - Operationalising Responsible AI-PAPER.pdf</li><li>AI4Bharat-IndicNLP Corpus Monolingual Corpora and Word Embeddings for Indic Languages.pdf</li><li>ALBERT A Lite BERT for Self-supervised Learning of Language Representations.pdf</li><li>an Analysis of Current Trends for Sanskrit As a Computer Programming Language.pdf</li><li>An empirical, quantitative analysis of the differences between sarcasm and Irony.pdf</li><li>An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf</li><li>Analyzing_The_Expressive_Power_Of_Graph.pdf</li><li>AnnCorra Annotating Corpora Guidelines For POS And Chunk Annotation For Indian Languages.pdf</li><li>Approaches to Cross-Domain Sentiment Analysis A Systematic Literature Review.pdf</li><li>Attention is all you need.pdf</li><li>Automatic sarcasm detection A survey.pdf</li><li>Automatic satire detection Are you having a laugh.pdf</li><li>Bag of tricks for efficient text classification.pdf</li><li>Baselines and bigrams Simple, good sentiment and topic classification.pdf</li><li>BERT Explained - A list of Frequently Asked Questions.pdf</li><li>BERT Pre-training of deep bidirectional transformers for language understanding.pdf</li><li>BHAAV- A Text Corpus for Emotion Analysis from Hindi Stories.pdf</li><li>Carer Contextualized affect representations for emotion recognition.pdf</li><li>CASCADE Contextual Sarcasm Detection in Online Discussion Forums.pdf</li><li>Challenges in Deploying Machine Learning a Survey of Case Studies.pdf</li><li>Clinical artificial intelligence quality improvement towards continual monitoring and updating of AI algorithms in healthcare.pdf</li><li>CLUE based load balancing in replicated web server.pdf</li><li>Clues for detecting irony in user-generated contents Oh&mldr;!! it_s so easy -).pdf</li><li>Code Mixing A Challenge for Language Identification in the Language of Social Media.pdf</li><li>Context-based Sarcasm Detection in Hindi Tweets.pdf</li><li>Contextualized sarcasm detection on twitter.pdf</li><li>Convolutional MKL Based Multimodal Emotion Recognition and Sentiment Analysis.pdf</li><li>Data governance A conceptual framework, structured review, and research agenda.pdf</li><li>Deep and Dense Sarcasm Detection.pdf</li><li>Deep learning based unsupervised POS tagging for Sanskrit.pdf</li><li>Detailed human avatars from monocular video.pdf</li><li>Detecting Sarcasm is Extremely Easy -).pdf</li><li>DIALOGPT Large-Scale Generative Pre-training for Conversational Response Generation.pdf</li><li>DistilBERT, a distilled version of BERT smaller, faster, cheaper and lighter.pdf</li><li>DRIFT Deep Reinforcement Learning for Functional Software Testing.pdf</li><li>Drop A reading comprehension benchmark requiring discrete reasoning over paragraphs.pdf</li><li>Dynamic routing between capsules.pdf</li><li>Effect of speech coding on speaker identification.pdf</li><li>Efficient estimation of word representations in vector space(2).pdf</li><li>ELECTRA Pre-training Text Encoders as Discriminators Rather Than Generators.pdf</li><li>Embedding Words as Distributions with a Bayesian Skip-gram Model.pdf</li><li>Enriching Word Vectors with Subword Information.pdf</li><li>Experience Grounds Language.pdf</li><li>Exploiting emojis for sarcasm detection.pdf</li><li>Exploiting Similarities among Languages for Machine Translation.pdf</li><li>Exploring the fine-grained analysis and automatic detection of irony on Twitter(2).pdf</li><li>Exploring the fine-grained analysis and automatic detection of irony on Twitter.pdf</li><li>Exploring the impact of pragmatic phenomena on irony detection in tweets A multilingual corpus study.pdf</li><li>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.pdf</li><li>Extensions to HMM-based Statistical Word Alignment Models.pdf</li><li>Fairness_In_Machine_Learning_A_Survey.pdf</li><li>Fake news detection of Indian and United States election data using machine learning algorithm.pdf</li><li>Fake News Detection on Social Media.pdf</li><li>FakeNewsNet A Data Repository with News Content, Social Context, and Spatiotemporal Information for Studying Fake News on Social.pdf</li><li>Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf</li><li>FastText.zip Compressing text classification models.pdf</li><li>Figurative messages and affect in Twitter Differences between #irony, #sarcasm and #not.pdf</li><li>Forecasting COVID-19 Confirmed Cases in Major Indian Cities and Their Connectedness with Mobility and Weather-related Parameters.pdf</li><li>From English To Foreign Languages Transferring Pre-trained Language Models.pdf</li><li>FROM Pre-trained Word Embeddings TO Pre-trained Language Models - Focus on BERT.pdf</li><li>Going deeper with convolutions.pdf</li><li>Graph Machine Learning NeurIPS 2020 Papers.pdf</li><li>Grouped Convolutional Neural Networks for Multivariate Time Series.pdf</li><li>Grouped Functional Time Series Forecasting An Application to Age-Specific Mortality Rates.pdf</li><li>Handbook of approximation algorithms and metaheuristics.pdf</li><li>Harnessing context incongruity for sarcasm detection.pdf</li><li>Harnessing Online News for Sarcasm Detection in Hindi Tweets.pdf</li><li>Hidden Markov Models.pdf</li><li>Hidden technical debt in machine learning systems.pdf</li><li>Hotpotqa A dataset for diverse, explainable multi-hop question answering.pdf</li><li>How multilingual is multilingual BERT.pdf</li><li>How to avoid machine learning pitfalls a guide for academic researchers.pdf</li><li>How to read a paper.pdf</li><li>HuggingFace_s Transformers State-of-the-art Natural Language Processing.pdf</li><li>Identifying machine learning techniques for classification of target advertising.pdf</li><li>Identifying sarcasm in Twitter A closer look.pdf</li><li>Improving Language Understanding by Generative Pre-Training.pdf</li><li>Improving the learnability of classifiers for Sanskrit OCR corrections.pdf</li><li>Indic sentiReview Natural language processing based sentiment analysis on major indian languages.pdf</li><li>Interactive-and-Visual-Prompt-Engineering-for-adhoc-Task-Adaptation-LLM.pdf</li><li>Investigations in computational sarcasm.pdf</li><li>Irony detection in twitter The role of affective content.pdf</li><li>Irony, Sarcasm and Parody in the American Sitcom Modern Family.pdf</li><li>iSarcasm A Dataset of Intended Sarcasm.pdf</li><li>K-means with Three different Distance Metrics.pdf</li><li>Knowledge Representation in Sanskrit and Artificial Intelligence.pdf</li><li>Learning Graph Search Heuristics.pdf</li><li>Learning latent causal graphs via mixture oracles.pdf</li><li>LearningSys_2015_paper_32.pdf</li><li>Lexicon-Based Methods for Sentiment Analysis.pdf</li><li>Lexicon-Based Sentiment Analysis in the Social Web.pdf</li><li>LightGBM A highly efficient gradient boosting decision tree.pdf</li><li>Linguistic Inquiry and Word Count LIWC2015.pdf</li><li>Machine Learning in Automated Text Categorization.pdf</li><li>Machine Learning within a Graph Database A Case Study on Link Prediction for Scholarly Data.pdf</li><li>Machine Translation Approaches and Survey for Indian Languages.pdf</li><li>Machine Translation of Bi-lingual Hindi-English (Hinglish) Text.pdf</li><li>Merlion A Machine Learning Library for Time Series.pdf</li><li>Mining of Massive Datasets.pdf</li><li>MLP-Mixer An all-MLP Architecture for Vision.pdf</li><li>Multi-modal sarcasm detection in Twitter with hierarchical fusion model.pdf</li><li>Multi-rule based ensemble feature selection model for sarcasm type detection in Twitter.pdf</li><li>Multimodal markers of irony and sarcasm.pdf</li><li>N Atural L Anguage I Nference Over.pdf</li><li>Natural Language Processing - A Panian Perspective.pdf</li><li>Natural language processing based features for sarcasm detection An investigation using bilingual social media texts.pdf</li><li>NeuralProphet Explainable Forecasting at Scale.pdf</li><li>On State-of-the-art of POS Tagger, Sandhi Splitter, Alankaar Finder and Samaas Finder for IndoAryan and Dravidian Languages.pdf</li><li>Opinion mining and sentiment analysis.pdf</li><li>Opinion-Based Entity Ranking (Author_s Draft).pdf</li><li>Part-of-speech tagging from 97_ to 100_ Is it time for some linguistics.pdf</li><li>PAVE Lazy-MDP based Ensemble to Improve Recall of Product Attribute Extraction Models.pdf</li><li>Real-time Sentiment Analysis of Hindi Tweets.pdf</li><li>Reasoning with sarcasm by reading in-between.pdf</li><li>Recent trends in deep learning based natural language processing Review Article.pdf</li><li>RECEPTIVE FIELDS OF SINGLE NEURONES IN THE CAT _ S STRIATE CORTEX.pdf</li><li>Recognition of consonant-vowel (CV) units under background noise using combined temporal and spectral preprocessing.pdf</li><li>Representing social media users for sarcasm detection.pdf</li><li>Retrospective Reader for Machine Reading Comprehension.pdf</li><li>RoBERTa A Robustly Optimized BERT Pretraining Approach.pdf</li><li>Robotics , AI , and.pdf</li><li>ROC graphs Notes and practical considerations for researchers.pdf</li><li>Sanskrit sandhi splitting using Seq2(Seq)22.pdf</li><li>Sanskrit word segmentation using character-level recurrent and convolutional neural networks.pdf</li><li>Sarc-M Sarcasm Detection in Typo-graphic Memes.pdf</li><li>Sarcasm as contrast between a positive sentiment and negative situation.pdf</li><li>Sarcasm Detection in Hindi sentences using Support Vector machine.pdf</li><li>Sarcasm detection in tweets.pdf</li><li>Sarcasm detection on twitterA behavioral modeling approach.pdf</li><li>Sarcastic sentiment detection in tweets streamed in real time a big data approach.pdf</li><li>Scalable linear algebra on a relational database system.pdf</li><li>Scaling Large Production Clusters with Partitioned Synchronization This paper is included in the Proceedings of the.pdf</li><li>Semantics-Aware BERT for Language Understanding.pdf</li><li>Semi-supervised recognition of sarcastic sentences in twitter and Amazon.pdf</li><li>SentencePiece A simple and language independent subword tokenizer and detokenizer for neural text processing.pdf</li><li>Sentiment Analysis for Hindi Language.pdf</li><li>Sentiment Analysis in a Resource Scarce LanguageHindi.pdf</li><li>Sentiment Analysis In Hindi.pdf</li><li>Sentiment Analysis in Indian languages o Definition.pdf</li><li>Sentiment Analysis of Hindi Review based on Negation and Discourse Relation.pdf</li><li>Sentiment classification using machine learning techniques with syntax features.pdf</li><li>Skillful writing of an awful research paper.pdf</li><li>Social media and fake news in the 2016 election.pdf</li><li>Sound classification using convolutional neural network and tensor deep stacking network.pdf</li><li>Sparse, contextually informed models for irony detection Exploiting user communities, entities and sentiment.pdf</li><li>SQuad 100,000 questions for machine comprehension of text.pdf</li><li>ST4_Method_Random_Forest.pdf</li><li>Statistical Methods in Natural Language Processing.pdf</li><li>StructBERT Incorporating Language Structures into Pre-training for Deep Language Understanding.pdf</li><li>Structural S tudies on S mall A myloid O ligomers RT-6.pdf</li><li>Superintelligence.pdf</li><li>Systematic literature review of sentiment analysis on Twitter using soft computing techniques.pdf</li><li>Text categorization with support vector machines Learning with many relevant features.pdf</li><li>Text normalization of code mix and sentiment analysis.pdf</li><li>The Differential Role of Ridicule in Sarcasm and Irony The Differential Role of Ridicule in Sarcasm and Irony.pdf</li><li>The highest form of intelligence Sarcasm increases creativity for both expressers and recipients.pdf</li><li>The Modern Mathematics of Deep Learning *.pdf</li><li>The Paninian approach to natural language processing.pdf</li><li>The perfect solution for detecting sarcasm in tweets #not.pdf</li><li>Thumbs Up or Thumbs Down Semantic Orientation Applied to Unsupervised Classification of Reviews.pdf</li><li>THU_NGN at SemEval-2018 Task 3 Tweet Irony Detection with Densely connected LSTM and Multi-task Learning.pdf</li><li>TnT - A Statistical Part-of-Speech Tagger.pdf</li><li>To BLOB or Not To BLOB Large Object Storage in a Database or a Filesystem To BLOB or Not To BLOB Large Object Storage in a Dat.pdf</li><li>Towards Demystifying Serverless Machine Learning Training.pdf</li><li>Towards multimodal sarcasm detection (an obviously perfect paper).pdf</li><li>Towards sub-word level compositions for sentiment analysis of Hindi-English code mixed text.pdf</li><li>Triple-View Feature Learning for Medical Image Segmentation.pdf</li><li>Twitter as a corpus for sentiment analysis and opinion mining.pdf</li><li>Two improved continuous bag-of-word models.pdf</li><li>Understanding Diffusion Models A Unified Perspective Introduction Generative Models.pdf</li><li>Universal Sentence Encoder.pdf</li><li>Unsupervised Irony Detection A Probabilistic Model with Word Embeddings.pdf</li><li>UR-Funny A multimodal language dataset for understanding humor.pdf</li><li>Use of Sanskrit for natural language processing.pdf</li><li>Using TF-IDF to Determine Word Relevance in Document Queries.pdf</li><li>Using Word Embeddings for Query Translation for Hindi to English Cross Language Information Retrieval.pdf</li><li>Very deep convolutional networks for large-scale image recognition.pdf</li><li>We are IntechOpen , the world &rsquo; s leading publisher of Open Access books Built by scientists , for scientists TOP 1 _.pdf</li><li>When BERT Plays the Lottery, All Tickets Are Winning.pdf</li><li>XGBoost A scalable tree boosting system.pdf</li><li>XLNet Generalized Autoregressive Pretraining for Language Understanding.pdf</li></ol><h2 id=ai-papers-available-on-my-google-drive>AI Papers Available on my Google Drive<a class=td-heading-self-link href=#ai-papers-available-on-my-google-drive aria-label="Heading self-link"></a></h2><p>You can download these papers from <a href="https://drive.google.com/drive/folders/1_dMdWy_qytFEDs2JQr4ecSs92T1RG_Ju?usp=sharing">link</a></p><ol><li>A Comprehensive Survey on Graph Neural Networks-PAPER.pdf</li><li>A machine learning approach to predicting psychosis-PAPER.pdf</li><li>A Selective Overview of Deep Learning-PAPER.pdf</li><li>A Short introduction to boosting-PAPER.pdf</li><li>A Survey of the State of Explainable AI for NLP-PAPER.pdf</li><li>A Survey on Explainable AI (XAI) towards Medical XAI-PAPER.pdf</li><li>AI and Ethics - Operationalising Responsible AI-PAPER.pdf</li><li>Analyzing The Expressive Power of Graph Neural Network in a Spectral Perspective-PAPER.pdf</li><li>Attention-Mechanism-Transformers-BERT-and-GPT-PAPER.pdf</li><li>Can GPT-4 Perform Neural Architecture Search-PAPER.pdf</li><li>Challenges in Deploying Machine Learning-PAPER.pdf</li><li>Clinical AI quality improvement-PAPER.pdf</li><li>Cramming-Training-a-Language-Model-On-A-Single-GPU-in-one-Day-PAPER.pdf</li><li>DataGovernance-A conceptual framework, structured review-PAPER.pdf</li><li>Detailed human avatar-PAPER.pdf</li><li>DRIFT_26_CameraReadySubmission_NeurIPS_DRL-PAPER.pdf</li><li>Dynamic Routing Between Capsules-PAPER.pdf</li><li>Fairness in Machine Learning A Survey-PAPER.pdf</li><li>Forecasting COVID-19 Confirmed Cases-PAPER.pdf</li><li>Generalization Beyond Overfitting On Small Datasets-PAPER.pdf</li><li>GPTrillion-Paper.pdf</li><li>GPTs-are-GPTs-An-Early-Look-at-the-Labor-Market-Impact-Potential-of-Large-Language-Models-PAPER.pdf</li><li>GraphMachine Learning NeurIPS 2020-PAPER.pdf</li><li>Grouped Convolutional Neural Networks for Multivariate Time Series -PAPER.pdf</li><li>Grouped functional time series forecasting An application to age-specific mortality rates-PAPER.pdf</li><li>Hidden Technical Debt in Machine Learning Systems-PAPER.pdf</li><li>Hidden technical debt in machine learning systems.pdf</li><li>How to avoid machine learning pitfalls-PAPER.pdf</li><li>How to Read a Paper-ARTC.pdf</li><li>Identifying machine learning techniques for classification of target advertising-PAPER.pdf</li><li>Introducing-GPTrillion-PAPER.pdf</li><li>Large Object Storage in a Database or a Filesystem-PAPER.pdf</li><li>Learning Graph Heuristic Search-PAPER.pdf</li><li>Learning latent causal graphs via mixture oracles-PAPER.pdf</li><li>LightGBM A Highly Efficient Gradient Boosting-PAPER.pdf</li><li>Machine Learning within a Graph Database- A Case Study on Link Prediction for Scholarly Data-PAPER.pdf</li><li>Merlion- A Machine Learning Library for Time Series-PAPER.pdf</li><li>Model Evaluation, Model Selection, and Algorithm Selection-PAPER.pdf</li><li>NeuralProphet-Explainable Forecasting at Scale-PAPER.pdf</li><li>PAVE-Lazy-MDP based Ensemble to Improve Recall of Product Attribute Extraction Models-PAPER.pdf</li><li>Precise Zero-Shot Dense Retrieval without Relevance Labels-PAPER.pdf</li><li>Randomforest-PAPER.pdf</li><li>Receptive Fields of Single Neurones in the Cats Striate Cortex-PAPER.pdf</li><li>Robotics, AI, and Humanity Science-PAPERS.pdf</li><li>Scalable Linear Algebra on a Relational Database System-PAPER.pdf</li><li>Scaling Large Production Clusters-PAPER.pdf</li><li>Skillful writing of an awful research paper-GUIDE.pdf</li><li>The Modern Mathematics of Deep Learning-PAPER.pdf</li><li>Towards Demystifying Serverless Machine Learning Training-PAPER.pdf</li><li>Triple-View Feature Learning for Medical Image Segmentation-PAPER.pdf</li><li>Understanding Diffusion Models- A Unified Perspective-PAPER.pdf</li><li>VeML-An-End-to-End-Machine-Learning-Lifecycle-for-Large-Scale-and-High-Dimensional-Data-PAPER.pdf</li><li>Very Deep Convolutional Networks for Large Scale Image Recognition-PAPER.pdf</li><li>XGBoost A Scalable Tree Boosting System-PAPER.pdf</li><li>XGBoost Reliable Large-scale Tree Boosting System-PAPER.pdf</li></ol><h2 id=recent-papers>Recent Papers<a class=td-heading-self-link href=#recent-papers aria-label="Heading self-link"></a></h2><ol><li><a href=https://blog.research.google/2022/03/detecting-signs-of-disease-from.html>Detecting Signs of Disease from External Images of the Eye</a>, THURSDAY, MARCH 24, 2022</li><li><a href=https://www.nature.com/articles/s41551-021-00745-6>Deep-learning models for the detection and incidence prediction of chronic kidney disease and type 2 diabetes from retinal fundus images</a>, 15 June 2021</li><li><a href=https://www.nature.com/articles/s41551-019-0487-z>Detection of anaemia from retinal fundus images via deep learning</a>, 23 December 2019</li><li><a href=https://blog.research.google/2018/02/assessing-cardiovascular-risk-factors.html>Assessing Cardiovascular Risk Factors with Computer Vision</a>, MONDAY, FEBRUARY 19, 2018</li></ol><p><strong>Author</strong><br>Dr Hari Thapliyaal<br>dasarpai.com<br>linkedin.com/in/harithapliyal</p><div class=category-section><h4 class=category-section__title>Categories:</h4><div class=category-badges><a href=https://localhost:1313/categories/dsblog class=category-badge>dsblog</a><a href=https://localhost:1313/categories/dsresources class=category-badge>dsresources</a></div></div><div class=td-tags><h4 class=td-tags__title>Tags:</h4><div class=category-badges><a href=https://localhost:1313/tags/llm class=category-badge>LLM</a><a href=https://localhost:1313/tags/transformer class=category-badge>Transformer</a><a href=https://localhost:1313/tags/encoder class=category-badge>Encoder</a><a href=https://localhost:1313/tags/decoder class=category-badge>Decoder</a><a href=https://localhost:1313/tags/research-papers class=category-badge>Research Papers</a><a href=https://localhost:1313/tags/ai-research class=category-badge>AI Research</a><a href=https://localhost:1313/tags/machine-learning class=category-badge>Machine Learning</a><a href=https://localhost:1313/tags/deep-learning class=category-badge>Deep Learning</a><a href=https://localhost:1313/tags/academic-resources class=category-badge>Academic Resources</a><a href=https://localhost:1313/tags/ai-development class=category-badge>AI Development</a><a href=https://localhost:1313/tags/research-collection class=category-badge>Research Collection</a></div></div><div class=td-author-box><div class=td-author-box__info><h4 class=td-author-box__name>Blowfish</h4><p class=td-author-box__bio>A powerful, lightweight theme for Hugo.</p><div class=td-author-box__links><a href target=_blank rel=noopener aria-label></a><a href target=_blank rel=noopener aria-label></a></div></div></div><div class=td-social-share><h4 class=td-social-share__title>Share this article:</h4><ul class=td-social-share__list><div class=social-share><a href="https://twitter.com/intent/tweet?text=Important%20AI%20Paper%20List&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fselect-ai-papers%2f" target=_blank rel=noopener aria-label="Share on Twitter"><i class="fab fa-twitter"></i>
</a><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fselect-ai-papers%2f" target=_blank rel=noopener aria-label="Share on Facebook"><i class="fab fa-facebook"></i>
</a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fselect-ai-papers%2f&title=Important%20AI%20Paper%20List" target=_blank rel=noopener aria-label="Share on LinkedIn"><i class="fab fa-linkedin"></i>
</a><a href="https://www.reddit.com/submit?url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fselect-ai-papers%2f&title=Important%20AI%20Paper%20List" target=_blank rel=noopener aria-label="Share on Reddit"><i class="fab fa-reddit"></i>
</a><a href="mailto:?subject=Important%20AI%20Paper%20List&body=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fselect-ai-papers%2f" aria-label="Share via Email"><i class="fas fa-envelope"></i></a></div></ul></div><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href=https://localhost:1313/dsblog/paperwithcode-resources/ aria-label="Previous page"><div class=td-pager__meta><i class="fa-solid fa-angle-left"></i>
<span class=td-pager__meta-label><b>Previous:</b></span>
<span class=td-pager__meta-title>Paper with Code Resources</span></div></a><a class="td-pager__link td-pager__link--next" href=https://localhost:1313/dsblog/Distances-in-Machine-Learning/ aria-label="Next page"><div class=td-pager__meta><span class=td-pager__meta-label><b>Next:</b></span>
<span class=td-pager__meta-title>Distances in Machine Learning</span>
<i class="fa-solid fa-angle-right"></i></div></a></ul></div><div class=col-md-2></div></div></main><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class=col-2><a href=https://dasarpai.com target=_blank rel=noopener><img src=https://localhost:1313/assets/images/site-logo.png alt=dasarpAI width=100 style=border-radius:12px></a></div><div class=col-8><div class=row></div><div class=row></div></div><div class=col-2></div><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"></div></div></div></footer></div><script src=https://localhost:1313/js/main.min.7c31f98e62c36b0c9c834ce3f8260a0e21895dd6aa0773e81a64b104eae3b2e8.js integrity="sha256-fDH5jmLDawycg0zj+CYKDiGJXdaqB3PoGmSxBOrjsug=" crossorigin=anonymous></script><script defer src=https://localhost:1313/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=https://localhost:1313/js/tabpane-persist.js></script><script src=https://localhost:1313/js/asciinema-player.js></script><script>(function(){var e=document.querySelector("#td-section-nav");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></body></html>