<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.147.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href="../../favicons/favicon.ico?v=1"><link rel=apple-touch-icon href="../../favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><link rel=icon type=image/png href="../../favicons/favicon-16x16.png?v=1" sizes=16x16><link rel=icon type=image/png href="../../favicons/favicon-32x32.png?v=1" sizes=32x32><link rel=apple-touch-icon href="../../favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><title>Cost Functions and Optimizers in Machine Learning | Blowfish</title><meta property="og:url" content="https://dasarpai.github.io/dsblog/Cost-Functions-and-Optimizers-in-Machine-Learning/">
<meta property="og:site_name" content="Blowfish"><meta property="og:title" content="Cost Functions and Optimizers in Machine Learning"><meta property="og:description" content="Cost-Functions-and-Optimizers-in-Machine-Learning What is machine learning? Machine learning is a subfield of artificial intelligence that focuses on the development of algorithms and statistical models that enable computers to improve their performance on a specific task through experience."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="dsblog"><meta property="article:published_time" content="2023-02-01T00:00:00+00:00"><meta property="article:modified_time" content="2023-02-01T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Model Optimization"><meta property="article:tag" content="Neural Networks"><meta property="article:tag" content="Loss Functions"><meta property="article:tag" content="Gradient Descent"><meta itemprop=name content="Cost Functions and Optimizers in Machine Learning"><meta itemprop=description content="Cost-Functions-and-Optimizers-in-Machine-Learning What is machine learning? Machine learning is a subfield of artificial intelligence that focuses on the development of algorithms and statistical models that enable computers to improve their performance on a specific task through experience."><meta itemprop=datePublished content="2023-02-01T00:00:00+00:00"><meta itemprop=dateModified content="2023-02-01T00:00:00+00:00"><meta itemprop=wordCount content="3974"><meta itemprop=keywords content="cost functions,machine learning optimizers,loss functions,gradient descent,neural network training,optimization algorithms,model optimization,learning rate,training optimization,mathematical optimization"><meta name=twitter:card content="summary"><meta name=twitter:title content="Cost Functions and Optimizers in Machine Learning"><meta name=twitter:description content="Cost-Functions-and-Optimizers-in-Machine-Learning What is machine learning? Machine learning is a subfield of artificial intelligence that focuses on the development of algorithms and statistical models that enable computers to improve their performance on a specific task through experience."><link rel=preload href=../../scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css as=style integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link href=../../scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css rel=stylesheet integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link rel=stylesheet type=text/css href=https://dasarpai.github.io/css/asciinema-player.css><script src=https://code.jquery.com/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><link rel=stylesheet href=../../css/custom.css><script src=../../js/lunr.js></script></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-light nav-shadow flex-column flex-md-row td-navbar"><a id=agones-top class=navbar-brand href=../../><svg id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg> <span class="text-uppercase fw-bold">Blowfish</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=../../><span class=active>Data Science</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/docs/><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/docs/><span>Docs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../community/><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/docs/shortcodes/><span>Short Codes</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/blog/><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://github.com/nunocoracao/blowfish><span></span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href>GitHub</a></li><li class="nav-item dropdown d-none d-lg-block"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Release</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://development.agones.dev>Development</a>
<a class=dropdown-item href=https://agones.dev></a><a class=dropdown-item href=https://1-47-0.agones.dev>1.47.0</a>
<a class=dropdown-item href=https://1-46-0.agones.dev>1.46.0</a>
<a class=dropdown-item href=https://1-45-0.agones.dev>1.45.0</a>
<a class=dropdown-item href=https://1-44-0.agones.dev>1.44.0</a>
<a class=dropdown-item href=https://1-43-0.agones.dev>1.43.0</a>
<a class=dropdown-item href=https://1-42-0.agones.dev>1.42.0</a>
<a class=dropdown-item href=https://1-41-0.agones.dev>1.41.0</a>
<a class=dropdown-item href=https://1-40-0.agones.dev>1.40.0</a>
<a class=dropdown-item href=https://1-39-0.agones.dev>1.39.0</a>
<a class=dropdown-item href=https://1-38-0.agones.dev>1.38.0</a>
<a class=dropdown-item href=https://1-37-0.agones.dev>1.37.0</a>
<a class=dropdown-item href=https://1-36-0.agones.dev>1.36.0</a>
<a class=dropdown-item href=https://1-35-0.agones.dev>1.35.0</a>
<a class=dropdown-item href=https://1-34-0.agones.dev>1.34.0</a>
<a class=dropdown-item href=https://1-33-0.agones.dev>1.33.0</a>
<a class=dropdown-item href=https://1-32-0.agones.dev>1.32.0</a>
<a class=dropdown-item href=https://1-31-0.agones.dev>1.31.0</a></div></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><ul class=dropdown-menu><li><a class=dropdown-item href=../../it/>Italiano</a></li></ul></div></li></ul></div><div class="navbar-nav mx-lg-2 d-none d-lg-block"></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><div class=row><div class=col-md-2></div><div class=col-md-8><p><img src=../../assets/images/dspost/dsp6045-Cost-Functions-and-Optimizers-in-Machine-Learning.jpg alt=Cost-Functions-and-Optimizers-in-Machine-Learning></p><h1 id=cost-functions-and-optimizers-in-machine-learning>Cost-Functions-and-Optimizers-in-Machine-Learning<a class=td-heading-self-link href=#cost-functions-and-optimizers-in-machine-learning aria-label="Heading self-link"></a></h1><h2 id=what-is-machine-learning>What is machine learning?<a class=td-heading-self-link href=#what-is-machine-learning aria-label="Heading self-link"></a></h2><p>Machine learning is a subfield of artificial intelligence that focuses on the <strong>development of algorithms and statistical models</strong> that enable computers to improve their performance on a specific task through experience.</p><p>In machine learning, the goal is to develop models that can <strong>automatically learn patterns and relationships in data, and use that knowledge to make predictions or take actions</strong>. The models are trained on a large dataset, and the learning process involves <strong>optimizing the parameters of the model to minimize the prediction error</strong>. For this purpose every algorithms uses some <strong>cost function or loss function</strong>.</p><p>There are various types of machine learning, including supervised learning, unsupervised learning, semi-supervised learning, and reinforcement learning. These approaches are used in a wide range of applications, including image classification, speech recognition, natural language processing, recommendation systems, and predictive analytics.</p><h2 id=what-is-cost-function>What is cost function?<a class=td-heading-self-link href=#what-is-cost-function aria-label="Heading self-link"></a></h2><p>A cost function, also known as a loss function or objective function, is a <strong>mathematical function that measures the difference between the predicted output of a model and the actual output</strong>. The cost function is used to evaluate the performance of a machine learning model and <strong>guide the optimization process during training</strong>.</p><p>The goal of training a machine learning model is to minimize the value of the cost function. This is achieved by adjusting the parameters of the model to reduce the prediction error. The choice of cost function will depend on the type of problem being solved and the type of model being used.</p><p>For example, in a binary classification problem, a common cost function is the cross-entropy loss, which measures the difference between the predicted probabilities and the actual class labels. In a regression problem, a common cost function is the mean squared error, which measures the average squared difference between the predicted values and the actual values.</p><p>The cost function provides a measure of the model&rsquo;s performance, and the optimization process aims to find the values of the model&rsquo;s parameters that minimize the cost function. The optimization process is usually performed using gradient descent or other optimization algorithms, which iteratively update the parameters to reduce the value of the cost function.</p><h2 id=what-is-the-difference-between-loss-function-and-cost-function>What is the difference between loss function and cost function?<a class=td-heading-self-link href=#what-is-the-difference-between-loss-function-and-cost-function aria-label="Heading self-link"></a></h2><p>The terms &ldquo;cost function&rdquo; and &ldquo;loss function&rdquo; are often used interchangeably, but they do have subtle differences depending on the context:</p><h3 id=1-loss-function>1. <strong>Loss Function:</strong><a class=td-heading-self-link href=#1-loss-function aria-label="Heading self-link"></a></h3><ul><li><strong>Definition</strong>: A loss function measures the error for a single training example. It quantifies how well the model&rsquo;s prediction matches the actual target value for that particular example.</li><li><strong>Use Case</strong>: Typically used in contexts where you&rsquo;re evaluating or updating the model on a per-example basis.</li><li><strong>Example</strong>: In a binary classification task, if your model predicts the probability of a sample belonging to the positive class, the loss function might be binary cross-entropy, which compares this prediction to the actual class label.</li></ul><h3 id=2-cost-function>2. <strong>Cost Function:</strong><a class=td-heading-self-link href=#2-cost-function aria-label="Heading self-link"></a></h3><ul><li><strong>Definition</strong>: A cost function is generally the average (or sum) of the loss functions over an entire dataset. It provides a measure of the overall model performance across all training examples.</li><li><strong>Use Case</strong>: Used during the training process to evaluate and minimize the overall error of the model.</li><li><strong>Example</strong>: The cost function could be the Mean Squared Error (MSE) for a regression task, which is the average of the squared errors (individual losses) over all the training examples.</li></ul><h3 id=summary-of-differences>Summary of Differences:<a class=td-heading-self-link href=#summary-of-differences aria-label="Heading self-link"></a></h3><ul><li><p><strong>Scope</strong>:</p><ul><li><strong>Loss function</strong> is usually focused on a single data point.</li><li><strong>Cost function</strong> aggregates the loss across all data points in the dataset.</li></ul></li><li><p><strong>Usage</strong>:</p><ul><li>The term &ldquo;loss function&rdquo; is more commonly used when referring to the error for individual predictions.</li><li>The term &ldquo;cost function&rdquo; is often used when referring to the total error used to train the model (e.g., in optimization algorithms).</li></ul></li></ul><h3 id=example-in-practice>Example in Practice:<a class=td-heading-self-link href=#example-in-practice aria-label="Heading self-link"></a></h3><ul><li><strong>Loss Function</strong>: If you&rsquo;re using Mean Squared Error as your loss function, then for each training example, the loss might be calculated as:
$$
\text{Loss} = (y_{\text{true}} - y_{\text{pred}})^2
$$</li><li><strong>Cost Function</strong>: The cost function would then be the average of these losses across all examples:
$$
\text{Cost} = \frac{1}{n} \sum_{i=1}^{n} (y_{\text{true, i}} - y_{\text{pred, i}})^2
$$</li></ul><p>In summary, the loss function is a measure of error on a single data point, while the cost function is a measure of error across the entire dataset.</p><p>The specific form of the cost function will depend on the type of problem being solved and the nature of the output variables. For example, in regression problems, the mean squared error (MSE) is often used as the cost function, while in classification problems, the cross-entropy loss is commonly used.</p><h2 id=what-are-different-cost-functions>What are different cost functions?<a class=td-heading-self-link href=#what-are-different-cost-functions aria-label="Heading self-link"></a></h2><p>Cost functions, also known as loss functions, are mathematical functions used in machine learning to quantify the difference between the predicted output of a model and the actual target values. The goal of training a model is to minimize this cost function, thereby improving the model&rsquo;s performance. Common Cost Functions:</p><ol><li><p><strong>Mean Squared Error (MSE)</strong></p><ul><li><strong>Description</strong>: Measures the average squared difference between predicted values and actual values.</li><li><strong>Formula</strong>:
$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$
where $$ y_i $$ is the actual value, $$ \hat{y}_i $$ is the predicted value, and $$ n $$ is the number of data points.</li><li><strong>Use Case</strong>: Commonly used in regression tasks.</li></ul></li><li><p><strong>Mean Absolute Error (MAE)</strong></p><ul><li><strong>Description</strong>: Measures the average absolute difference between predicted values and actual values.</li><li><strong>Formula</strong>:
$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$</li><li><strong>Use Case</strong>: Used in regression tasks, particularly when outliers are less influential.</li></ul></li><li><p><strong>Binary Cross-Entropy (Log Loss)</strong></p><ul><li><strong>Description</strong>: Measures the difference between predicted probabilities and actual binary labels. It penalizes incorrect predictions more heavily.</li><li><strong>Formula</strong>:
$$
\text{Binary Cross-Entropy} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
$$</li><li><strong>Use Case</strong>: Used in binary classification tasks.</li></ul></li><li><p><strong>Categorical Cross-Entropy</strong></p><ul><li><strong>Description</strong>: Measures the difference between the predicted probability distribution and the actual one-hot encoded target distribution.</li><li><strong>Formula</strong>:
$$
\text{Categorical Cross-Entropy} = -\sum_{i=1}^{n} \sum_{j=1}^{k} y_{ij} \log(\hat{y}<em>{ij})
$$
where $$ k $$ is the number of classes, and $$ y</em>{ij} $$ is the actual one-hot encoded value.</li><li><strong>Use Case</strong>: Used in multi-class classification tasks.</li></ul></li><li><p><strong>Huber Loss</strong></p><ul><li><strong>Description</strong>: Combines MSE and MAE, offering a balance between the two. It is less sensitive to outliers than MSE.</li><li><strong>Formula</strong>:
$$
\text{Huber Loss} =
\begin{cases}
\frac{1}{2}(y_i - \hat{y}_i)^2 & \text{for } |y_i - \hat{y}_i| \leq \delta \
\delta \cdot |y_i - \hat{y}_i| - \frac{1}{2} \delta^2 & \text{otherwise}
\end{cases}
$$
where $$ \delta $$ is a threshold.</li><li><strong>Use Case</strong>: Used in regression tasks, especially when dealing with outliers.</li></ul></li><li><p><strong>Hinge Loss</strong></p><ul><li><strong>Description</strong>: Used for &ldquo;maximum-margin&rdquo; classification, such as SVMs. It penalizes predictions that are on the wrong side of the decision boundary or within a margin.</li><li><strong>Formula</strong>:
$$
\text{Hinge Loss} = \frac{1}{n} \sum_{i=1}^{n} \max(0, 1 - y_i \cdot \hat{y}_i)
$$
where $$ y_i $$ is the true label (-1 or 1) and $$ \hat{y}_i $$ is the predicted output.</li><li><strong>Use Case</strong>: Used in Support Vector Machines (SVM) for binary classification.</li></ul></li><li><p><strong>Kullback-Leibler (KL) Divergence</strong></p><ul><li><strong>Description</strong>: Measures the divergence between two probability distributions, often used as a regularization term in models like VAEs.</li><li><strong>Formula</strong>:
$$
\text{KL Divergence} = \sum_{i} P(x_i) \log \frac{P(x_i)}{Q(x_i)}
$$</li><li><strong>Use Case</strong>: Used in probabilistic models, VAEs, and reinforcement learning.</li></ul></li><li><p><strong>Poisson Loss</strong></p><ul><li><strong>Description</strong>: Used for count data and assumes that the target variable follows a Poisson distribution.</li><li><strong>Formula</strong>:
$$
\text{Poisson Loss} = \hat{y}_i - y_i \log(\hat{y}_i)
$$</li><li><strong>Use Case</strong>: Used in models where the target variable represents counts.</li></ul></li><li><p><strong>Cosine Similarity Loss</strong></p><ul><li><strong>Description</strong>: Measures the cosine of the angle between two non-zero vectors, indicating their similarity.</li><li><strong>Formula</strong>:
$$
\text{Cosine Similarity Loss} = 1 - \frac{\sum_{i=1}^{n} y_i \hat{y}<em>i}{\sqrt{\sum</em>{i=1}^{n} y_i^2} \cdot \sqrt{\sum_{i=1}^{n} \hat{y}_i^2}}
$$</li><li><strong>Use Case</strong>: Used in tasks like text similarity, recommendation systems, and face recognition.</li></ul></li><li><p><strong>Wasserstein Loss</strong></p><ul><li><strong>Description</strong>: Used in Wasserstein GANs (WGANs) to measure the distance between the real data distribution and the generated data distribution.</li><li><strong>Formula</strong>: Depends on the specific WGAN implementation but generally involves the Earth Mover&rsquo;s Distance (EMD).</li><li><strong>Use Case</strong>: Used in GANs to improve training stability and address issues like mode collapse.</li></ul></li></ol><p>Different cost functions are chosen based on the type of problem (regression, classification, etc.) and the specific characteristics of the data (e.g., presence of outliers). The goal is to find a cost function that aligns well with the task and helps the model converge to an optimal solution during training.</p><h2 id=what-are-different-cost-functions-for-different-machine-learning-goals>What are different cost functions for different Machine learning goals?<a class=td-heading-self-link href=#what-are-different-cost-functions-for-different-machine-learning-goals aria-label="Heading self-link"></a></h2><h3 id=cost-function-for-regression>Cost Function for Regression<a class=td-heading-self-link href=#cost-function-for-regression aria-label="Heading self-link"></a></h3><p>If the goal is Regression, we want to predict a continuous number, then we can use use these cost/loss functions.</p><ul><li>Mean Squared Error (MSE): used for regression problems, measures the average squared difference between the predicted output and the actual output.</li><li>Mean Absolute Error (MAE): also used for regression problems, measures the average absolute difference between the predicted output and the actual output.</li><li>Huber Loss: a combination of mean squared error and mean absolute error, used for robust regression.</li><li>Smooth L1 Loss: also known as the Huber Loss, a combination of mean squared error and mean absolute error, used for object detection in computer vision.</li><li>Log-Cosh Loss: a smooth approximation of the mean absolute error, used for regression problems.</li></ul><h3 id=cost-function-for-classification>Cost Function for Classification<a class=td-heading-self-link href=#cost-function-for-classification aria-label="Heading self-link"></a></h3><p>If the goal is Classification, we want to predict a class/category, then we can use use these cost/loss functions.</p><ul><li>Binary Cross-Entropy Loss: a variation of cross-entropy loss, used for binary classification problems.</li><li>Cross-Entropy Loss: measures the difference between the predicted class probabilities and the actual class label, used for multi-class classification problems.</li><li>Hinge Loss: used for maximum-margin classification problems, measures the margin between the predicted class and the incorrect class.</li><li>Squared Hinge Loss: a variation of hinge loss, used for maximum-margin classification problems.</li><li>Multi-Class Logarithmic Loss: used for multi-class classification problems, measures the average log loss across all classes.</li><li>Focal Loss: used for object detection in computer vision, adds a term that modulates the cross-entropy loss based on the prediction confidence.</li><li>Categorical Cross-Entropy Loss: another variation of cross-entropy loss for multi-class classification problems.</li></ul><h3 id=cost-function-for-clustering>Cost Function for Clustering<a class=td-heading-self-link href=#cost-function-for-clustering aria-label="Heading self-link"></a></h3><p>If the goal is Clustering, we want to group samples/records/examples, then we can use use these cost/loss functions.</p><ul><li>Sum of Squared Errors (SSE): measures the sum of squared differences between each data point and its nearest cluster center.</li><li>Within-Cluster Sum of Squared Errors (WCSS): similar to SSE, but measures the sum of squared differences between each data point and its nearest cluster center, averaged across all clusters.</li><li>Davies-Bouldin Index: measures the similarity between each pair of clusters, based on the distance between their cluster centers and the size of their cluster.</li><li>Silhouette Score: measures the similarity between each data point and its own cluster compared to other clusters.</li><li>Calinski-Harabasz Index: measures the ratio of between-cluster variance to within-cluster variance, based on the sum of squared differences from the cluster centers.</li></ul><h3 id=cost-function-for-sementic-segmenration>Cost Function for Sementic Segmenration<a class=td-heading-self-link href=#cost-function-for-sementic-segmenration aria-label="Heading self-link"></a></h3><p>If the goal is Semantic Segmentation (a type of image analysis task in computer vision where the goal is to classify each pixel in an image into a predefined category or class.), then we can use use these cost/loss functions.</p><ul><li>Cross-Entropy Loss: measures the difference between the predicted class probabilities and the actual class label.</li><li>Dice Loss: measures the overlap between the predicted and ground-truth segmentation masks.</li><li>Intersection over Union (IoU) Loss: similar to Dice loss, measures the overlap between the predicted and ground-truth segmentation masks, but normalizes the overlap based on the size of the masks.</li><li>Jaccard Loss: a variant of IoU loss, which measures the overlap between the predicted and ground-truth segmentation masks.</li><li>Focal Loss: adds a term that modulates the cross-entropy loss based on the prediction confidence, which can improve the performance of the model on the hard examples.</li></ul><h2 id=cost-function-for-text-genration>Cost Function for Text Genration<a class=td-heading-self-link href=#cost-function-for-text-genration aria-label="Heading self-link"></a></h2><p>Can you tell what different loss functions are available to update the parameters during learning in Text Generation models?</p><p>In text generation models, various loss functions are used to update the model&rsquo;s parameters and improve performance. The choice of loss function depends on the model architecture and the specific text generation task. Here are some common loss functions used in text generation:</p><h3 id=1-cross-entropy-loss-negative-log-likelihood-loss>1. <strong>Cross-Entropy Loss (Negative Log-Likelihood Loss)</strong><a class=td-heading-self-link href=#1-cross-entropy-loss-negative-log-likelihood-loss aria-label="Heading self-link"></a></h3><ul><li><strong>Description</strong>: Measures the difference between the predicted probability distribution over the vocabulary and the actual target distribution (one-hot encoded vector).</li><li><strong>Use Case</strong>: Used extensively in sequence-to-sequence models, language models, and other text generation tasks where the goal is to predict the next token in a sequence.</li><li><strong>Formula</strong>:
$$
\text{Cross-Entropy Loss} = -\sum_{i=1}^{N} \log P(y_i | x)
$$
where $$ y_i $$ is the actual token and $$ P(y_i | x) $$ is the predicted probability of the token given the input $$ x $$.</li></ul><h3 id=2-reinforcement-learning-based-loss-policy-gradient>2. <strong>Reinforcement Learning-Based Loss (Policy Gradient)</strong><a class=td-heading-self-link href=#2-reinforcement-learning-based-loss-policy-gradient aria-label="Heading self-link"></a></h3><ul><li><strong>Description</strong>: Used when the text generation model is trained using reinforcement learning techniques, such as the REINFORCE algorithm. The loss is based on the reward signal received from the environment or from a discriminator in GANs.</li><li><strong>Use Case</strong>: Used in models like SeqGAN, where text is generated in sequence and evaluated based on a reward function rather than direct supervised labels.</li><li><strong>Formula</strong>:
$$
\text{Policy Gradient Loss} = - \mathbb{E}[R(\tau) \log \pi_\theta(\tau)]
$$
where $$ R(\tau) $$ is the reward associated with the generated sequence $$ \tau $$, and $$ \pi_\theta $$ is the policy (model) with parameters $$ \theta $$.</li></ul><h3 id=3-maximum-likelihood-estimation-mle-loss>3. <strong>Maximum Likelihood Estimation (MLE) Loss</strong><a class=td-heading-self-link href=#3-maximum-likelihood-estimation-mle-loss aria-label="Heading self-link"></a></h3><ul><li><strong>Description</strong>: A loss function that maximizes the likelihood of the observed data given the model parameters. It’s closely related to cross-entropy loss in the context of text generation.</li><li><strong>Use Case</strong>: Common in sequence generation tasks where the objective is to maximize the probability of the training sequences.</li><li><strong>Formula</strong>:
$$
\text{MLE Loss} = -\sum_{t=1}^{T} \log P(y_t | y_{&lt;t}, x)
$$
where $$ y_t $$ is the target token at time step $$ t $$, and $$ y_{&lt;t} $$ represents the previous tokens.</li></ul><h3 id=4-perplexity>4. <strong>Perplexity</strong><a class=td-heading-self-link href=#4-perplexity aria-label="Heading self-link"></a></h3><ul><li><strong>Description</strong>: Perplexity is not a direct loss function but a metric derived from cross-entropy loss. It measures how well a probability distribution or probability model predicts a sample.</li><li><strong>Use Case</strong>: Used as an evaluation metric for language models, lower perplexity indicates a better model.</li><li><strong>Formula</strong>:
$$
\text{Perplexity} = 2^{\text{Cross-Entropy Loss}}
$$</li></ul><h3 id=5-kl-divergence-kullback-leibler-divergence>5. <strong>KL Divergence (Kullback-Leibler Divergence)</strong><a class=td-heading-self-link href=#5-kl-divergence-kullback-leibler-divergence aria-label="Heading self-link"></a></h3><ul><li><strong>Description</strong>: Measures the divergence between the predicted distribution and a target distribution. It’s often used in variational autoencoders (VAEs) for text generation.</li><li><strong>Use Case</strong>: Regularizes models to ensure that the generated distribution doesn’t diverge too far from a prior distribution.</li><li><strong>Formula</strong>:
$$
\text{KL Divergence} = \sum_{i=1}^{N} P(y_i | x) \log \frac{P(y_i | x)}{Q(y_i)}
$$
where $$ P(y_i | x) $$ is the predicted probability and $$ Q(y_i) $$ is the target probability.</li></ul><h3 id=6-bleu-score-based-loss>6. <strong>BLEU Score-Based Loss</strong><a class=td-heading-self-link href=#6-bleu-score-based-loss aria-label="Heading self-link"></a></h3><ul><li><strong>Description</strong>: BLEU (Bilingual Evaluation Understudy) is a metric used to evaluate the quality of text generated by comparing it with reference texts. Some approaches involve directly optimizing BLEU or using it as a reward in reinforcement learning.</li><li><strong>Use Case</strong>: Used in machine translation and other generation tasks where the quality of the generated text relative to reference text is important.</li><li><strong>Formula</strong>: BLEU is not directly a loss function, but can be used as a reward signal:
$$
\text{BLEU} = \text{exp} \left( \min \left( 1 - \frac{\text{length of reference}}{\text{length of hypothesis}}, 0 \right) + \sum_{n=1}^{N} w_n \log p_n \right)
$$
where $$ p_n $$ is the precision of n-grams and $$ w_n $$ is the weight for the n-gram order.</li></ul><h3 id=7-gan-discriminator-loss>7. <strong>GAN Discriminator Loss</strong><a class=td-heading-self-link href=#7-gan-discriminator-loss aria-label="Heading self-link"></a></h3><ul><li><strong>Description</strong>: In GANs like SeqGAN or TextGAN, the discriminator provides a loss signal that reflects how well the generator’s text resembles real text. The generator’s loss is typically the inverse of the discriminator’s success.</li><li><strong>Use Case</strong>: Used in GAN-based text generation models to refine the generator&rsquo;s output to be more realistic.</li><li><strong>Formula</strong>:
$$
\text{Generator Loss} = -\log(D(G(z)))
$$
where $$ D(G(z)) $$ is the discriminator&rsquo;s probability that the generated text $$ G(z) $$ is real.</li></ul><h3 id=summary>Summary:<a class=td-heading-self-link href=#summary aria-label="Heading self-link"></a></h3><p>Different loss functions serve different purposes in updating the parameters of text generation models. Cross-Entropy Loss and MLE Loss are standard for sequence modeling, while reinforcement learning-based losses and GAN discriminator losses are used for more advanced techniques like GANs and policy gradient methods. Each loss function is chosen based on the specific needs of the text generation task and the model architecture.</p><h2 id=can-you-explain-with-example-how-kl-divergence-loss-function-works>Can you explain with example how KL Divergence loss function works?<a class=td-heading-self-link href=#can-you-explain-with-example-how-kl-divergence-loss-function-works aria-label="Heading self-link"></a></h2><p>When we say &ldquo;measures the divergence between the predicted distribution and a target distribution,&rdquo; we&rsquo;re referring to how different or similar two probability distributions are from each other. Specifically, it’s about comparing the distribution of the model&rsquo;s predictions (the predicted distribution) with the actual or expected distribution (the target distribution).</p><h3 id=what-is-a-probability-distribution>What is a Probability Distribution?<a class=td-heading-self-link href=#what-is-a-probability-distribution aria-label="Heading self-link"></a></h3><p>A probability distribution assigns probabilities to different outcomes or events. For example, in the context of text generation, a probability distribution might tell us the likelihood of each word or token being the next in a sequence.</p><h3 id=kl-divergence-explained>KL Divergence Explained:<a class=td-heading-self-link href=#kl-divergence-explained aria-label="Heading self-link"></a></h3><p><strong>Kullback-Leibler (KL) Divergence</strong> is a measure of how one probability distribution diverges from a second, expected probability distribution. It&rsquo;s not symmetric, meaning $$ (\text{KL}P | Q) $$ is not necessarily the same as $$ \text{KL}(Q | P) $$. The KL divergence is used to quantify how much information is lost when we use the predicted distribution instead of the target distribution.</p><h3 id=example>Example:<a class=td-heading-self-link href=#example aria-label="Heading self-link"></a></h3><p>Let&rsquo;s consider a simple example where we have a model that is predicting the next word in a sentence.</p><h4 id=scenario>Scenario:<a class=td-heading-self-link href=#scenario aria-label="Heading self-link"></a></h4><p>Suppose we&rsquo;re trying to predict the next word after &ldquo;The cat is&rdquo;. The target distribution might look like this based on real-world data:</p><ul><li>Target distribution $$ Q $$:<ul><li>&ldquo;sleeping&rdquo;: 0.7</li><li>&ldquo;running&rdquo;: 0.2</li><li>&ldquo;eating&rdquo;: 0.1</li></ul></li></ul><p>This distribution reflects the true probabilities of each word occurring next based on our training data.</p><p>Now, suppose our model predicts the following distribution:</p><ul><li>Predicted distribution $$ P $$:<ul><li>&ldquo;sleeping&rdquo;: 0.5</li><li>&ldquo;running&rdquo;: 0.3</li><li>&ldquo;eating&rdquo;: 0.2</li></ul></li></ul><h3 id=kl-divergence-calculation>KL Divergence Calculation:<a class=td-heading-self-link href=#kl-divergence-calculation aria-label="Heading self-link"></a></h3><p>The KL divergence between the target distribution $$ Q $$ and the predicted distribution $$ P $$ can be calculated as:</p><p>$$
\text{KL}(Q | P) = \sum_{i} Q(i) \log \frac{Q(i)}{P(i)}
$$</p><p>For our example:</p><p>$$
\text{KL}(Q | P) = (0.7) \log \frac{0.7}{0.5} + (0.2) \log \frac{0.2}{0.3} + (0.1) \log \frac{0.1}{0.2}
$$</p><p>Let&rsquo;s compute this step by step:</p><ol><li><p>For &ldquo;sleeping&rdquo;:
$$
0.7 \log \frac{0.7}{0.5} = 0.7 \times \log(1.4) \approx 0.7 \times 0.3365 = 0.2356
$$</p></li><li><p>For &ldquo;running&rdquo;:
$$
0.2 \log \frac{0.2}{0.3} = 0.2 \times \log(0.6667) \approx 0.2 \times (-0.1761) = -0.0352
$$</p></li><li><p>For &ldquo;eating&rdquo;:
$$
0.1 \log \frac{0.1}{0.2} = 0.1 \times \log(0.5) \approx 0.1 \times (-0.3010) = -0.0301
$$</p></li></ol><p>Finally, summing these values gives us:</p><p>$$
\text{KL}(Q | P) = 0.2356 - 0.0352 - 0.0301 = 0.1703
$$</p><h3 id=interpretation>Interpretation:<a class=td-heading-self-link href=#interpretation aria-label="Heading self-link"></a></h3><ul><li><strong>KL Divergence Value</strong>: The KL divergence value of 0.1703 indicates that there&rsquo;s some divergence between the predicted and target distributions. If the predicted distribution $$ P $$ were exactly the same as the target distribution $$ Q $$, the KL divergence would be 0, indicating no divergence.</li><li><strong>Minimizing KL Divergence</strong>: In practice, during training, we try to minimize the KL divergence to make the model’s predictions as close as possible to the target distribution.</li></ul><h3 id=summary-1>Summary:<a class=td-heading-self-link href=#summary-1 aria-label="Heading self-link"></a></h3><p>KL Divergence measures how much the predicted probability distribution diverges from the actual, expected distribution. In the context of our example, it quantifies how different the model’s predicted probabilities for the next word in a sentence are from the true probabilities based on training data. Minimizing KL divergence during training helps improve the model’s accuracy in making predictions.</p><h2 id=what-is-optimizer>What is Optimizer?<a class=td-heading-self-link href=#what-is-optimizer aria-label="Heading self-link"></a></h2><p>Optimizers play a crucial role in deep neural network training. They are responsible for updating the model&rsquo;s parameters in order to minimise the loss function, and ultimately improve the performance of the model. There are many different optimisers available, each with their own strengths and weaknesses, and choosing the right optimiser can make a significant impact on the training process.</p><h2 id=what-are-the-various-optimization-algorithms>What are the various optimization algorithms?<a class=td-heading-self-link href=#what-are-the-various-optimization-algorithms aria-label="Heading self-link"></a></h2><p>Some popular optimisers include stochastic gradient descent (SGD), momentum, Adagrad, Adadelta, RProp, RMSprop, Adam, AMSGrad, and Nadam. These optimisers differ in how they calculate the updates to the model&rsquo;s parameters, with some taking into account the historical gradient information, others using momentum to smooth out updates, and others adapting the learning rate based on the magnitude of the gradients. It&rsquo;s important to carefully consider the properties of the cost function and the structure of the model when selecting an optimiser, as this can have a significant impact on the speed and stability of the training process.</p><h2 id=can-you-explain-with-example-how-these-optimizers-update-the-model-parameters>Can you explain with example how these optimizers update the model parameters?<a class=td-heading-self-link href=#can-you-explain-with-example-how-these-optimizers-update-the-model-parameters aria-label="Heading self-link"></a></h2><ul><li><p>Stochastic Gradient Descent (SGD) - It updates the model parameters by taking the gradient of the loss function with respect to the parameters and subtracting it from the parameters.<br>$$\theta = \theta - \alpha \frac{\partial J(\theta)}{\partial \theta}$$</p><p>where $$\theta$$ is the model parameter, $$\alpha$$ is the learning rate, and $$J(\theta)$$ is the cost function.</p></li><li><p>Momentum - It accumulates the gradient of the previous steps to avoid oscillation and converge faster.<br>$v = \beta v - \alpha \frac{\partial J(\theta)}{\partial \theta}$</p><p>$$\theta = \theta + v$$</p><p>where $v$ is the velocity term, $$\beta$$ is the momentum hyperparameter.</p></li><li><p>Nesterov Accelerated Gradient (NAG) - It is an improved version of Momentum that takes into account the future position of the parameters based on the estimated gradient.<br>$v = \beta v - \alpha \frac{\partial J(\theta + \beta v)}{\partial \theta}$</p><p>$$\theta = \theta + v$$</p></li><li><p>Adagrad - It adapts the learning rate to the parameters, performing larger updates for infrequent parameters and smaller updates for frequent parameters.<br>$$G = G + \left(\frac{\partial J(\theta)}{\partial \theta}\right)^2$$</p><p>$$\theta = \theta - \frac{\alpha}{\sqrt{G + \epsilon}} \frac{\partial J(\theta)}{\partial \theta}$$</p><p>where $$G$$ is the sum of squares of past gradients, and $$\epsilon$$ is a small value to prevent division by zero.</p></li><li><p>Adadelta - It is an extension of Adagrad that reduces its aggressive, monotonically decreasing learning rate.
$E[g^2]t = \gamma E[g^2]{t-1} + (1 - \gamma)\left(\frac{\partial J(\theta)}{\partial \theta}\right)^2$</p><p>$$\Delta \theta_t = -\frac{\sqrt{E[\Delta \theta^2]_{t-1} + \epsilon}}{\sqrt{E[g^2]_t + \epsilon}} \frac{\partial J(\theta)}{\partial \theta}$$</p><p>$$\theta = \theta + \Delta \theta_t$$</p><p>where $E[g^2]$ and $E[\Delta \theta^2]$ are the moving average of the square of gradients and the square of parameter updates, respectively, and $$\gamma$$ is the decay rate.</p></li><li><p>RProp - It uses the sign of the gradient to determine the direction of the update, with a dynamically adjusted step size for each parameter.</p><p>$$\Delta \theta_i = \text{sign}(\frac{\partial J(\theta)}{\partial \theta_i})\Delta \theta_{i,prev}$$</p><p>$$\theta_i = \theta_i - \Delta \theta_i$$</p><p>where $$\theta_i$$ is the current value of a model parameter, $$\frac{\partial J(\theta)}{\partial \theta_i}$$ is the gradient of the loss function with respect to the parameter, $$\Delta \theta_{i,prev}$$ is the previous update to the parameter, and $$\text{sign}(\cdot)$$ is the sign function. The step size $$\Delta \theta_i$$ is determined dynamically based on the magnitude of the gradient.</p></li><li><p>Adam (Adaptive Moment Estimation) - It combines the advantages of Momentum and Adagrad, by considering both the average and the variance of the gradient for parameter updates.<br>$$m = \beta_1 m + (1 - \beta_1) \frac{\partial J(\theta)}{\partial \theta}$$</p><p>$$v = \beta_2 v + (1 - \beta_2) \left(\frac{\partial J(\theta)}{\partial \theta}\right)^2$$</p><p>$$\hat{m} = \frac{m}{1 - \beta_1^t}$$</p><p>$$\hat{v} = \frac{v}{1 - \beta_2^t}$$</p><p>$$\theta = \theta - \frac{\alpha}{\sqrt{\hat{v}} + \epsilon} \hat{m}$$</p><p>where $$m$$ and $$v$$ are the first and second moment estimates, respectively, $$\beta_1$$ and $$\beta_2$$ are hyperparameters, and the rest of the terms are as defined above.</p></li><li><p>AMSGrad - It is an extension of Adam that ensures the learning rate does not get too small, even if the gradient is small.<br>$m = \beta_1 m + (1 - \beta_1) \frac{\partial J(\theta)}{\partial \theta}$</p><p>$$v = \max(\beta_2 v + (1 - \beta_2) \left(\frac{\partial J(\theta)}{\partial \theta}\right)^2, v_{t-1})$$</p><p>$$\hat{m} = \frac{m}{1 - \beta_1^t}$$</p><p>$$\hat{v} = \frac{v}{1 - \beta_2^t}$$</p><p>$$\theta = \theta - \frac{\alpha}{\sqrt{\hat{v}} + \epsilon} \hat{m}$$</p></li><li><p>Nadam (Nesterov-Accelerated Adaptive Moment Estimation) - It combines NAG and Adam to take advantage of the rapid convergence of NAG and the adaptive learning rate of Adam.<br>$$m = \beta_1 m + (1 - \beta_1) \frac{\partial J(\theta)}{\partial \theta}$$</p><p>$$v = \beta_2 v + (1 - \beta_2) \left(\frac{\partial J(\theta)}{\partial \theta}\right)^2$$</p><p>$$\hat{m} = \frac{m}{1 - \beta_1^t}$$</p><p>$$\hat{v} = \frac{v}{1 - \beta_2^t}$$</p><p>$$\theta = \theta - \frac{\alpha}{\sqrt{\hat{v}} + \epsilon} \left(\beta_1 \hat{m} + (1 - \beta_1) \frac{\partial J(\theta)}{\partial \theta}\right)$$</p></li></ul><p><strong>Author</strong><br>Dr Hari Thapliyal<br><a href=https://linkedin.com/in/harithapliyal>https://linkedin.com/in/harithapliyal</a><br><a href=https://dasarpai.com>https://dasarpai.com</a></p><div class=category-section><h4 class=category-section__title>Categories:</h4><div class=category-badges><a href=../../categories/dsblog class=category-badge>dsblog</a></div></div><div class=td-tags><h4 class=td-tags__title>Tags:</h4><div class=category-badges><a href=../../tags/machine-learning class=category-badge>Machine Learning</a><a href=../../tags/deep-learning class=category-badge>Deep Learning</a><a href=../../tags/model-optimization class=category-badge>Model Optimization</a><a href=../../tags/neural-networks class=category-badge>Neural Networks</a><a href=../../tags/loss-functions class=category-badge>Loss Functions</a><a href=../../tags/gradient-descent class=category-badge>Gradient Descent</a><a href=../../tags/model-training class=category-badge>Model Training</a><a href=../../tags/mathematical-optimization class=category-badge>Mathematical Optimization</a><a href=../../tags/algorithm-design class=category-badge>Algorithm Design</a></div></div><div class=td-author-box><div class=td-author-box__info><h4 class=td-author-box__name>Blowfish</h4><p class=td-author-box__bio>A powerful, lightweight theme for Hugo.</p><div class=td-author-box__links><a href target=_blank rel=noopener aria-label></a><a href target=_blank rel=noopener aria-label></a></div></div></div><div class=td-social-share><h4 class=td-social-share__title>Share this article:</h4><ul class=td-social-share__list><div class=social-share><a href="https://twitter.com/intent/tweet?text=Cost%20Functions%20and%20Optimizers%20in%20Machine%20Learning&url=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fCost-Functions-and-Optimizers-in-Machine-Learning%2f" target=_blank rel=noopener aria-label="Share on Twitter"><i class="fab fa-twitter"></i>
</a><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fCost-Functions-and-Optimizers-in-Machine-Learning%2f" target=_blank rel=noopener aria-label="Share on Facebook"><i class="fab fa-facebook"></i>
</a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fCost-Functions-and-Optimizers-in-Machine-Learning%2f&title=Cost%20Functions%20and%20Optimizers%20in%20Machine%20Learning" target=_blank rel=noopener aria-label="Share on LinkedIn"><i class="fab fa-linkedin"></i>
</a><a href="https://www.reddit.com/submit?url=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fCost-Functions-and-Optimizers-in-Machine-Learning%2f&title=Cost%20Functions%20and%20Optimizers%20in%20Machine%20Learning" target=_blank rel=noopener aria-label="Share on Reddit"><i class="fab fa-reddit"></i>
</a><a href="mailto:?subject=Cost%20Functions%20and%20Optimizers%20in%20Machine%20Learning&body=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fCost-Functions-and-Optimizers-in-Machine-Learning%2f" aria-label="Share via Email"><i class="fas fa-envelope"></i></a></div></ul></div><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href=../../dsblog/Application-of-AI-in-BFSI/ aria-label="Previous page"><div class=td-pager__meta><i class="fa-solid fa-angle-left"></i>
<span class=td-pager__meta-label><b>Previous:</b></span>
<span class=td-pager__meta-title>Application of AI in BFSI</span></div></a><a class="td-pager__link td-pager__link--next" href=../../dsblog/Responsible-AI/ aria-label="Next page"><div class=td-pager__meta><span class=td-pager__meta-label><b>Next:</b></span>
<span class=td-pager__meta-title>Responsible AI</span>
<i class="fa-solid fa-angle-right"></i></div></a></ul></div><div class=col-md-2></div></div></main><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class=col-2><a href=https://dasarpai.com target=_blank rel=noopener><img src=https://dasarpai.github.io/assets/images/site-logo.png alt=dasarpAI width=100 style=border-radius:12px></a></div><div class=col-8><div class=row></div><div class=row></div></div><div class=col-2></div><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"></div></div></div></footer></div><script src=../../js/main.min.4e728f6d1777a74bef81c238fd7b573f42f1848a910503e07f877f49f1f442cb.js integrity="sha256-TnKPbRd3p0vvgcI4/XtXP0LxhIqRBQPgf4d/SfH0Qss=" crossorigin=anonymous></script><script defer src=../../js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=../../js/tabpane-persist.js></script><script src=https://dasarpai.github.io/js/asciinema-player.js></script><script>(function(){var e=document.querySelector("#td-section-nav");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></body></html>