<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.147.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href="../../favicons/favicon.ico?v=1"><link rel=apple-touch-icon href="../../favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><link rel=icon type=image/png href="../../favicons/favicon-16x16.png?v=1" sizes=16x16><link rel=icon type=image/png href="../../favicons/favicon-32x32.png?v=1" sizes=32x32><link rel=apple-touch-icon href="../../favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><title>Introduction to ML Model Deployment | Blowfish</title><meta property="og:url" content="https://dasarpai.github.io/dsblog/Introduction-to-ML-Model-deployment/">
<meta property="og:site_name" content="Blowfish"><meta property="og:title" content="Introduction to ML Model Deployment"><meta property="og:description" content="Introduction to AI Model deployment Big Players Amazon Amazon has many products and one of their product is AWS Cloud. Under this product they sell IT infrastructure (storage, memory, network, VM, webhosting etc.) Amazon SageMaker is Cloud based Machine Learning Platform, and this is one of the product under AWS Cloud. Amazon SageMaker can be used to train AI model, host AI model, monitor the model and hosts many other services which any Data Science project need from data gathering to model serving. AWS is oldest cloud service provider in the market. AWS Sagemaker was launched in Nov'17. Google Google has hundreds of products like gmail, youtube, google drive etc. One of their product is called Google Cloud. Under this product they sell IT infrastrcture like Amazon sells under AWS. VertexAI is Cloud based Machine Learning platform of Google. VertexAI is part of Google Cloud. VertexAI can be used to train AI Model,host AI model, monitor the model etc. VertexAI was launched in Jun'21 Microsoft Like Amazon’s cloud platform which is called AWS Cloud, Microsoft’s cloud plateform is called Azure. Microsoft’s AI product is called Azure Machine Learning. Today (Jul'23) Azure Machine Learning has has most of the capabilites than any other player’s AI product. Azure Machine Learning was launched Feb'14 What is GenAI? There are many kinds of AI models like classifier models, regressor models, clustering models, reinforcement models, etc. An AI model which has the ability to generate text, images, video, and music is called GenAI. They all take inspiration from the human brain, therefore they all have neural network (NN) architecture. There are dozens (if not hundreds) types of NN architecture that can be used to create different kinds of AI models. The type of NN architecture depends upon the data which is used for developing the model and the problem which we want to solve using AI model. Researchers in universities or big corporations like Google, Facebook, Amazon, and Microsoft keep developing new architecture, and using these architectures they develop the foundational models. Once foundational models are developed, they release a research paper. In this, they inform the world what architecture they used, what data they used, what parameters (weights & biases) the model has learned, what are the results of their product and compare that with other existing models. They can develop these foundational models with one set of hyperparameters, and they can release these foundational models of different sizes (it depends upon the number of parameters used). AI product builders pick up these foundational models and fine-tune these based on the exact business problem in their hands. Which foundational model do they choose, it also depends upon the size of the model, the kind of data it has used to create those foundational models, and what was the performance of the model on a similar task which the product developer want to solve."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="dsblog"><meta property="article:published_time" content="2023-07-19T00:00:00+00:00"><meta property="article:modified_time" content="2023-07-19T00:00:00+00:00"><meta property="article:tag" content="MLOps"><meta property="article:tag" content="Model Deployment"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="DevOps"><meta property="article:tag" content="Production AI"><meta property="article:tag" content="Cloud Computing"><meta itemprop=name content="Introduction to ML Model Deployment"><meta itemprop=description content="Introduction to AI Model deployment Big Players Amazon Amazon has many products and one of their product is AWS Cloud. Under this product they sell IT infrastructure (storage, memory, network, VM, webhosting etc.) Amazon SageMaker is Cloud based Machine Learning Platform, and this is one of the product under AWS Cloud. Amazon SageMaker can be used to train AI model, host AI model, monitor the model and hosts many other services which any Data Science project need from data gathering to model serving. AWS is oldest cloud service provider in the market. AWS Sagemaker was launched in Nov'17. Google Google has hundreds of products like gmail, youtube, google drive etc. One of their product is called Google Cloud. Under this product they sell IT infrastrcture like Amazon sells under AWS. VertexAI is Cloud based Machine Learning platform of Google. VertexAI is part of Google Cloud. VertexAI can be used to train AI Model,host AI model, monitor the model etc. VertexAI was launched in Jun'21 Microsoft Like Amazon’s cloud platform which is called AWS Cloud, Microsoft’s cloud plateform is called Azure. Microsoft’s AI product is called Azure Machine Learning. Today (Jul'23) Azure Machine Learning has has most of the capabilites than any other player’s AI product. Azure Machine Learning was launched Feb'14 What is GenAI? There are many kinds of AI models like classifier models, regressor models, clustering models, reinforcement models, etc. An AI model which has the ability to generate text, images, video, and music is called GenAI. They all take inspiration from the human brain, therefore they all have neural network (NN) architecture. There are dozens (if not hundreds) types of NN architecture that can be used to create different kinds of AI models. The type of NN architecture depends upon the data which is used for developing the model and the problem which we want to solve using AI model. Researchers in universities or big corporations like Google, Facebook, Amazon, and Microsoft keep developing new architecture, and using these architectures they develop the foundational models. Once foundational models are developed, they release a research paper. In this, they inform the world what architecture they used, what data they used, what parameters (weights & biases) the model has learned, what are the results of their product and compare that with other existing models. They can develop these foundational models with one set of hyperparameters, and they can release these foundational models of different sizes (it depends upon the number of parameters used). AI product builders pick up these foundational models and fine-tune these based on the exact business problem in their hands. Which foundational model do they choose, it also depends upon the size of the model, the kind of data it has used to create those foundational models, and what was the performance of the model on a similar task which the product developer want to solve."><meta itemprop=datePublished content="2023-07-19T00:00:00+00:00"><meta itemprop=dateModified content="2023-07-19T00:00:00+00:00"><meta itemprop=wordCount content="2234"><meta itemprop=keywords content="Machine Learning Deployment,MLOps,Model Serving,Production AI,ML Infrastructure,Model Optimization,Deployment Frameworks,Cloud Deployment"><meta name=twitter:card content="summary"><meta name=twitter:title content="Introduction to ML Model Deployment"><meta name=twitter:description content="Introduction to AI Model deployment Big Players Amazon Amazon has many products and one of their product is AWS Cloud. Under this product they sell IT infrastructure (storage, memory, network, VM, webhosting etc.) Amazon SageMaker is Cloud based Machine Learning Platform, and this is one of the product under AWS Cloud. Amazon SageMaker can be used to train AI model, host AI model, monitor the model and hosts many other services which any Data Science project need from data gathering to model serving. AWS is oldest cloud service provider in the market. AWS Sagemaker was launched in Nov'17. Google Google has hundreds of products like gmail, youtube, google drive etc. One of their product is called Google Cloud. Under this product they sell IT infrastrcture like Amazon sells under AWS. VertexAI is Cloud based Machine Learning platform of Google. VertexAI is part of Google Cloud. VertexAI can be used to train AI Model,host AI model, monitor the model etc. VertexAI was launched in Jun'21 Microsoft Like Amazon’s cloud platform which is called AWS Cloud, Microsoft’s cloud plateform is called Azure. Microsoft’s AI product is called Azure Machine Learning. Today (Jul'23) Azure Machine Learning has has most of the capabilites than any other player’s AI product. Azure Machine Learning was launched Feb'14 What is GenAI? There are many kinds of AI models like classifier models, regressor models, clustering models, reinforcement models, etc. An AI model which has the ability to generate text, images, video, and music is called GenAI. They all take inspiration from the human brain, therefore they all have neural network (NN) architecture. There are dozens (if not hundreds) types of NN architecture that can be used to create different kinds of AI models. The type of NN architecture depends upon the data which is used for developing the model and the problem which we want to solve using AI model. Researchers in universities or big corporations like Google, Facebook, Amazon, and Microsoft keep developing new architecture, and using these architectures they develop the foundational models. Once foundational models are developed, they release a research paper. In this, they inform the world what architecture they used, what data they used, what parameters (weights & biases) the model has learned, what are the results of their product and compare that with other existing models. They can develop these foundational models with one set of hyperparameters, and they can release these foundational models of different sizes (it depends upon the number of parameters used). AI product builders pick up these foundational models and fine-tune these based on the exact business problem in their hands. Which foundational model do they choose, it also depends upon the size of the model, the kind of data it has used to create those foundational models, and what was the performance of the model on a similar task which the product developer want to solve."><link rel=preload href=../../scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css as=style integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link href=../../scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css rel=stylesheet integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link rel=stylesheet type=text/css href=https://dasarpai.github.io/css/asciinema-player.css><script src=https://code.jquery.com/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><link rel=stylesheet href=../../css/custom.css><script src=../../js/lunr.js></script></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-light nav-shadow flex-column flex-md-row td-navbar"><a id=agones-top class=navbar-brand href=../../><svg id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg> <span class="text-uppercase fw-bold">Blowfish</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=../../><span class=active>Data Science</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/docs/><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/docs/><span>Docs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../community/><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/docs/shortcodes/><span>Short Codes</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/blog/><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://github.com/nunocoracao/blowfish><span></span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href>GitHub</a></li><li class="nav-item dropdown d-none d-lg-block"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Release</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://development.agones.dev>Development</a>
<a class=dropdown-item href=https://agones.dev></a><a class=dropdown-item href=https://1-47-0.agones.dev>1.47.0</a>
<a class=dropdown-item href=https://1-46-0.agones.dev>1.46.0</a>
<a class=dropdown-item href=https://1-45-0.agones.dev>1.45.0</a>
<a class=dropdown-item href=https://1-44-0.agones.dev>1.44.0</a>
<a class=dropdown-item href=https://1-43-0.agones.dev>1.43.0</a>
<a class=dropdown-item href=https://1-42-0.agones.dev>1.42.0</a>
<a class=dropdown-item href=https://1-41-0.agones.dev>1.41.0</a>
<a class=dropdown-item href=https://1-40-0.agones.dev>1.40.0</a>
<a class=dropdown-item href=https://1-39-0.agones.dev>1.39.0</a>
<a class=dropdown-item href=https://1-38-0.agones.dev>1.38.0</a>
<a class=dropdown-item href=https://1-37-0.agones.dev>1.37.0</a>
<a class=dropdown-item href=https://1-36-0.agones.dev>1.36.0</a>
<a class=dropdown-item href=https://1-35-0.agones.dev>1.35.0</a>
<a class=dropdown-item href=https://1-34-0.agones.dev>1.34.0</a>
<a class=dropdown-item href=https://1-33-0.agones.dev>1.33.0</a>
<a class=dropdown-item href=https://1-32-0.agones.dev>1.32.0</a>
<a class=dropdown-item href=https://1-31-0.agones.dev>1.31.0</a></div></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><ul class=dropdown-menu><li><a class=dropdown-item href=../../it/>Italiano</a></li></ul></div></li></ul></div><div class="navbar-nav mx-lg-2 d-none d-lg-block"></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><div class=row><div class=col-md-2></div><div class=col-md-8><p><img src=../../assets/images/dspost/dsp6077-Introduction-to-ML-Model-deployment.jpg alt="Introduction to AI Model Deployement"></p><h1 id=introduction-to-ai-model-deployment>Introduction to AI Model deployment<a class=td-heading-self-link href=#introduction-to-ai-model-deployment aria-label="Heading self-link"></a></h1><h2 id=big-players>Big Players<a class=td-heading-self-link href=#big-players aria-label="Heading self-link"></a></h2><ul><li>Amazon<ul><li>Amazon has many products and one of their product is <strong>AWS Cloud</strong>. Under this product they sell IT infrastructure (storage, memory, network, VM, webhosting etc.)</li><li><strong>Amazon SageMaker</strong> is Cloud based Machine Learning Platform, and this is one of the product under AWS Cloud.</li><li>Amazon SageMaker can be used to train AI model, host AI model, monitor the model and hosts many other services which any Data Science project need from data gathering to model serving.</li><li>AWS is oldest cloud service provider in the market.</li><li>AWS Sagemaker was launched in Nov'17.</li></ul></li><li>Google<ul><li>Google has hundreds of products like gmail, youtube, google drive etc. One of their product is called <strong>Google Cloud</strong>. Under this product they sell IT infrastrcture like Amazon sells under AWS.</li><li><strong>VertexAI</strong> is Cloud based Machine Learning platform of Google. VertexAI is part of Google Cloud.</li><li>VertexAI can be used to train AI Model,host AI model, monitor the model etc.</li><li>VertexAI was launched in Jun'21</li></ul></li><li>Microsoft<ul><li>Like Amazon&rsquo;s cloud platform which is called AWS Cloud, Microsoft&rsquo;s cloud plateform is called <strong>Azure</strong>.</li><li>Microsoft&rsquo;s AI product is called <strong>Azure Machine Learning</strong>.</li><li>Today (Jul'23) Azure Machine Learning has has most of the capabilites than any other player&rsquo;s AI product.</li><li>Azure Machine Learning was launched Feb'14</li></ul></li></ul><h2 id=what-is-genai>What is GenAI?<a class=td-heading-self-link href=#what-is-genai aria-label="Heading self-link"></a></h2><p>There are many kinds of AI models like classifier models, regressor models, clustering models, reinforcement models, etc. An AI model which has the ability to generate text, images, video, and music is called GenAI. They all take inspiration from the human brain, therefore they all have neural network (NN) architecture. There are dozens (if not hundreds) types of NN architecture that can be used to create different kinds of AI models. The type of NN architecture depends upon the data which is used for developing the model and the problem which we want to solve using AI model. Researchers in universities or big corporations like Google, Facebook, Amazon, and Microsoft keep developing new architecture, and using these architectures they develop the foundational models. Once foundational models are developed, they release a research paper. In this, they inform the world what architecture they used, what data they used, what parameters (weights & biases) the model has learned, what are the results of their product and compare that with other existing models. They can develop these foundational models with one set of hyperparameters, and they can release these foundational models of different sizes (it depends upon the number of parameters used). AI product builders pick up these foundational models and fine-tune these based on the exact business problem in their hands. Which foundational model do they choose, it also depends upon the size of the model, the kind of data it has used to create those foundational models, and what was the performance of the model on a similar task which the product developer want to solve.</p><h3 id=what-is-large-language-model>What is Large Language Model?<a class=td-heading-self-link href=#what-is-large-language-model aria-label="Heading self-link"></a></h3><p>Large Language Models or LLM are the foundational models developed by researchers. They are very big in the size. For example, GPT3 was a 175 bn parameter model, PaLM was 540 bn parameter model. You cannot load these models on a normal machine for the prediction, forget about fine-tuning or customization. Therefore, most of the time you will find these LLMs are made available as service from the cloud providers like AWS SageMaker, Azure Machine Learning, VertexAI etc.</p><ul><li>LLaMa (Large Language Model Meta AI) is LLM family of state-of-the-art open-access large language models released by Meta. It is 65 bn parameter model. Other Models at Meta are <a href=https://huggingface.co/facebook>Meta Models at HuggingFace</a></li><li>GPT is LLM family of state-of-art model language models developed by OpenAI. ChatGPT (OpenAI&rsquo;s very famous Product) is based on GPT3.5 model. Other models of OpenAI are <a href=https://huggingface.co/openai>OpenAI Models at HuggingFace</a></li><li>PaLM is LLM family state of the art model from Google. Other Models at google are <a href=../../dsblog/model-garden-of-vertexai>VertexAI Model Garden</a>
<a href=https://huggingface.co/google>https://huggingface.co/google</a></li></ul><h3 id=what-is-foundational-model>What is Foundational Model?<a class=td-heading-self-link href=#what-is-foundational-model aria-label="Heading self-link"></a></h3><p>A foundational model is developed by researchers using a huge corpus of different types of data. They are built of unique architecture. These can use fine-tuned for many kinds of downstream tasks like classification, generation, translation, etc.</p><ul><li><strong>Foundational Models Developed by OpenAI:</strong> clip-vit, diffusers-cd, diffusers-ct, imagegpt, GPT, jukebox, shap-e, shap, whisper</li><li><strong>Foundational Models Developed by Microsoft:</strong> amos, beit, BioGPT, BiomedCLIP, BiomedNLP, BiomedVLP, bloom, ClimaX, cocoLM, CodeBert, CodeExecutor, CodeGPT, CodeReviewer, conditional, cvt, deberta, DialoGPT, DialogRPT, dit, dolly, focalnet, git, GODEL, graphcodebert, infoxlm, layoutlm, longcoder, lts, markuplm, mdeberta, MiniLM, mpnet, Multilingual, Promptist, prophetnet, reacc, resnet, speecht5, SportsBERT, ssr, swin, table, tapex, trocr, unihanlm, unilm, unispeech, unixcoder, vq, wavlm, xclip, xdoc, xlm, xprophetnet, xtremedistil</li><li><strong>Foundational Models of Google:</strong> BERT, bert2bert, bigbird, bit, byt5, canine, ddpm, deeplabv3, deplot, efficientnet, electra, flan, fnet, long, matcha, maxim, mobileBERT, mobilenet, mt5, multiberts, muril, music, ncsnpp, owlvit, pegasus, pix2struct, realm, reformer, rembert, roberta2roberta, switch, T5, tapas, ul2, umt5, VIT, vivit</li><li><strong>Foundational Models Developed by Meta:</strong> BART blenderbot, contriever, convnext, convnextv2, data2vec, deformable, deit, DETR, dino, dinov2, DiT, dpr, dragon, encodec, esm, esm1b, esm1v, esm2, esmfold, FairBERTa, fastspeech2, fastText, flava, galactica, genre, hubert, ic-gan, incoder, levit, m2m100, mask2former, maskformer, mBart, mcontriever, mgenre, MMS, muppet, musicgen, nllb, npm, opt, perturber, rag, regnet, roberta, roscoe, s2t, sam, spar, stylenerf, tart, textless, timesformer, tts-transformer, unit-hifigan, vc1, vit, wav2vec2, wmt19, wmt21, xglm, xlm, xm, xmod</li></ul><h3 id=deploying-a-llm-on-sagemaker>Deploying a LLM on SageMaker<a class=td-heading-self-link href=#deploying-a-llm-on-sagemaker aria-label="Heading self-link"></a></h3><h4 id=1-library-installation-and-setup>1. Library Installation and Setup<a class=td-heading-self-link href=#1-library-installation-and-setup aria-label="Heading self-link"></a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=err>!</span><span class=n>pip</span> <span class=n>install</span> <span class=s2>&#34;sagemaker==2.163.0&#34;</span> <span class=o>--</span><span class=n>upgrade</span> <span class=o>--</span><span class=n>quiet</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>sagemaker</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>boto3</span>
</span></span><span class=line><span class=cl><span class=n>sess</span> <span class=o>=</span> <span class=n>sagemaker</span><span class=o>.</span><span class=n>Session</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>sagemaker_session_bucket</span><span class=o>=</span><span class=kc>None</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>sagemaker_session_bucket</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>sess</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># set to default bucket if a bucket name is not given</span>
</span></span><span class=line><span class=cl>    <span class=n>sagemaker_session_bucket</span> <span class=o>=</span> <span class=n>sess</span><span class=o>.</span><span class=n>default_bucket</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>role</span> <span class=o>=</span> <span class=n>sagemaker</span><span class=o>.</span><span class=n>get_execution_role</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>except</span> <span class=ne>ValueError</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>iam</span> <span class=o>=</span> <span class=n>boto3</span><span class=o>.</span><span class=n>client</span><span class=p>(</span><span class=s1>&#39;iam&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>role</span> <span class=o>=</span> <span class=n>iam</span><span class=o>.</span><span class=n>get_role</span><span class=p>(</span><span class=n>RoleName</span><span class=o>=</span><span class=s1>&#39;sagemaker_execution_role&#39;</span><span class=p>)[</span><span class=s1>&#39;Role&#39;</span><span class=p>][</span><span class=s1>&#39;Arn&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>sess</span> <span class=o>=</span> <span class=n>sagemaker</span><span class=o>.</span><span class=n>Session</span><span class=p>(</span><span class=n>default_bucket</span><span class=o>=</span><span class=n>sagemaker_session_bucket</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;sagemaker role arn: </span><span class=si>{</span><span class=n>role</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;sagemaker session region: </span><span class=si>{</span><span class=n>sess</span><span class=o>.</span><span class=n>boto_region_name</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><h4 id=2-retrieve-the-necessary-sagemaker-container-for-tgi-deployment>2. Retrieve the necessary SageMaker container for TGI deployment<a class=td-heading-self-link href=#2-retrieve-the-necessary-sagemaker-container-for-tgi-deployment aria-label="Heading self-link"></a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sagemaker.huggingface</span> <span class=kn>import</span> <span class=n>get_huggingface_llm_image_uri</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># retrieve the llm image uri</span>
</span></span><span class=line><span class=cl><span class=n>llm_image</span> <span class=o>=</span> <span class=n>get_huggingface_llm_image_uri</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;huggingface&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>version</span><span class=o>=</span><span class=s2>&#34;0.8.2&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># print ecr image uri</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;llm image uri: </span><span class=si>{</span><span class=n>llm_image</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><h4 id=3-load-the-model>3. Load the Model<a class=td-heading-self-link href=#3-load-the-model aria-label="Heading self-link"></a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>json</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sagemaker.huggingface</span> <span class=kn>import</span> <span class=n>HuggingFaceModel</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define Model and Endpoint configuration parameter</span>
</span></span><span class=line><span class=cl><span class=n>config</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;HF_MODEL_ID&#39;</span><span class=p>:</span> <span class=s2>&#34;decapoda-research/llama-7b-hf&#34;</span><span class=p>,</span> <span class=c1># model_id from hf.co/models</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;SM_NUM_GPUS&#39;</span><span class=p>:</span> <span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>(</span><span class=n>number_of_gpu</span><span class=p>),</span> <span class=c1># Number of GPU used per replica</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;MAX_INPUT_LENGTH&#39;</span><span class=p>:</span> <span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>(</span><span class=mi>1024</span><span class=p>),</span>  <span class=c1># Max length of input text</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;MAX_TOTAL_TOKENS&#39;</span><span class=p>:</span> <span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>(</span><span class=mi>2048</span><span class=p>),</span>  <span class=c1># Max length of the generation (including input text)</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h4 id=4-create-huggingfacemodel-with-the-image-uri>4. Create HuggingFaceModel with the image uri<a class=td-heading-self-link href=#4-create-huggingfacemodel-with-the-image-uri aria-label="Heading self-link"></a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>llm_model</span> <span class=o>=</span> <span class=n>HuggingFaceModel</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>role</span><span class=o>=</span><span class=n>role</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>image_uri</span><span class=o>=</span><span class=n>llm_image</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>env</span><span class=o>=</span><span class=n>config</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><h4 id=5-deploy-the-model-on-sagemaker-instance-creating-endpoint>5. Deploy the Model on SageMaker Instance (Creating Endpoint)<a class=td-heading-self-link href=#5-deploy-the-model-on-sagemaker-instance-creating-endpoint aria-label="Heading self-link"></a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>instance_type</span> <span class=o>=</span> <span class=s2>&#34;ml.g5.12xlarge&#34;</span>
</span></span><span class=line><span class=cl><span class=n>number_of_gpu</span> <span class=o>=</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl><span class=n>health_check_timeout</span> <span class=o>=</span> <span class=mi>300</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>llm</span> <span class=o>=</span> <span class=n>llm_model</span><span class=o>.</span><span class=n>deploy</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>initial_instance_count</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>instance_type</span><span class=o>=</span><span class=n>instance_type</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>container_startup_health_check_timeout</span><span class=o>=</span><span class=n>health_check_timeout</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><h4 id=6-test-your-deployment>6. Test your deployment<a class=td-heading-self-link href=#6-test-your-deployment aria-label="Heading self-link"></a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>llm</span><span class=o>.</span><span class=n>predict</span><span class=p>({</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;inputs&#34;</span><span class=p>:</span> <span class=s2>&#34;My name is Hari Thapliyal and I am Data Scientist.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>})</span>
</span></span></code></pre></div><h3 id=model-installation-locaion>Model Installation Locaion<a class=td-heading-self-link href=#model-installation-locaion aria-label="Heading self-link"></a></h3><p>Model can be installed on Local machine or on Public cloud or private cloud. Model can deployed on Linux, MacOS, Andorid, iPhone, or Windows Machine</p><ul><li>Local Machine<ul><li>Linux</li><li>MacOS</li><li>Windows</li></ul></li><li>Cloud<ul><li>Google Cloud</li><li>AWS Cloud</li><li>Azure Cloud</li></ul></li><li>Cloud AI Platform<ul><li>AWS SageMaker</li><li>VertexAI</li><li>Azure AI</li></ul></li></ul><h3 id=open-source-deep-learning-model-development-frameworks>Open-Source Deep Learning Model Development Frameworks<a class=td-heading-self-link href=#open-source-deep-learning-model-development-frameworks aria-label="Heading self-link"></a></h3><ul><li><strong>PyTorch</strong> deep learning framework developed by Facebook&rsquo;s AI Research (FAIR) lab. It has gained significant popularity among researchers and developers due to its flexibility, dynamic computation graph, and intuitive API. PyTorch is known for its ease of use and strong support for custom operations and dynamic neural networks. It is good for quick AI model development, fast prototyping is possible.</li><li><strong>TensorFlow 2.0+</strong>: TensorFlow, underwent significant changes with the release of TensorFlow 2.0. It now includes eager execution by default, making it more intuitive and Pythonic. TensorFlow&rsquo;s updated version has better integration with Keras, simplifying the process of building and training models.</li><li><strong>Keras</strong> deep learning API written in Python that acts as a high-level interface to other deep learning frameworks, including TensorFlow, Pytorch and Microsoft Cognitive Toolkit (CNTK). It provides a simple and user-friendly interface for building and training neural networks, making it a popular choice for beginners and quick prototyping.</li><li><strong>Apache MXNet</strong> deep learning framework developed by Apache Software Foundation. It offers both symbolic and imperative programming models, allowing developers to choose between static and dynamic computation graphs. MXNet is designed for efficiency and scalability, making it suitable for a wide range of applications.</li><li><strong>Gluon</strong> is an interface for Apache MXNet that provides a high-level abstraction for building deep learning models. It aims to combine the ease of use of dynamic computation graphs with the performance advantages of static computation graphs.</li><li><strong>Caffe</strong> deep learning framework developed by Berkeley Vision and Learning Center (BVLC). It is known for its efficiency, especially for convolutional neural networks (CNNs) used in computer vision tasks. However, compared to newer frameworks, Caffe may be less flexible and harder to customize.</li><li><strong>Chainer</strong> is an open-source deep learning framework developed by Preferred Networks. Like PyTorch, Chainer emphasizes dynamic computation graphs and flexibility, allowing users to define complex neural network architectures more easily.</li><li><strong>Deep Java Library (DJL)</strong> is an open-source, high-level, engine-agnostic Java framework for deep learning.</li><li><strong><a href=https://parl.ai/docs/zoo.html>ParlAI</a></strong> : ParlAI (pronounced “par-lay”) is a python framework for sharing, training and testing dialogue models, from open-domain chitchat, to task-oriented dialogue, to visual question answering.</li></ul><h3 id=format-used-for-representing-deep-learning-models>Format Used for Representing Deep Learning Models.<a class=td-heading-self-link href=#format-used-for-representing-deep-learning-models aria-label="Heading self-link"></a></h3><ul><li><strong>ONNX</strong> (Open Neural Network Exchange): It allows interoperability between different deep learning frameworks, enabling users to move models between frameworks without rebuilding them from scratch. ONNX is joint effort of Microsoft, AWS, FAIR and IBM.</li><li><strong>TensorFlow SavedModel</strong> is a format specific to TensorFlow, and it is used for saving and restoring TensorFlow models. It includes the model&rsquo;s architecture, weights, and metadata, making it easy to save a trained model and load it later for inference or further training.</li><li><strong>PyTorch JIT</strong> (Just-In-Time) compilation allows users to export their models in a serialized format that can be later loaded and executed without the need for the original Python code. The JIT-compiled model can be used in production without requiring the entire PyTorch framework to be present.</li><li><strong>HDF5 or H5</strong> is a data format commonly used for storing and sharing large datasets and is also used to save deep learning models. Keras, which is integrated with TensorFlow, commonly uses the &ldquo;.h5&rdquo; format to save models. The HDF5 format allows saving the model architecture, weights, optimizer state, and other metadata.</li><li><strong>MLIR</strong> (Multi-Level Intermediate Representation) is an intermediate representation developed by LLVM that aims to provide a common format for various machine learning frameworks. While still in its early stages, MLIR holds promise for representing models in a unified format and enabling efficient transformations and optimizations across different frameworks.</li><li><strong>CoreML</strong> is a format developed by Apple for representing machine learning models specifically for deployment on Apple devices like iPhones and iPads. It allows developers to integrate pre-trained models into their iOS and macOS applications.</li><li><strong>Computation Graph Configuration</strong> is developed by Deeplearning4j (DL4J). It is a deep learning framework for Java, and it uses a specific configuration format for serializing its computation graphs, making it portable across different platforms.</li></ul><h3 id=deep-learning-model-inference-libraries>Deep Learning Model Inference Libraries<a class=td-heading-self-link href=#deep-learning-model-inference-libraries aria-label="Heading self-link"></a></h3><p>There are several products and libraries designed to accelerate deep learning inference and optimize model performance on specific hardware. These products and libraries focus on optimizing and accelerating deep learning inference on specific hardware platforms, allowing developers to deploy their models with improved performance and efficiency on target devices. Depending on the hardware architecture and deployment environment, choosing the right product or library can significantly impact the inference speed and overall user experience.</p><ul><li><strong>OpenVINO</strong> (Open Visual Inference & Neural Network Optimization) is an open-source toolkit developed by <strong>Intel</strong> that provides hardware-accelerated deep learning inference for Intel CPUs, integrated GPUs, and other hardware accelerators. It optimizes pre-trained models from various deep learning frameworks and deploys them efficiently on Intel-powered devices.</li><li><strong>DNNL</strong> (Deep Neural Network Library) formerly MKL-DNN, is a library developed by <strong>Intel</strong> to optimize deep learning workloads on Intel CPUs. It provides efficient implementations of primitives and functions for deep learning, improving performance on Intel architectures.</li><li><strong>nGraph</strong> is an open-source deep learning compiler developed by <strong>Intel</strong> it optimizes models for various hardware targets, including CPUs, GPUs, and specialized accelerators. It enables efficient execution of deep learning workloads across different hardware architectures.</li><li><strong>TensorRT</strong> (Tensor Runtime) is a high-performance deep learning inference optimizer and runtime library developed by <strong>NVIDIA</strong>.</li><li><strong>Core ML</strong> is a framework by <em>Apple</em>* that allows developers to integrate pre-trained machine learning models into iOS, macOS, watchOS, and tvOS applications. It supports hardware acceleration using the Apple Neural Engine on compatible devices, making it ideal for on-device inference.</li><li><strong>SNPE</strong> (Snapdragon Neural Processing Engine) is a deep learning inference SDK developed by <strong>Qualcomm</strong> for deployment on devices powered by Qualcomm Snapdragon processors. It optimizes deep learning models and accelerates inference on Snapdragon CPUs, GPUs, and DSPs.</li><li><strong>TVM</strong> (Tensor <strong>Virtual</strong> Machine) is open-source deep learning compiler stack developed by the <strong>Apache Software Foundation</strong>. It optimizes and deploys machine learning models on various hardware targets, including CPUs, GPUs, and specialized accelerators. It supports integration with different frameworks like TensorFlow, PyTorch, and ONNX.</li><li><strong>MIVisionX</strong> is developed by <strong>AMD</strong>. It includes tools and libraries for deep learning inference, image processing, and computer vision tasks, optimized for AMD GPUs and CPUs.</li><li><strong>DJL Serving</strong> is a high performance universal stand-alone model serving solution. It is deeloped by <strong>DJL</strong></li><li><strong>Huggingface Text Generation Interface (TGI)</strong></li><li><a href=https://aws.amazon.com/sagemaker/jumpstart/getting-started><strong>Amazon SageMaker JumpStart</strong></a> : Built-in algorithms with pretrained models from model hubs, pretrained foundation models, and prebuilt solutions to solve common use cases</li></ul><h3 id=ai-model-zoos>AI Model Zoos<a class=td-heading-self-link href=#ai-model-zoos aria-label="Heading self-link"></a></h3><ul><li><a href=https://parl.ai/docs/zoo.html>ParlAI</a> or <a href=https://github.com/facebookresearch/ParlAI>https://github.com/facebookresearch/ParlAI</a></li><li><a href=https://cloud.google.com/model-garden>Google Model Garden</a></li><li><a href="https://huggingface.co/?trending=model">Huggingface</a></li><li><a href=https://tfhub.dev/>TF Hub</a></li><li><a href=https://github.com/tensorflow/models/tree/master/official>Tensorflow on Git</a></li><li><a href=https://paperswithcode.com/>PaperwithCode</a></li><li><a href=https://github.com/openvinotoolkit/open_model_zoo>OpenVino Toolkit</a></li><li><a href=https://deci.ai/resources/videos/tutorial-infery/>Deci.ai</a></li><li><a href=https://www.elinux.org/Jetson_Zoo#Model_Zoo>Jetson Zoo</a></li><li><a href=https://modelzoo.co/>Modelzoo</a></li><li><a href=https://google.github.io/mediapipe/>Mediapipe on github</a></li><li><a href=https://github.com/magenta/magenta/tree/main/magenta/models>Magenta on github</a></li><li><a href=https://github.com/likedan/Awesome-CoreML-Models>Awesome CoreML on github</a></li><li><a href=https://github.com/PINTO0309/PINTO_model_zoo>Pinto Model zoo on github</a></li><li><a href=https://github.com/onnx/models>ONNX</a></li><li><a href=https://www.catalyzex.com/>CatalyZex</a></li></ul><p><strong>Author</strong><br>Dr Hari Thapliyaal<br>dasarpai.com<br>linkedin.com/in/harithapliyal</p><div class=category-section><h4 class=category-section__title>Categories:</h4><div class=category-badges><a href=../../categories/dsblog class=category-badge>dsblog</a></div></div><div class=td-tags><h4 class=td-tags__title>Tags:</h4><div class=category-badges><a href=../../tags/mlops class=category-badge>MLOps</a><a href=../../tags/model-deployment class=category-badge>Model Deployment</a><a href=../../tags/machine-learning class=category-badge>Machine Learning</a><a href=../../tags/devops class=category-badge>DevOps</a><a href=../../tags/production-ai class=category-badge>Production AI</a><a href=../../tags/cloud-computing class=category-badge>Cloud Computing</a><a href=../../tags/model-serving class=category-badge>Model Serving</a></div></div><div class=td-author-box><div class=td-author-box__info><h4 class=td-author-box__name>Blowfish</h4><p class=td-author-box__bio>A powerful, lightweight theme for Hugo.</p><div class=td-author-box__links><a href target=_blank rel=noopener aria-label></a><a href target=_blank rel=noopener aria-label></a></div></div></div><div class=td-social-share><h4 class=td-social-share__title>Share this article:</h4><ul class=td-social-share__list><div class=social-share><a href="https://twitter.com/intent/tweet?text=Introduction%20to%20ML%20Model%20Deployment&url=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fIntroduction-to-ML-Model-deployment%2f" target=_blank rel=noopener aria-label="Share on Twitter"><i class="fab fa-twitter"></i>
</a><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fIntroduction-to-ML-Model-deployment%2f" target=_blank rel=noopener aria-label="Share on Facebook"><i class="fab fa-facebook"></i>
</a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fIntroduction-to-ML-Model-deployment%2f&title=Introduction%20to%20ML%20Model%20Deployment" target=_blank rel=noopener aria-label="Share on LinkedIn"><i class="fab fa-linkedin"></i>
</a><a href="https://www.reddit.com/submit?url=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fIntroduction-to-ML-Model-deployment%2f&title=Introduction%20to%20ML%20Model%20Deployment" target=_blank rel=noopener aria-label="Share on Reddit"><i class="fab fa-reddit"></i>
</a><a href="mailto:?subject=Introduction%20to%20ML%20Model%20Deployment&body=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fIntroduction-to-ML-Model-deployment%2f" aria-label="Share via Email"><i class="fas fa-envelope"></i></a></div></ul></div><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href=../../dsblog/AWS-SageMaker-Jumpstart-Models/ aria-label="Previous page"><div class=td-pager__meta><i class="fa-solid fa-angle-left"></i>
<span class=td-pager__meta-label><b>Previous:</b></span>
<span class=td-pager__meta-title>AWS SageMaker Jumpstart Models</span></div></a><a class="td-pager__link td-pager__link--next" href=../../dsblog/AI-Product-and-Services-from-Google-Azure-and-AWS/ aria-label="Next page"><div class=td-pager__meta><span class=td-pager__meta-label><b>Next:</b></span>
<span class=td-pager__meta-title>AI Product and Services from Google, Azure and AWS</span>
<i class="fa-solid fa-angle-right"></i></div></a></ul></div><div class=col-md-2></div></div></main><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class=col-2><a href=https://dasarpai.com target=_blank rel=noopener><img src=https://dasarpai.github.io/assets/images/site-logo.png alt=dasarpAI width=100 style=border-radius:12px></a></div><div class=col-8><div class=row></div><div class=row></div></div><div class=col-2></div><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"></div></div></div></footer></div><script src=../../js/main.min.4e728f6d1777a74bef81c238fd7b573f42f1848a910503e07f877f49f1f442cb.js integrity="sha256-TnKPbRd3p0vvgcI4/XtXP0LxhIqRBQPgf4d/SfH0Qss=" crossorigin=anonymous></script><script defer src=../../js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=../../js/tabpane-persist.js></script><script src=https://dasarpai.github.io/js/asciinema-player.js></script><script>(function(){var e=document.querySelector("#td-section-nav");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></body></html>