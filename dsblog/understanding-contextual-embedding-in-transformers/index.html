<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.147.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href="https://localhost:1313/favicons/favicon.ico?v=1"><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-16x16.png?v=1" sizes=16x16><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-32x32.png?v=1" sizes=32x32><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><title>Understanding Contextual Embedding in Transformers | Blowfish</title><meta property="og:url" content="https://localhost:1313/dsblog/understanding-contextual-embedding-in-transformers/">
<meta property="og:site_name" content="Blowfish"><meta property="og:title" content="Understanding Contextual Embedding in Transformers"><meta property="og:description" content="Introduction Embedding can be confusing for many people, and contextual embedding performed by transformers can be even more perplexing. Even after gaining an understanding, many questions remain. In this article, we aim to address the following questions."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="dsblog"><meta property="article:published_time" content="2025-01-30T00:00:00+00:00"><meta property="article:modified_time" content="2025-01-30T00:00:00+00:00"><meta property="article:tag" content="Transformers"><meta property="article:tag" content="Embeddings"><meta property="article:tag" content="Deep Learning"><meta itemprop=name content="Understanding Contextual Embedding in Transformers"><meta itemprop=description content="Introduction Embedding can be confusing for many people, and contextual embedding performed by transformers can be even more perplexing. Even after gaining an understanding, many questions remain. In this article, we aim to address the following questions."><meta itemprop=datePublished content="2025-01-30T00:00:00+00:00"><meta itemprop=dateModified content="2025-01-30T00:00:00+00:00"><meta itemprop=wordCount content="1911"><meta itemprop=keywords content="Contextual Embedding in Transformers,How Transformers Handle Context,What is Fixed Embedding,How Contextural Embedding is Generated,What will be the output size of attention formula softmax,What is meaning of a LLM has context length of 2 million tokens,How many attention layers we keep in transformer like gpt4"><meta name=twitter:card content="summary"><meta name=twitter:title content="Understanding Contextual Embedding in Transformers"><meta name=twitter:description content="Introduction Embedding can be confusing for many people, and contextual embedding performed by transformers can be even more perplexing. Even after gaining an understanding, many questions remain. In this article, we aim to address the following questions."><link rel=preload href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css as=style integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css rel=stylesheet integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link rel=stylesheet type=text/css href=https://localhost:1313/css/asciinema-player.css><script src=https://code.jquery.com/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><link rel=stylesheet href=https://localhost:1313/css/custom.css><script src=https://localhost:1313/js/lunr.js></script></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-light nav-shadow flex-column flex-md-row td-navbar"><a id=agones-top class=navbar-brand href=https://localhost:1313/><svg id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg> <span class="text-uppercase fw-bold">Blowfish</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=https://localhost:1313/><span class=active>Data Science</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Docs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/community/><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/shortcodes/><span>Short Codes</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/blog/><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://github.com/nunocoracao/blowfish><span></span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href>GitHub</a></li><li class="nav-item dropdown d-none d-lg-block"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Release</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://development.agones.dev>Development</a>
<a class=dropdown-item href=https://agones.dev></a><a class=dropdown-item href=https://1-47-0.agones.dev>1.47.0</a>
<a class=dropdown-item href=https://1-46-0.agones.dev>1.46.0</a>
<a class=dropdown-item href=https://1-45-0.agones.dev>1.45.0</a>
<a class=dropdown-item href=https://1-44-0.agones.dev>1.44.0</a>
<a class=dropdown-item href=https://1-43-0.agones.dev>1.43.0</a>
<a class=dropdown-item href=https://1-42-0.agones.dev>1.42.0</a>
<a class=dropdown-item href=https://1-41-0.agones.dev>1.41.0</a>
<a class=dropdown-item href=https://1-40-0.agones.dev>1.40.0</a>
<a class=dropdown-item href=https://1-39-0.agones.dev>1.39.0</a>
<a class=dropdown-item href=https://1-38-0.agones.dev>1.38.0</a>
<a class=dropdown-item href=https://1-37-0.agones.dev>1.37.0</a>
<a class=dropdown-item href=https://1-36-0.agones.dev>1.36.0</a>
<a class=dropdown-item href=https://1-35-0.agones.dev>1.35.0</a>
<a class=dropdown-item href=https://1-34-0.agones.dev>1.34.0</a>
<a class=dropdown-item href=https://1-33-0.agones.dev>1.33.0</a>
<a class=dropdown-item href=https://1-32-0.agones.dev>1.32.0</a>
<a class=dropdown-item href=https://1-31-0.agones.dev>1.31.0</a></div></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><ul class=dropdown-menu><li><a class=dropdown-item href=https://localhost:1313/it/>Italiano</a></li></ul></div></li></ul></div><div class="navbar-nav mx-lg-2 d-none d-lg-block"></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><div class=row><div class=col-md-2></div><div class=col-md-8><p><img src=https://localhost:1313/assets/images/dspost/dsp6214-Understanding-Contextual-Embedding-in-Transformer.jpg alt="Understanding Contextual Embedding in Transformers"></p><h2 id=introduction>Introduction<a class=td-heading-self-link href=#introduction aria-label="Heading self-link"></a></h2><p>Embedding can be confusing for many people, and contextual embedding performed by transformers can be even more perplexing. Even after gaining an understanding, many questions remain. In this article, we aim to address the following questions.</p><ul><li>What is Embedding?</li><li>What is Fixed Embedding?</li><li>How Transformers Handle Context</li><li>How this token &lsquo;bank&rsquo; and corresponding embedding is stored in embedding database?</li><li>How contextural embedding is generated?</li><li>What will be the output size of attention formula softmax?</li><li>What is meaning of a LLM has context length of 2 million tokens?</li><li>How many attention layers we keep in transformer like gpt4?</li><li>What is the meaning of 96 attention layers, are they attention head count?</li></ul><h2 id=what-is-embedding>What is Embedding?<a class=td-heading-self-link href=#what-is-embedding aria-label="Heading self-link"></a></h2><p>An embedding is a way to represent discrete data (like words or tokens) as continuous vectors of numbers.</p><p>for example</p><pre tabindex=0><code>&#34;cat&#34; → [0.2, -0.5, 0.1, 0.8, ...]  # e.g., 100 dimensions vector
&#34;dog&#34; → [0.3, -0.4, 0.2, 0.7, ...]
</code></pre><p>Each dimension potentially represents some feature, they may be Masculinity/femininity, Animate/inanimate, Abstract/concrete etc.</p><p>Embedding helps</p><ul><li>convert discrete symbols into a numbers which can be processed by neural networks.</li><li>These numbers can also capture the relationships between words and in sementic operations like Queen = King - Man + Woman.</li><li>Reduce dimensionality (compared to one-hot encoding)</li></ul><h2 id=what-is-fixed-embedding>What is Fixed Embedding?<a class=td-heading-self-link href=#what-is-fixed-embedding aria-label="Heading self-link"></a></h2><p>A word &ldquo;bank&rdquo; can have multiple meaning linking to finance, dependence or river. In LLM when we do the tokenization in all case the token for this word will be same. But, what about embedding when &lsquo;bank&rsquo; word appears in different contexts</p><p><strong>Word Embeddings vs. Contextual Embeddings</strong></p><p>In traditional word embeddings (like Word2Vec or GloVe):</p><ul><li>Each word has a single, static embedding vector</li><li>&ldquo;bank&rdquo; would have the same embedding regardless of context</li><li>This is a limitation, as it can&rsquo;t distinguish between financial bank vs. river bank</li></ul><p>In contextual embedding (transormer models like BERT, GPT):</p><ul><li>Words get contextual embeddings that change based on surrounding text</li><li>&ldquo;bank&rdquo; gets different embedding representations depending on its usage</li><li>The model learns to create distinct representations for different meanings</li></ul><h2 id=how-transformers-handle-context>How Transformers Handle Context<a class=td-heading-self-link href=#how-transformers-handle-context aria-label="Heading self-link"></a></h2><p>Let&rsquo;s look at examples:</p><pre tabindex=0><code>&#34;I went to the bank to deposit money&#34;
&#34;The river bank was muddy&#34;
</code></pre><p>In these sentences:</p><ul><li>The initial token embeddings are combined with positional encodings</li><li>Each self-attention layer considers the relationships between &ldquo;bank&rdquo; and other words</li><li>Words like &ldquo;deposit,&rdquo; &ldquo;money,&rdquo; &ldquo;river,&rdquo; and &ldquo;muddy&rdquo; influence how &ldquo;bank&rdquo; is represented</li><li>The resulting contextual embeddings for &ldquo;bank&rdquo; will be different in each case</li></ul><p>Step 1. Initial Embedding:</p><ul><li>The word &ldquo;bank&rdquo; is first tokenized</li><li>It gets a base embedding from the embedding layer (typically there are different models for this work, these models are called embedding models)</li></ul><p>Step 2. Contextual Processing:</p><ul><li>Self-attention mechanisms look at surrounding words</li><li>Each attention head can focus on different aspects of meaning</li><li>Multiple transformer layers progressively refine the representation</li></ul><p>Step 3. Final Representation:</p><ul><li>The final embedding captures the specific meaning in that context</li><li>The financial &ldquo;bank&rdquo; embedding will be closer to other financial terms</li><li>The geographical &ldquo;bank&rdquo; embedding will be closer to other geographical terms</li></ul><p>Real-world Example</p><p>Consider these vectors (simplified for illustration):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Hypothetical embedding dimensions</span>
</span></span><span class=line><span class=cl><span class=n>bank</span> <span class=o>=</span> <span class=p>[</span><span class=mf>0.8</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>,</span> <span class=o>...</span><span class=p>]</span>  <span class=c1># if this is financal bank then it will be close to the words like &#34;money&#34;, &#34;deposit&#34;</span>
</span></span><span class=line><span class=cl><span class=n>bank</span> <span class=o>=</span> <span class=p>[</span><span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.7</span><span class=p>,</span> <span class=mf>0.9</span><span class=p>,</span> <span class=o>...</span><span class=p>]</span>  <span class=c1># if this is related to river then it will be close to words like &#34;water&#34;, swimming,&#34;river&#34;, &#34;shore&#34;</span>
</span></span></code></pre></div><p>The transformer model automatically generates these different representations based on context, allowing it to:</p><ul><li>Understand the appropriate meaning</li><li>Make relevant predictions</li><li>Handle ambiguity effectively</li></ul><p>This is why transformers are so powerful at handling polysemy - they don&rsquo;t just look up static word meanings but dynamically construct meanings based on context, much like humans do.</p><h2 id=how-this-token-bank-and-corresponding-embedding-is-stored-in-embedding-database>How this token &lsquo;bank&rsquo; and corresponding embedding is stored in embedding database?<a class=td-heading-self-link href=#how-this-token-bank-and-corresponding-embedding-is-stored-in-embedding-database aria-label="Heading self-link"></a></h2><p><strong>1. Token Storage (Vocabulary)</strong></p><ul><li>The tokenizer maintains a fixed vocabulary mapping</li><li>&ldquo;bank&rdquo; as a token is stored in a vocabulary dictionary/lookup table</li><li>Each token has a unique integer ID</li><li>Example vocabulary entry:</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>vocab</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;bank&#34;</span><span class=p>:</span> <span class=mi>2847</span><span class=p>,</span>  <span class=c1># unique ID</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;bank&#34;</span><span class=p>:</span> <span class=s2>&#34;▁bank&#34;</span><span class=p>,</span>  <span class=c1># actual token (might include special chars for word boundaries. Plus actual token need not be a complete word, for example you will not time one token for a word &#34;simultaneously&#34;)</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p><strong>2. Embedding Storage:</strong></p><ul><li>The embedding layer is implemented as a matrix/lookup table</li><li>Dimensions: (vocab_size × embedding_dim)</li><li>Each row corresponds to a token&rsquo;s base embedding vector</li><li>Example structure:</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>embedding_matrix</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># For token &#34;bank&#34; with ID 2847:</span>
</span></span><span class=line><span class=cl><span class=n>base_embedding</span> <span class=o>=</span> <span class=n>embedding_matrix</span><span class=p>[</span><span class=mi>2847</span><span class=p>]</span>  <span class=c1># Gets base embedding vector</span>
</span></span></code></pre></div><p>Key Points:</p><ul><li>There is only ONE base embedding vector per token</li><li>The contextual embeddings are generated on-the-fly during processing</li><li>The model doesn&rsquo;t store different embeddings for different meanings</li><li>The context-specific meanings emerge from the transformer layers</li></ul><p><strong>3. What&rsquo;s Actually Stored:</strong></p><pre tabindex=0><code>Token Storage:
&#34;bank&#34; -&gt; 2847 (ID)

Embedding Matrix:
Row 2847: [0.1, 0.3, -0.2, ...] (base embedding vector)
</code></pre><p><strong>4. During Processing:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># When processing &#34;financial bank&#34;:</span>
</span></span><span class=line><span class=cl><span class=n>input_ids</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;I went to the bank to deposit money&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>base_embeddings</span> <span class=o>=</span> <span class=n>embedding_matrix</span><span class=p>[</span><span class=n>input_ids</span><span class=p>]</span>  <span class=c1># Look up base embeddings</span>
</span></span><span class=line><span class=cl><span class=n>contextual_embeddings</span> <span class=o>=</span> <span class=n>transformer_layers</span><span class=p>(</span><span class=n>base_embeddings</span><span class=p>)</span>  <span class=c1># Generate context-specific embeddings</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># When processing &#34;river bank&#34;:</span>
</span></span><span class=line><span class=cl><span class=n>input_ids</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;The river bank was muddy&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>base_embeddings</span> <span class=o>=</span> <span class=n>embedding_matrix</span><span class=p>[</span><span class=n>input_ids</span><span class=p>]</span>  <span class=c1># Same base embeddings</span>
</span></span><span class=line><span class=cl><span class=n>contextual_embeddings</span> <span class=o>=</span> <span class=n>transformer_layers</span><span class=p>(</span><span class=n>base_embeddings</span><span class=p>)</span>  <span class=c1># Different context-specific embeddings</span>
</span></span></code></pre></div><p>The different meanings of &ldquo;bank&rdquo; emerge from:</p><ul><li>The transformer&rsquo;s attention mechanisms</li><li>Layer-by-layer contextual processing</li><li>Interaction with surrounding tokens</li></ul><p>Important Note:</p><ul><li>The model doesn&rsquo;t explicitly store different embeddings for different meanings</li><li>It learns to transform the base embedding based on context</li><li>This makes the system more efficient and flexible</li><li>The meaning disambiguation happens dynamically during processing</li></ul><p>Token &ldquo;bank&rdquo; → Base Embedding → Transformer Layers → Contextual Embedding
↑
(considers surrounding context)</p><p><strong>5. Post Processing</strong></p><ul><li>The contextual embeddings are used temporarily for the current task</li><li>They exist only during processing (encoding/decoding)</li><li>After the task is complete, only the results are kept, not the intermediate contextual embeddings</li></ul><h2 id=how-contextural-embedding-is-generated>How contextural embedding is generated?<a class=td-heading-self-link href=#how-contextural-embedding-is-generated aria-label="Heading self-link"></a></h2><p>To generate that contextual embedding we take help of that formula of Query (Q), Key (K), and Value (V) given in &ldquo;Attention is all you need&rdquo; paper?</p><p><strong>1. For each token&rsquo;s base embedding, the model creates:</strong></p><pre tabindex=0><code>Q = W_q × base_embedding  (Query)
K = W_k × base_embedding  (Key)
V = W_v × base_embedding  (Value)
</code></pre><p><strong>2. The attention formula then is:</strong></p><pre tabindex=0><code>Attention(Q, K, V) = softmax(QK^T/√d_k)V
</code></pre><p>Where:</p><ul><li>QK^T computes compatibility scores between tokens</li><li>√d_k is the scaling factor to prevent vanishing gradients</li><li>softmax creates attention weights</li><li>Final multiplication with V produces the contextual representation</li></ul><p><strong>3. Example for &ldquo;bank&rdquo;:</strong></p><ul><li><p>When processing &ldquo;bank&rdquo; in &ldquo;river bank&rdquo;:</p><ul><li>Q for &ldquo;bank&rdquo; will attend more strongly to &ldquo;river&rdquo;</li><li>The resulting contextual embedding shifts toward geographical meaning</li></ul></li><li><p>When processing &ldquo;bank&rdquo; in &ldquo;deposit money at the bank&rdquo;:</p><ul><li>Q for &ldquo;bank&rdquo; will attend more strongly to &ldquo;deposit&rdquo;, &ldquo;money&rdquo;</li><li>The resulting contextual embedding shifts toward financial meaning</li></ul></li></ul><p><strong>4. Multi-head attention:</strong></p><ul><li>Multiple sets of Q, K, V transformations</li><li>Each head can focus on different aspects of context</li><li>Results are concatenated and linearly transformed</li></ul><p>This mechanism allows the model to dynamically weigh different aspects of context when creating the contextual embeddings for each token.</p><h2 id=what-will-be-the-output-size-of-attention-formula-softmax>What will be the output size of attention formula softmax?<a class=td-heading-self-link href=#what-will-be-the-output-size-of-attention-formula-softmax aria-label="Heading self-link"></a></h2><p>If d_k=1024 and based embedding is 1024 then in that what will be the output size of attention formula softmax?</p><ol><li>Initial dimensions:</li></ol><ul><li>Base embedding dimension = 1024</li><li>d_k = 1024</li><li>Let&rsquo;s say we have a sequence length of n tokens</li></ul><ol start=2><li>Creating Q, K, V matrices:</li></ol><ul><li>Q: (n × d_k) = (n × 1024)</li><li>K: (n × d_k) = (n × 1024)</li><li>V: (n × d_k) = (n × 1024)</li></ul><ol start=3><li>In the attention formula:</li></ol><pre tabindex=0><code>QK^T/√d_k
</code></pre><ul><li>Q: (n × 1024)</li><li>K^T: (1024 × n)</li><li>QK^T results in: (n × n) matrix</li><li>This represents attention scores between every token pair. There are n token and it is nxn correlation matrix.</li></ul><ol start=4><li>After softmax:</li></ol><ul><li>Input: (n × n) matrix</li><li>Output: (n × n) matrix of attention weights</li><li>Each row sums to 1</li></ul><ol start=5><li>Final multiplication with V:</li></ol><pre tabindex=0><code>softmax(QK^T/√d_k)V
</code></pre><ul><li>Attention weights: (n × n)</li><li>V: (n × 1024)</li><li>Result: (n × 1024)</li></ul><p>So the softmax actually operates on an (n × n) attention matrix. The 1024 dimension (d_k) is used for the intermediate calculation but doesn&rsquo;t determine the final attention matrix size. The attention matrix size depends on the sequence length n (how many tokens are in your input).</p><p>For example, if you&rsquo;re processing 5 tokens:</p><ul><li>Attention matrix would be 5×5</li><li>Each token can attend to all other tokens</li><li>Final output preserves the sequence length and embedding dimension: 5×1024</li></ul><h2 id=what-is-meaning-of-a-llm-has-context-length-of-2-million-tokens>What is meaning of a LLM has context length of 2 million tokens?<a class=td-heading-self-link href=#what-is-meaning-of-a-llm-has-context-length-of-2-million-tokens aria-label="Heading self-link"></a></h2><p>In the case of LLM like gemini where they say context length is 2 million tokens, in that case this n is 2 millions?</p><p>Yes, theoretically in that case n would be 2 million, which reveals one of the biggest challenges in transformer architecture - the quadratic complexity problem.</p><p>Let&rsquo;s break down what this means:</p><ol><li>Memory Requirements:</li></ol><ul><li>For 2M tokens, attention matrix is 2M × 2M</li><li>That&rsquo;s 4 trillion (4 × 10^12) elements</li><li>Even with float16 precision, this needs ~8TB of memory just for one attention layer</li></ul><ol start=2><li>Computational Complexity:</li></ol><ul><li>Computing QK^T is O(n²)</li><li>For n = 2M, that&rsquo;s 4 trillion operations per attention layer</li><li>Multiple this by number of heads and layers</li></ul><p>This is why models like Gemini likely use optimization techniques to handle such long contexts:</p><ol><li>Sparse Attention:</li></ol><ul><li>Not all tokens attend to all other tokens</li><li>Use patterns like local attention, sliding windows</li><li>Reduces from O(n²) to O(n log n) or even O(n)</li></ul><ol start=2><li>Memory-efficient techniques:</li></ol><ul><li>Flash Attention</li><li>Sparse or linear transformers</li><li>State space models</li><li>Rotary position embeddings</li></ul><ol start=3><li>Attention Patterns like:</li></ol><ul><li>Sliding window attention</li><li>Dilated attention</li><li>Hierarchical attention</li></ul><p>Without these optimizations, the original transformer attention mechanism would be computationally infeasible for such long contexts.</p><h2 id=how-many-attention-layers-we-keep-in-transformer-like-gpt4>How many attention layers we keep in transformer like gpt4?<a class=td-heading-self-link href=#how-many-attention-layers-we-keep-in-transformer-like-gpt4 aria-label="Heading self-link"></a></h2><p>The exact architecture details of GPT-4 aren&rsquo;t public, but we can discuss what&rsquo;s known about transformer architectures in general and earlier models:</p><ol><li>Typical Structure:</li></ol><ul><li>Multiple transformer blocks/layers (anywhere from 12 to hundreds)</li><li>Each block contains:<ul><li>Multi-head attention layer</li><li>Feed-forward neural network</li><li>Layer normalization</li><li>Residual connections</li></ul></li></ul><ol start=2><li>Known Examples:</li></ol><ul><li>GPT-3 (175B parameter model):<ul><li>96 attention layers</li><li>96 heads per layer</li></ul></li><li>GPT-2:<ul><li>Large variant: 48 layers</li><li>Medium: 24 layers</li><li>Small: 12 layers</li></ul></li></ul><ol start=3><li>General Pattern:</li></ol><ul><li>Larger models tend to have more layers</li><li>Common to use powers of 2 for layer counts</li><li>Often balanced with number of attention heads</li><li>More layers = deeper understanding but more compute</li></ul><ol start=4><li>Trade-offs:</li></ol><ul><li>More layers = better performance but:<ul><li>Higher computational cost</li><li>More memory usage</li><li>Longer inference time</li><li>Risk of vanishing gradients</li></ul></li></ul><h2 id=what-is-the-meaning-of-96-attention-layers-are-they-attention-head-count>What is the meaning of 96 attention layers, are they attention head count?<a class=td-heading-self-link href=#what-is-the-meaning-of-96-attention-layers-are-they-attention-head-count aria-label="Heading self-link"></a></h2><p>N</p><ol><li>Layer vs Head Count:</li></ol><ul><li>Number of layers and heads are independent parameters</li><li>A model with 96 layers might have 12, 20, or 96 heads per layer</li><li>The head count doesn&rsquo;t need to match layer count</li></ul><ol start=2><li>Example Configurations:</li></ol><pre tabindex=0><code>GPT-3:
- 96 layers
- 96 attention heads per layer

BERT-base:
- 12 layers
- 12 attention heads per layer

BERT-large:
- 24 layers
- 16 attention heads per layer
</code></pre><ol start=3><li>Each Layer&rsquo;s Structure:</li></ol><pre tabindex=0><code>Transformer Layer
│
├── Multi-Head Attention (e.g., 12 heads)
│   ├── Head 1: (Q1,K1,V1)
│   ├── Head 2: (Q2,K2,V2)
│   └── ... up to Head 12
│
├── Feed Forward Network
└── Layer Normalization
</code></pre><ol start=4><li>Process Flow:</li></ol><ul><li>Input → Layer 1 (12 heads) → Layer 2 (12 heads) → &mldr; → Layer 96 (12 heads)</li><li>Each head in a layer processes the same input differently</li><li>Results from all heads are concatenated and projected</li></ul><p><strong>The number of heads is more about parallel processing of different attention patterns, while the number of layers is about depth of processing.</strong></p><div class=category-section><h4 class=category-section__title>Categories:</h4><div class=category-badges><a href=https://localhost:1313/categories/dsblog class=category-badge>dsblog</a></div></div><div class=td-tags><h4 class=td-tags__title>Tags:</h4><div class=category-badges><a href=https://localhost:1313/tags/transformers class=category-badge>Transformers</a><a href=https://localhost:1313/tags/embeddings class=category-badge>Embeddings</a><a href=https://localhost:1313/tags/deep-learning class=category-badge>Deep Learning</a></div></div><div class=td-author-box><div class=td-author-box__info><h4 class=td-author-box__name>Blowfish</h4><p class=td-author-box__bio>A powerful, lightweight theme for Hugo.</p><div class=td-author-box__links><a href target=_blank rel=noopener aria-label></a><a href target=_blank rel=noopener aria-label></a></div></div></div><div class=td-social-share><h4 class=td-social-share__title>Share this article:</h4><ul class=td-social-share__list><div class=social-share><a href="https://twitter.com/intent/tweet?text=Understanding%20Contextual%20Embedding%20in%20Transformers&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2funderstanding-contextual-embedding-in-transformers%2f" target=_blank rel=noopener aria-label="Share on Twitter"><i class="fab fa-twitter"></i>
</a><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2flocalhost%3a1313%2fdsblog%2funderstanding-contextual-embedding-in-transformers%2f" target=_blank rel=noopener aria-label="Share on Facebook"><i class="fab fa-facebook"></i>
</a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2funderstanding-contextual-embedding-in-transformers%2f&title=Understanding%20Contextual%20Embedding%20in%20Transformers" target=_blank rel=noopener aria-label="Share on LinkedIn"><i class="fab fa-linkedin"></i>
</a><a href="https://www.reddit.com/submit?url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2funderstanding-contextual-embedding-in-transformers%2f&title=Understanding%20Contextual%20Embedding%20in%20Transformers" target=_blank rel=noopener aria-label="Share on Reddit"><i class="fab fa-reddit"></i>
</a><a href="mailto:?subject=Understanding%20Contextual%20Embedding%20in%20Transformers&body=https%3a%2f%2flocalhost%3a1313%2fdsblog%2funderstanding-contextual-embedding-in-transformers%2f" aria-label="Share via Email"><i class="fas fa-envelope"></i></a></div></ul></div><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href=https://localhost:1313/dsblog/understanding-working-of-cnn/ aria-label="Previous page"><div class=td-pager__meta><i class="fa-solid fa-angle-left"></i>
<span class=td-pager__meta-label><b>Previous:</b></span>
<span class=td-pager__meta-title>Understanding the Working of CNN</span></div></a><a class="td-pager__link td-pager__link--next" href=https://localhost:1313/dsblog/exploring-tokenization-and-embedding-in-nlp/ aria-label="Next page"><div class=td-pager__meta><span class=td-pager__meta-label><b>Next:</b></span>
<span class=td-pager__meta-title>Exploring Tokenization and Embedding in NLP</span>
<i class="fa-solid fa-angle-right"></i></div></a></ul></div><div class=col-md-2></div></div></main><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class=col-2><a href=https://dasarpai.com target=_blank rel=noopener><img src=https://localhost:1313/assets/images/site-logo.png alt=dasarpAI width=100 style=border-radius:12px></a></div><div class=col-8><div class=row></div><div class=row></div></div><div class=col-2></div><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"></div></div></div></footer></div><script src=https://localhost:1313/js/main.min.7c31f98e62c36b0c9c834ce3f8260a0e21895dd6aa0773e81a64b104eae3b2e8.js integrity="sha256-fDH5jmLDawycg0zj+CYKDiGJXdaqB3PoGmSxBOrjsug=" crossorigin=anonymous></script><script defer src=https://localhost:1313/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=https://localhost:1313/js/tabpane-persist.js></script><script src=https://localhost:1313/js/asciinema-player.js></script><script>(function(){var e=document.querySelector("#td-section-nav");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></body></html>