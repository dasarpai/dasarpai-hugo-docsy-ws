<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.147.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href="https://localhost:1313/favicons/favicon.ico?v=1"><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-16x16.png?v=1" sizes=16x16><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-32x32.png?v=1" sizes=32x32><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><title>Transformers Demystified A Step-by-Step Guide | Blowfish</title><meta property="og:url" content="https://localhost:1313/dsblog/transformers-demystified-a-step-by-step-guide/">
<meta property="og:site_name" content="Blowfish"><meta property="og:title" content="Transformers Demystified A Step-by-Step Guide"><meta property="og:description" content="Transformers Demystified A Step-by-Step Guide All modern Transformers are based on a paper “Attention is all you need”
Introduction This was the mother paper of all the transformer architectures we see today around NLP, Multimodal, Deep Learning. It was presented by Ashish Vaswani et al from Deep Learning / Google in 2017. We will discuss following and anything whatever question/observation/idea I have."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="dsblog"><meta property="article:published_time" content="2024-07-25T00:00:00+00:00"><meta property="article:modified_time" content="2024-07-25T00:00:00+00:00"><meta property="article:tag" content="Transformers"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Neural Networks"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="AI Architecture"><meta itemprop=name content="Transformers Demystified A Step-by-Step Guide"><meta itemprop=description content="Transformers Demystified A Step-by-Step Guide All modern Transformers are based on a paper “Attention is all you need”
Introduction This was the mother paper of all the transformer architectures we see today around NLP, Multimodal, Deep Learning. It was presented by Ashish Vaswani et al from Deep Learning / Google in 2017. We will discuss following and anything whatever question/observation/idea I have."><meta itemprop=datePublished content="2024-07-25T00:00:00+00:00"><meta itemprop=dateModified content="2024-07-25T00:00:00+00:00"><meta itemprop=wordCount content="7248"><meta itemprop=keywords content="Transformer Architecture,Neural Networks,Attention Mechanism,Deep Learning Models,NLP Architecture,Machine Learning Models,AI Model Design,Language Processing"><meta name=twitter:card content="summary"><meta name=twitter:title content="Transformers Demystified A Step-by-Step Guide"><meta name=twitter:description content="Transformers Demystified A Step-by-Step Guide All modern Transformers are based on a paper “Attention is all you need”
Introduction This was the mother paper of all the transformer architectures we see today around NLP, Multimodal, Deep Learning. It was presented by Ashish Vaswani et al from Deep Learning / Google in 2017. We will discuss following and anything whatever question/observation/idea I have."><link rel=preload href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css as=style integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css rel=stylesheet integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link rel=stylesheet type=text/css href=https://localhost:1313/css/asciinema-player.css><script src=https://code.jquery.com/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><link rel=stylesheet href=https://localhost:1313/css/custom.css><script src=https://localhost:1313/js/lunr.js></script></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-light nav-shadow flex-column flex-md-row td-navbar"><a id=agones-top class=navbar-brand href=https://localhost:1313/><svg id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg> <span class="text-uppercase fw-bold">Blowfish</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=https://localhost:1313/><span class=active>Data Science</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Docs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/community/><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/shortcodes/><span>Short Codes</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/blog/><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://github.com/nunocoracao/blowfish><span></span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href>GitHub</a></li><li class="nav-item dropdown d-none d-lg-block"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Release</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://development.agones.dev>Development</a>
<a class=dropdown-item href=https://agones.dev></a><a class=dropdown-item href=https://1-47-0.agones.dev>1.47.0</a>
<a class=dropdown-item href=https://1-46-0.agones.dev>1.46.0</a>
<a class=dropdown-item href=https://1-45-0.agones.dev>1.45.0</a>
<a class=dropdown-item href=https://1-44-0.agones.dev>1.44.0</a>
<a class=dropdown-item href=https://1-43-0.agones.dev>1.43.0</a>
<a class=dropdown-item href=https://1-42-0.agones.dev>1.42.0</a>
<a class=dropdown-item href=https://1-41-0.agones.dev>1.41.0</a>
<a class=dropdown-item href=https://1-40-0.agones.dev>1.40.0</a>
<a class=dropdown-item href=https://1-39-0.agones.dev>1.39.0</a>
<a class=dropdown-item href=https://1-38-0.agones.dev>1.38.0</a>
<a class=dropdown-item href=https://1-37-0.agones.dev>1.37.0</a>
<a class=dropdown-item href=https://1-36-0.agones.dev>1.36.0</a>
<a class=dropdown-item href=https://1-35-0.agones.dev>1.35.0</a>
<a class=dropdown-item href=https://1-34-0.agones.dev>1.34.0</a>
<a class=dropdown-item href=https://1-33-0.agones.dev>1.33.0</a>
<a class=dropdown-item href=https://1-32-0.agones.dev>1.32.0</a>
<a class=dropdown-item href=https://1-31-0.agones.dev>1.31.0</a></div></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><ul class=dropdown-menu><li><a class=dropdown-item href=https://localhost:1313/it/>Italiano</a></li></ul></div></li></ul></div><div class="navbar-nav mx-lg-2 d-none d-lg-block"></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><div class=row><div class=col-md-2></div><div class=col-md-8><p><img src=https://localhost:1313/assets/images/dspost/dsp6113-transformers-demystified-a-step-by-step-guide.jpg alt="Transformers Demystified A Step-by-Step Guide"></p><h1 id=transformers-demystified-a-step-by-step-guide>Transformers Demystified A Step-by-Step Guide<a class=td-heading-self-link href=#transformers-demystified-a-step-by-step-guide aria-label="Heading self-link"></a></h1><p>All modern Transformers are based on a paper &ldquo;Attention is all you need&rdquo;</p><h2 id=introduction>Introduction<a class=td-heading-self-link href=#introduction aria-label="Heading self-link"></a></h2><p>This was the mother paper of all the transformer architectures we see today around NLP, Multimodal, Deep Learning. It was presented by Ashish Vaswani et al from Deep Learning / Google in 2017. We will discuss following and anything whatever question/observation/idea I have.</p><ul><li>The need
Why this paper was needed? What problem it solved?</li><li>What is transformer? What is encoder transformer? What is decoder transformer? What is encoder-decoder transformer?</li><li>What is embedding? What is need for embedding? What are different types of embedding? What embeddingg is proposed in this work</li><li>What benchmark dataset was used, what metrics were used and what was the performance of this model?</li><li>Finally we will looks all the calculations with one illustration.</li></ul><p>Encourage all to read this <a href=https://arxiv.org/abs/1706.03762>original paper</a>.</p><h2 id=what-was-need-of-this-work>What was need of this work?<a class=td-heading-self-link href=#what-was-need-of-this-work aria-label="Heading self-link"></a></h2><p>This paper addressed several limitations of previous sequence-to-sequence models used for tasks such as machine translation, text summarization, and other natural language processing (NLP) applications. The need for this paper arose from various challenges and limitations in existing models:</p><h3 id=limitations-of-previous-models>Limitations of Previous Models<a class=td-heading-self-link href=#limitations-of-previous-models aria-label="Heading self-link"></a></h3><ol><li><p><strong>Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) Networks</strong>:</p><ul><li><strong>Sequential Computation</strong>: RNNs and LSTMs process sequences step-by-step, which makes it difficult to parallelize computations and slows down training and inference.</li><li><strong>Long-Range Dependencies</strong>: RNNs and LSTMs struggle to capture dependencies in long sequences, leading to difficulties in understanding context over long distances.</li><li><strong>Gradient Issues</strong>: These models can suffer from vanishing or exploding gradient problems, particularly with long sequences.</li></ul></li><li><p><strong>Convolutional Neural Networks (CNNs)</strong>:</p><ul><li><strong>Fixed Context Size</strong>: CNNs have a fixed receptive field, which can limit their ability to capture long-range dependencies effectively.</li><li><strong>Complexity</strong>: Extending CNNs to capture longer contexts can significantly increase the model&rsquo;s complexity and computational cost.</li></ul></li></ol><p>The introduction of the Transformer architecture had a profound impact on the field of NLP and beyond. It paved the way for the development of large-scale pre-trained language models like BERT, GPT, and others, which have since become the foundation for many state-of-the-art AI applications. The principles of the Transformer architecture have also been adapted for other domains, such as image processing and reinforcement learning.</p><h2 id=nlp-tasks>NLP Tasks<a class=td-heading-self-link href=#nlp-tasks aria-label="Heading self-link"></a></h2><ol><li><p><strong>Text Classification</strong>:</p><ul><li><strong>Spam Detection</strong>: Classifying messages as spam or non-spam.</li><li><strong>Topic Classification</strong>: Categorizing text into predefined topics or categories.</li><li><strong>Sarcasm Detection</strong></li><li><strong>Offensive Language Detection</strong></li></ul></li><li><p><strong>Sentiment and Emotion Analysis</strong>: Determining the sentiment or emotional tone expressed in a text.</p></li><li><p><strong>Named Entity Recognition (NER)</strong>: Identifying and classifying named entities (e.g., people, organizations, locations) within a text.</p></li><li><p><strong>Part-of-Speech Tagging (POS Tagging)</strong>: Assigning parts of speech (e.g., nouns, verbs, adjectives) to each word in a text.</p></li><li><p><strong>Machine Translation</strong>: Translating text from one language to another (e.g., English to French).</p></li><li><p><strong>Language Modeling</strong>: Predicting the next word or character in a sequence, often used in generating text or improving other NLP tasks.</p></li><li><p><strong>Text Summarization</strong>:</p><ul><li><strong>Extractive Summarization</strong>: Extracting key sentences from a text to create a summary.</li><li><strong>Abstractive Summarization</strong>: Generating a concise summary that captures the main ideas of the text.</li></ul></li><li><p><strong>Question Answering</strong>: Providing answers to questions based on a given text or dataset.</p></li><li><p><strong>Text Generation</strong>: Generating coherent and contextually relevant text, such as in chatbots or story generation.</p></li><li><p><strong>Text Similarity</strong>: Measuring how similar two pieces of text are, which can be used in tasks like duplicate detection or paraphrase identification.</p></li><li><p><strong>Coreference Resolution</strong>: Identifying when different expressions in a text refer to the same entity.</p></li><li><p><strong>Speech Recognition</strong>: Converting spoken language into text.</p></li><li><p><strong>Speech Synthesis (Text-to-Speech)</strong>: Converting text into spoken language.</p></li><li><p><strong>Dialogue Systems</strong>:</p><ul><li><strong>Chatbots</strong>: Engaging in conversation with users.</li><li><strong>Virtual Assistants</strong>: Assisting users with tasks through natural language interactions.</li></ul></li><li><p><strong>Information Retrieval</strong>: Finding relevant information within large datasets or the web, such as search engines.</p></li><li><p><strong>Dependency Parsing</strong>: Analyzing the grammatical structure of a sentence to establish relationships between words.</p></li><li><p><strong>Grammar and Spelling Correction</strong>: Identifying and correcting grammatical errors and typos in text.</p></li><li><p><strong>Textual Entailment</strong>: Determining if one sentence logically follows from another.</p></li><li><p><strong>Word Sense Disambiguation</strong>: Identifying which sense of a word is used in a given context.</p></li><li><p><strong>Natural Language Inference (NLI)</strong>: Determining if a premise logically entails a hypothesis.</p></li></ol><p>Each of these tasks involves different techniques and models, often leveraging machine learning and deep learning to achieve state-of-the-art performance.</p><h2 id=background>Background<a class=td-heading-self-link href=#background aria-label="Heading self-link"></a></h2><p>Google Translate was launched on April 28, 2006. Initially, it was a statistical machine translation service that used United Nations and European Parliament transcripts to gather linguistic data.</p><p>It used a statistical machine translation (SMT) approach. This method relied on statistical models to translate text based on patterns found in large volumes of bilingual text corpora. The SMT system analyzed these patterns to make educated guesses about the most likely translations.</p><p>In 2016, Google Translate transitioned to using a neural machine translation (NMT) system, specifically the Google Neural Machine Translation (GNMT) system. This system uses deep learning techniques and neural networks to provide more accurate and natural translations by considering the entire sentence as a whole, rather than translating piece by piece.</p><h3 id=metrics>Metrics<a class=td-heading-self-link href=#metrics aria-label="Heading self-link"></a></h3><p>BLEU (Bilingual Evaluation Understudy) score, which measures the quality of machine-translated text against reference translations.</p><p>Google reported that the new NMT system achieved improvements ranging from 55% to 85% across various language pairs in terms of BLEU scores compared to their previous SMT system. This was a substantial leap in translation quality.</p><ul><li>For Chinese to English translations, the BLEU score improvement was reported to be around 24.2, a significant increase from the previous SMT system.</li><li>For English to French, the BLEU score improvement was noted as being around 5-8 points higher than the SMT system.</li></ul><h3 id=benchmarks>Benchmarks<a class=td-heading-self-link href=#benchmarks aria-label="Heading self-link"></a></h3><ol><li><p><strong>WMT 2014 English-to-German (En-De) Translation Task</strong>:</p><ul><li><strong>Dataset</strong>: The dataset consisted of 4.5 million sentence pairs.</li><li><strong>Performance</strong>: The Transformer model achieved a BLEU score of 28.4, which was a significant improvement over the previous state-of-the-art models.</li></ul></li><li><p><strong>WMT 2014 English-to-French (En-Fr) Translation Task</strong>:</p><ul><li><strong>Dataset</strong>: The dataset consisted of 36 million sentence pairs.</li><li><strong>Performance</strong>: The Transformer model achieved a BLEU score of 41.8, which also outperformed previous models.</li></ul></li></ol><h2 id=key-terms>Key terms<a class=td-heading-self-link href=#key-terms aria-label="Heading self-link"></a></h2><ol><li><p><strong>Transformer Architecture</strong>: The paper introduces the Transformer, a novel architecture solely based on attention mechanisms, dispensing with recurrence and convolutions entirely.</p></li><li><p><strong>Attention Mechanism</strong>: The core idea is the use of self-attention, allowing the model to weigh the importance of different words in a sentence when forming a representation of each word.</p></li><li><p><strong>Self-Attention</strong>: Self-attention allows each word to focus on different parts of the input sentence, making it easier for the model to understand context and relationships between words.</p></li><li><p><strong>Multi-Head Attention</strong>: Instead of performing a single attention function, the Transformer employs multiple attention heads, each focusing on different parts of the sentence, capturing diverse aspects of the information.</p></li><li><p><strong>Positional Encoding</strong>: Since the Transformer does not have recurrence, it uses positional encodings to give the model information about the position of each word in the sentence.</p></li><li><p><strong>Layer Normalization and Residual Connections</strong>: Each sub-layer (multi-head attention and feed-forward) is followed by layer normalization and residual connections, aiding in training deep networks.</p></li><li><p><strong>Encoder-Decoder Structure</strong>: The Transformer is composed of an encoder (which processes the input) and a decoder (which generates the output). Each consists of multiple layers of self-attention and feed-forward networks.</p></li><li><p><strong>Performance</strong>: The Transformer achieves state-of-the-art performance on several NLP tasks while being more parallelizable and faster to train than recurrent architectures like LSTMs and GRUs.</p></li><li><p><strong>Scalability</strong>: Due to its architecture, the Transformer scales well with the amount of available data and computational power, making it suitable for large-scale tasks.</p></li></ol><h2 id=with-encoder-only-architecture-we-can-do>With &ldquo;Encoder Only&rdquo; Architecture we can do.<a class=td-heading-self-link href=#with-encoder-only-architecture-we-can-do aria-label="Heading self-link"></a></h2><p>An encoder-only architecture, such as BERT (Bidirectional Encoder Representations from Transformers), is typically used for tasks that require understanding and representing input sequences without generating sequences. Here are some common NLP tasks that can be effectively handled using an encoder-only architecture:</p><ol><li><p><strong>Text Classification</strong>:</p><ul><li><strong>Sentiment Analysis</strong>: Determining the sentiment (positive, negative, neutral) expressed in a piece of text.</li><li><strong>Spam Detection</strong>: Classifying messages as spam or not spam.</li><li><strong>Topic Classification</strong>: Categorizing text into predefined topics or categories.</li></ul></li><li><p><strong>Named Entity Recognition (NER)</strong>: Identifying and classifying named entities (e.g., persons, organizations, locations) within the text.</p></li><li><p><strong>Part-of-Speech Tagging (POS Tagging)</strong>: Assigning parts of speech (e.g., nouns, verbs, adjectives) to each word in the text.</p></li><li><p><strong>Question Answering (Extractive)</strong>: Extracting an answer from a given context based on a query. For instance, answering questions from a passage of text.</p></li><li><p><strong>Textual Entailment</strong>: Determining whether a given hypothesis logically follows from a premise (also known as Natural Language Inference, NLI).</p></li><li><p><strong>Sentence Pair Classification</strong>:</p><ul><li><strong>Paraphrase Detection</strong>: Identifying whether two sentences are paraphrases of each other.</li><li><strong>Semantic Similarity</strong>: Measuring how similar two sentences or phrases are in meaning.</li></ul></li><li><p><strong>Coreference Resolution</strong>: Determining which expressions in a text refer to the same entity.</p></li><li><p><strong>Text Summarization (Extractive)</strong>: Selecting the most important sentences from a document to create a summary.</p></li><li><p><strong>Grammar and Spelling Correction</strong>: Identifying and correcting grammatical errors and typos in text.</p></li><li><p><strong>Information Retrieval</strong>: Ranking and retrieving relevant documents based on a query.</p></li><li><p><strong>Document Classification</strong>: Categorizing entire documents into classes or categories.</p></li><li><p><strong>Named Entity Disambiguation</strong>: Resolving ambiguities in named entities to identify the correct entity among potential candidates.</p></li><li><p><strong>Feature Extraction for Downstream Tasks</strong>: Generating contextualized embeddings from text to be used as features in other machine learning models.</p></li></ol><p>Encoder-only models are particularly effective in tasks that benefit from understanding the context and semantics of the input text, as they are designed to capture rich, contextual representations of the input data.</p><h2 id=with-decoder-only-architecture-we-can-do>With &ldquo;Decoder only&rdquo; Architecture we can do.<a class=td-heading-self-link href=#with-decoder-only-architecture-we-can-do aria-label="Heading self-link"></a></h2><p>Decoder-only architectures, such as GPT (Generative Pre-trained Transformer), are primarily designed for tasks involving text generation. These models are well-suited for autoregressive tasks where the goal is to predict or generate text based on a given context. Here are some common NLP tasks that can be effectively handled using a decoder-only architecture:</p><ol><li><p><strong>Text Generation</strong>:</p><ul><li><strong>Creative Writing</strong>: Generating coherent and contextually relevant creative content, such as stories, poems, or dialogues.</li><li><strong>Content Generation</strong>: Creating blog posts, articles, or other types of written content.</li></ul></li><li><p><strong>Language Modeling</strong>:</p><ul><li><strong>Next Word Prediction</strong>: Predicting the next word or token in a sequence given the preceding context.</li><li><strong>Completion</strong>: Providing completion for a partially written sentence or text.</li></ul></li><li><p><strong>Conversational AI</strong>:</p><ul><li><strong>Chatbots</strong>: Engaging in conversation with users, generating responses to user inputs.</li><li><strong>Virtual Assistants</strong>: Assisting with tasks through natural language interactions.</li></ul></li><li><p><strong>Text Summarization (Abstractive)</strong>: Generating a concise summary of a text that captures the main ideas, potentially rephrasing and reorganizing information.</p></li><li><p><strong>Machine Translation</strong>: Translating text from one language to another in an autoregressive manner, generating translated sentences token by token.</p></li><li><p><strong>Dialogue Generation</strong>:</p><ul><li><strong>Interactive Fiction</strong>: Generating dialogues in interactive fiction or role-playing scenarios.</li><li><strong>Conversational Agents</strong>: Generating responses in a conversation based on the context of the dialogue.</li></ul></li><li><p><strong>Storytelling</strong>: Creating narratives or expanding on a given prompt to generate a complete story or narrative.</p></li><li><p><strong>Autoregressive Text Editing</strong>: Modifying or editing text based on a given context, such as rewriting or expanding text.</p></li><li><p><strong>Text-based Games</strong>: Generating responses and interactions in text-based games or interactive storytelling environments.</p></li></ol><p>Decoder-only architectures excel in generating text sequences and modeling complex language patterns due to their autoregressive nature. They predict the next token in a sequence based on the previous tokens, which makes them ideal for tasks that involve creating or completing text.</p><h2 id=with-encoder-decoder-architecture-we-can-do-following-task>With Encoder-Decoder Architecture we can do following task.<a class=td-heading-self-link href=#with-encoder-decoder-architecture-we-can-do-following-task aria-label="Heading self-link"></a></h2><p>Encoder-decoder architectures, like those used in the original Transformer model and its derivatives (e.g., T5, BART), are particularly well-suited for tasks that involve transforming one sequence into another. These models leverage the encoder to process and understand the input sequence and the decoder to generate the output sequence. Here are some key tasks that benefit from an encoder-decoder architecture:</p><ol><li><p><strong>Machine Translation</strong>: Translating text from one language to another. The encoder processes the source language text, while the decoder generates the translated text in the target language.</p></li><li><p><strong>Text Summarization</strong>:</p><ul><li><strong>Abstractive Summarization</strong>: Generating a concise and coherent summary of a document, potentially rephrasing and synthesizing information from the source text.</li></ul></li><li><p><strong>Text-to-Text Tasks</strong>: Treating various NLP tasks as text-to-text problems, where both the input and output are sequences of text. Examples include:</p><ul><li><strong>Question Answering</strong>: Generating answers to questions based on a provided context or passage.</li><li><strong>Text Generation with Constraints</strong>: Generating text based on specific constraints or instructions.</li></ul></li><li><p><strong>Image Captioning</strong>: Generating a textual description of an image. The encoder processes features extracted from the image, and the decoder generates a descriptive sentence.</p></li><li><p><strong>Speech-to-Text</strong>: Converting spoken language (speech) into written text. The encoder processes the audio features, while the decoder generates the corresponding text.</p></li><li><p><strong>Text-Based Conversational Systems</strong>:</p><ul><li><strong>Dialogue Generation</strong>: Generating responses in a conversation where the input may be a previous dialogue context or user query, and the output is a coherent response.</li></ul></li><li><p><strong>Paraphrase Generation</strong>: Rewriting or generating paraphrased versions of a given text while preserving its meaning.</p></li><li><p><strong>Story Generation</strong>: Generating complete stories or narratives based on a prompt or initial context.</p></li><li><p><strong>Semantic Parsing</strong>: Converting natural language into a structured format or query (e.g., converting a sentence into a SQL query).</p></li><li><p><strong>Text Style Transfer</strong>: Transforming the style of a given text while preserving its original meaning (e.g., changing a formal text into an informal one).</p></li><li><p><strong>Multi-Modal Tasks</strong>: Combining multiple types of input (e.g., text and images) to generate output in one modality (e.g., generating text from images or audio).</p></li></ol><p>Encoder-decoder architectures are versatile and powerful for tasks that require generating output sequences based on complex input sequences, making them suitable for a wide range of applications in natural language processing and beyond.</p><h2 id=why-do-we-need-encoder-decoder-architectures>Why do we need encoder-decoder architectures?<a class=td-heading-self-link href=#why-do-we-need-encoder-decoder-architectures aria-label="Heading self-link"></a></h2><p>Both encoder-only and decoder-only architectures have their own strengths and are suited to different types of tasks. The choice between them often depends on the specific requirements of the task and the trade-offs in terms of computational resources, complexity, and performance. Here&rsquo;s a comparison of why you might choose an encoder-decoder architecture over a decoder-only one, and considerations about resource usage:</p><ol><li><p><strong>Handling Complex Input-Output Relationships</strong>:</p><ul><li><strong>Task Complexity</strong>: Encoder-decoder models excel at tasks where the input and output are significantly different in structure or length, such as machine translation or summarization. The encoder captures the complex relationships in the input sequence, and the decoder generates a well-formed output sequence.</li><li><strong>Contextual Encoding</strong>: The encoder can effectively represent the entire input sequence in a structured way, allowing the decoder to generate a sequence that reflects the input&rsquo;s context accurately.</li></ul></li><li><p><strong>Improved Performance on Sequence-to-Sequence Tasks</strong>:</p><ul><li><strong>Attention Mechanism</strong>: The encoder-decoder framework allows for sophisticated attention mechanisms that can focus on different parts of the input sequence while generating the output. This is crucial for tasks where the output needs to reference specific parts of the input.</li></ul></li><li><p><strong>Versatility</strong>:</p><ul><li><strong>Generalization</strong>: Encoder-decoder models can be adapted for a variety of tasks beyond just sequence generation, including text-to-text transformations and multi-modal tasks (e.g., generating text from images).</li></ul></li><li><p><strong>Decoupling of Representation and Generation</strong>:</p><ul><li><strong>Modularity</strong>: The separation of encoding and decoding processes allows for specialized models and training procedures. This can be advantageous when tuning models for specific tasks.</li></ul></li></ol><h3 id=resource-considerations>Resource Considerations<a class=td-heading-self-link href=#resource-considerations aria-label="Heading self-link"></a></h3><ol><li><p><strong>Computational Resources</strong>:</p><ul><li><strong>Encoder-Decoder Models</strong>: Typically require more resources compared to decoder-only models because they need to handle both encoding and decoding processes. This involves more parameters and more complex computations, particularly for tasks with long input sequences.</li><li><strong>Decoder-Only Models</strong>: Can be more resource-efficient for tasks that involve generating text based on a fixed context, as they focus solely on the generation process.</li></ul></li><li><p><strong>Training and Inference</strong>:</p><ul><li><strong>Encoder-Decoder Models</strong>: Training can be more resource-intensive due to the dual structure (encoder and decoder). Inference can also be slower because it involves both encoding the input and generating the output.</li><li><strong>Decoder-Only Models</strong>: Training might be less complex since there is only one component involved, and inference can be faster for generation tasks due to the lack of a separate encoding step.</li></ul></li></ol><h3 id=summary>Summary<a class=td-heading-self-link href=#summary aria-label="Heading self-link"></a></h3><ul><li><strong>Encoder-Decoder Models</strong>: Best suited for tasks where the input and output sequences are complex and need sophisticated handling. They provide a structured approach to managing different sequence transformations and can handle a wide range of sequence-to-sequence tasks.</li><li><strong>Decoder-Only Models</strong>: More efficient for tasks focused solely on text generation or autoregressive modeling, where the context is provided, and the focus is on generating a continuation or response.</li></ul><p>Choosing between encoder-decoder and decoder-only architectures depends on the specific task requirements and the trade-offs between performance and computational efficiency. For tasks involving intricate input-output relationships, an encoder-decoder model is often preferred despite the higher resource demands. For tasks centered on generating sequences based on a fixed context, a decoder-only model may be more resource-efficient.</p><h2 id=popular-transformer-architectures>Popular Transformer Architectures<a class=td-heading-self-link href=#popular-transformer-architectures aria-label="Heading self-link"></a></h2><p>Transformer architecture has evolved significantly since the original &ldquo;Attention Is All You Need&rdquo; paper. Here are some notable variations and advancements:</p><ol><li><p><strong>Original Transformer (Vaswani et al., 2017)</strong>:</p><ul><li><strong>Structure</strong>: Comprises an encoder-decoder architecture with self-attention and multi-head attention mechanisms.</li><li><strong>Use Case</strong>: Initially designed for machine translation.</li></ul></li><li><p><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>:</p><ul><li><strong>Structure</strong>: Uses only the encoder part of the Transformer.</li><li><strong>Training</strong>: Pre-trained using a masked language model (MLM) and next sentence prediction (NSP) objectives.</li><li><strong>Use Case</strong>: Effective for various NLP tasks like question answering, sentiment analysis, and named entity recognition.</li></ul></li><li><p><strong>GPT (Generative Pre-trained Transformer)</strong>:</p><ul><li><strong>Structure</strong>: Uses only the decoder part of the Transformer.</li><li><strong>Training</strong>: Pre-trained using a unidirectional (left-to-right) language model objective.</li><li><strong>Variants</strong>: GPT-2 and GPT-3, with GPT-4 being the latest, have scaled up the number of parameters and improved performance significantly.</li><li><strong>Use Case</strong>: Text generation, language translation, and more.</li></ul></li><li><p><strong>T5 (Text-to-Text Transfer Transformer)</strong>:</p><ul><li><strong>Structure</strong>: Converts all NLP tasks into a text-to-text format.</li><li><strong>Training</strong>: Pre-trained on a diverse mixture of tasks and fine-tuned for specific tasks.</li><li><strong>Use Case</strong>: Versatile across different NLP tasks.</li></ul></li><li><p><strong>RoBERTa (A Robustly Optimized BERT Pretraining Approach)</strong>:</p><ul><li><strong>Structure</strong>: An optimized version of BERT with more training data and longer training times.</li><li><strong>Use Case</strong>: Improved performance on various NLP benchmarks compared to BERT.</li></ul></li><li><p><strong>ALBERT (A Lite BERT)</strong>:</p><ul><li><strong>Structure</strong>: Reduces the number of parameters using factorized embedding parameterization and cross-layer parameter sharing.</li><li><strong>Use Case</strong>: Efficient and lightweight version of BERT for various NLP tasks.</li></ul></li><li><p><strong>DistilBERT</strong>:</p><ul><li><strong>Structure</strong>: A smaller, faster, and cheaper version of BERT.</li><li><strong>Training</strong>: Uses knowledge distillation during pre-training.</li><li><strong>Use Case</strong>: Suitable for environments with limited computational resources.</li></ul></li><li><p><strong>XLNet</strong>:</p><ul><li><strong>Structure</strong>: Integrates autoregressive and autoencoding approaches.</li><li><strong>Training</strong>: Uses permutation-based training to capture bidirectional context.</li><li><strong>Use Case</strong>: Effective for language modeling and various downstream NLP tasks.</li></ul></li><li><p><strong>Transformer-XL</strong>:</p><ul><li><strong>Structure</strong>: Introduces a segment-level recurrence mechanism.</li><li><strong>Training</strong>: Handles long-term dependencies better than traditional Transformers.</li><li><strong>Use Case</strong>: Suitable for tasks requiring long context understanding, like document modeling.</li></ul></li><li><p><strong>Vision Transformer (ViT)</strong>:</p><ul><li><strong>Structure</strong>: Applies Transformer architecture to image classification tasks.</li><li><strong>Training</strong>: Treats image patches as tokens and processes them similarly to text.</li><li><strong>Use Case</strong>: Effective for various computer vision tasks.</li></ul></li><li><p><strong>DeBERTa (Decoding-enhanced BERT with disentangled attention)</strong>:</p><ul><li><strong>Structure</strong>: Enhances BERT with disentangled attention and improved position embeddings.</li><li><strong>Use Case</strong>: Achieves state-of-the-art results on various NLP benchmarks.</li></ul></li><li><p><strong>Swin Transformer</strong>:</p><ul><li><strong>Structure</strong>: Applies hierarchical vision Transformer architecture with shifted windows.</li><li><strong>Training</strong>: Designed for image classification and dense prediction tasks.</li><li><strong>Use Case</strong>: Effective for object detection, semantic segmentation, and more.</li></ul></li></ol><p>These variations demonstrate the versatility and adaptability of Transformer architectures across a wide range of applications in both natural language processing and computer vision.</p><h2 id=why-multi-headed-attention>Why Multi headed attention?<a class=td-heading-self-link href=#why-multi-headed-attention aria-label="Heading self-link"></a></h2><p>The multi-head attention mechanism in Transformers enables the model to focus on different aspects of the input data simultaneously. Here are examples of various aspects that attention mechanisms can capture:</p><h3 id=syntactic-relationships>Syntactic Relationships:<a class=td-heading-self-link href=#syntactic-relationships aria-label="Heading self-link"></a></h3><p>Example: In the sentence &ldquo;The cat sat on the mat,&rdquo; different heads might focus on different parts of speech relationships, such as subject-verb (&ldquo;cat&rdquo; and &ldquo;sat&rdquo;) and preposition-object (&ldquo;on&rdquo; and &ldquo;mat&rdquo;).</p><h3 id=semantic-relationships>Semantic Relationships:<a class=td-heading-self-link href=#semantic-relationships aria-label="Heading self-link"></a></h3><p>Example: In the sentence &ldquo;He played the piano beautifully,&rdquo; one head might focus on the verb &ldquo;played&rdquo; and its direct object &ldquo;piano,&rdquo; while another head focuses on the adverb &ldquo;beautifully&rdquo; modifying &ldquo;played.&rdquo;</p><h3 id=coreference-resolution>Coreference Resolution:<a class=td-heading-self-link href=#coreference-resolution aria-label="Heading self-link"></a></h3><p>Example: In the text &ldquo;Alice went to the market. She bought apples,&rdquo; one head might track the coreference between &ldquo;Alice&rdquo; and &ldquo;She.&rdquo;</p><h3 id=long-range-dependencies>Long-Range Dependencies:<a class=td-heading-self-link href=#long-range-dependencies aria-label="Heading self-link"></a></h3><p>Example: In the sentence &ldquo;The book that you recommended to me last week was fascinating,&rdquo; one head might focus on the relationship between &ldquo;book&rdquo; and &ldquo;fascinating,&rdquo; which are far apart in the sentence.</p><h3 id=positional-information>Positional Information:<a class=td-heading-self-link href=#positional-information aria-label="Heading self-link"></a></h3><p>Example: Different heads can capture relative positional information, such as the beginning, middle, and end of a sentence, which is crucial for understanding the structure.</p><h3 id=named-entity-recognition>Named Entity Recognition:<a class=td-heading-self-link href=#named-entity-recognition aria-label="Heading self-link"></a></h3><p>Example: In the sentence &ldquo;Barack Obama was born in Hawaii,&rdquo; one head might focus on identifying &ldquo;Barack Obama&rdquo; as a person.</p><h3 id=polarity-and-sentiment>Polarity and Sentiment:<a class=td-heading-self-link href=#polarity-and-sentiment aria-label="Heading self-link"></a></h3><ul><li>Could capture positive or negative sentiment associated with different parts of the text.</li><li>May focus on identifying subjective vs. objective statements. Ho</li></ul><h2 id=how-attention-works>How attention works?<a class=td-heading-self-link href=#how-attention-works aria-label="Heading self-link"></a></h2><p>The input embedding is linearly projected into three different spaces to generate queries (𝑄), keys (𝐾), and values (𝑉).</p><p>$${Attention}(Q, K, V) = {softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$</p><p>Where:</p><ul><li>Q is the query matrix.</li><li>K is the key matrix.</li><li>V is the value matrix.</li><li>$d_k$ is the dimension of the keys.</li></ul><h2 id=how-multihead-attention-works>How multihead attention works.<a class=td-heading-self-link href=#how-multihead-attention-works aria-label="Heading self-link"></a></h2><ul><li>Base model has 512 dim embedding vector, large model has 1024 dim embedding vector.</li><li>Position vector of the same size is used.</li><li>Both vectors are pair wise added.</li><li>Base model has 8 heads and large model has 16 heads. Thus each head has 512/8 or 1024/16 i.e. 64 dim vector.</li></ul><h2 id=how-q-k-v-calculated>How Q, K, V Calculated?<a class=td-heading-self-link href=#how-q-k-v-calculated aria-label="Heading self-link"></a></h2><ol><li><p><strong>Input Embedding Dimension for Base Model</strong>: 512</p></li><li><p><strong>Number of Heads</strong>: 8</p></li><li><p><strong>Dimension per Head</strong>: Each head will handle $$\frac{512}{8} = 64$$ dimensions.</p></li></ol><h3 id=steps-for-multi-head-attention>Steps for Multi-Head Attention:<a class=td-heading-self-link href=#steps-for-multi-head-attention aria-label="Heading self-link"></a></h3><ol><li><p><strong>Linear Projections</strong>:</p><ul><li>The input embedding (of dimension 512) is linearly projected into three different spaces to generate queries ($$Q$$), keys ($K$), and values ($V$).</li><li>Each projection is typically done using separate learned weight matrices.</li><li>These projections result in three vectors: $Q$, $K$, and $V$, each of dimension 512.</li><li>Example: <strong>Linear Projections</strong>:<ul><li>For the input $X$ of shape $$[ batch_size, sequence_length, 512 ]$:</li><li>$Q = XW_Q$, where $W_Q$ is a weight matrix of shape $[512, 512]$.</li><li>$K = XW_K$, where $W_K$ is a weight matrix of shape $[512, 512]$.</li><li>$V = XW_V$, where $W_V$ is a weight matrix of shape $[512, 512]$.</li></ul></li></ul></li><li><p><strong>Splitting into Heads</strong>:</p><ul><li>After the projection, the $$Q$$, $$K$$, and $$V$$ vectors are split into 8 parts (heads).</li><li>Each part will have $$ \frac{512}{8} = 64 $$ dimensions.</li><li>This means each head gets a 64-dimensional sub-vector from $$Q$$, $$K$$, and $$V$$.</li><li>Example: <strong>Splitting into Heads</strong>:<ul><li>Each of the $$Q$$, $$K$$, and $$V$$ matrices (of shape $$ {batch_size}, {sequence_length}, 512 $$ is reshaped and split into 8 heads.</li><li>For each matrix, this reshaping results in shape $$ batch_size, sequence_length, 8, 64 $$.</li><li>The 8 heads mean we now have 8 sets of $$Q$$, $$K$$, and $$V$$ vectors, each of dimension 64.</li></ul></li></ul></li></ol><h3 id=scaled-dot-product-attention-per-head>Scaled Dot-Product Attention (Per Head):<a class=td-heading-self-link href=#scaled-dot-product-attention-per-head aria-label="Heading self-link"></a></h3><p>Each of the 8 heads performs scaled dot-product attention independently:
$$ {Attention}(Q_i, K_i, V_i) = {softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right) V_i $$
where $$ d_k = 64 $$ is the dimension of each head.</p><h3 id=concatenation-and-final-linear-layer>Concatenation and Final Linear Layer:<a class=td-heading-self-link href=#concatenation-and-final-linear-layer aria-label="Heading self-link"></a></h3><ol><li><p>The outputs from all 8 heads are concatenated:</p><ul><li>Concatenated output shape: $$batch_size, sequence_length, 8 \times 64 = batch_size, sequence_length, 512$$ .</li></ul></li><li><p>This concatenated vector is then passed through a final linear layer (with weight matrix of shape $$[512, 512]$$) to produce the final output of the multi-head attention mechanism.</p></li></ol><p>This process ensures that each head independently attends to different parts of the input, capturing diverse aspects of the data.</p><h2 id=floating-point-value-range>Floating Point Value Range<a class=td-heading-self-link href=#floating-point-value-range aria-label="Heading self-link"></a></h2><p>These embedding vectors holds floating point numbers. These numbers may be 64bit, 32bit, 16bit, 8bit, 4bit. What will be the range of value if we use these different bit size to hold floating number. This is useful when you are doing quantization.</p><p>Embedding vectors in many deep learning frameworks typically use IEEE 754 double-precision floating-point format (64-bit) for representing values, but there are also other precision formats used depending on the specific requirements and hardware capabilities.</p><p>Apart from number of parameters these floating point precision also make model bulky. To reduce the model size we reduce these floating point precision. This process is called quantization. For edge devices or making model run on low graded machine we can choose quantization of the model to 4bit or 8bit floating points, off course we suffer the quality of output due to this.</p><h3 id=common-precision-formats-for-embeddings>Common Precision Formats for Embeddings<a class=td-heading-self-link href=#common-precision-formats-for-embeddings aria-label="Heading self-link"></a></h3><ol><li><p><strong>Double Precision (64-bit)</strong>:</p><ul><li><strong>Format</strong>: IEEE 754 double-precision floating-point.</li><li><strong>Precision</strong>: Provides about 15-17 significant decimal digits of precision.</li><li><strong>Range</strong>: Can represent values from approximately $$ \pm 5 \times 10^{-324} $$ to $$ \pm 1.79 \times 10^{308} $$.</li><li><strong>Use Case</strong>: Often used when high precision is required, but it&rsquo;s less common in practice for embeddings due to the increased computational and memory overhead.</li></ul></li><li><p><strong>Single Precision (32-bit)</strong>:</p><ul><li><strong>Format</strong>: IEEE 754 single-precision floating-point.</li><li><strong>Precision</strong>: Provides about 6-9 significant decimal digits of precision.</li><li><strong>Range</strong>: Can represent values from approximately $$ \pm 1.18 \times 10^{-38} $$ to $$ \pm 3.4 \times 10^{38} $$.</li><li><strong>Use Case</strong>: More common for embeddings due to a good balance between precision and computational efficiency.</li></ul></li><li><p><strong>Half Precision (16-bit)</strong>:</p><ul><li><strong>Format</strong>: IEEE 754 half-precision floating-point.</li><li><strong>Precision</strong>: Provides about 3-4 significant decimal digits of precision.</li><li><strong>Range</strong>: Can represent values from approximately $$ \pm 6.1 \times 10^{-5} $$ to $$ \pm 6.5 \times 10^{4} $$.</li><li><strong>Use Case</strong>: Used to reduce memory usage and increase computational efficiency, especially during training with GPUs that support mixed precision.</li></ul></li><li><p><strong>BFloat16 (16-bit)</strong>:</p><ul><li><strong>Format</strong>: A variant of 16-bit floating-point with a different exponent and mantissa configuration, designed to be more efficient for certain computations.</li><li><strong>Precision</strong>: Similar to half precision but with a larger exponent range.</li><li><strong>Use Case</strong>: Used in some TensorFlow models and other frameworks to optimize performance while maintaining acceptable precision.</li></ul></li></ol><h3 id=in-summary>In Summary<a class=td-heading-self-link href=#in-summary aria-label="Heading self-link"></a></h3><ul><li><strong>IEEE 754 Double Precision</strong> is used when high precision is crucial, but it is less common for embeddings due to the larger memory footprint and computation requirements.</li><li><strong>IEEE 754 Single Precision</strong> is the most common format for embeddings in many deep learning applications due to its efficiency and sufficient precision.</li><li><strong>Half Precision</strong> and <strong>BFloat16</strong> are used for further optimization, particularly in training scenarios where memory and computational efficiency are critical.</li></ul><p>The choice of precision format depends on the trade-offs between precision, memory usage, and computational efficiency.</p><h2 id=illustration-of-working-of-transformer>Illustration of Working of Transformer<a class=td-heading-self-link href=#illustration-of-working-of-transformer aria-label="Heading self-link"></a></h2><p>For the sake of simplicity, let&rsquo;s walk through an example of how a ChatGPT-like architecture generates text, using an 8-dimensional word embedding, a vocabulary size of 100, and a multi-headed attention mechanism with 2 heads and 3 layers. We will also perform the computations for the self-attention using the Query (Q), Key (K), and Value (V) matrices.</p><h3 id=step-1-tokenization-and-embedding>Step 1: Tokenization and Embedding<a class=td-heading-self-link href=#step-1-tokenization-and-embedding aria-label="Heading self-link"></a></h3><p><strong>Vocabulary</strong>: 100 unique tokens.</p><p><strong>Input Sentence</strong>: &ldquo;Hello world&rdquo;</p><p><strong>Token IDs</strong>:</p><ul><li>&ldquo;Hello&rdquo; might be token ID 42.</li><li>&ldquo;world&rdquo; might be token ID 85.</li></ul><p><strong>Embedding</strong>: Each token ID is mapped to an 8-dimensional vector.</p><ul><li>Embedding for &ldquo;Hello&rdquo; (token ID 42): [0.1, -0.2, 0.3, 0.4, -0.5, 0.2, -0.1, 0.0]</li><li>Embedding for &ldquo;world&rdquo; (token ID 85): [-0.3, 0.1, 0.2, -0.4, 0.5, -0.2, 0.3, -0.1]</li></ul><p>We also need to compute position embedding. Refer <a href=https://localhost:1313/dsblog/transformers-demystified-a-step-by-step-guide/#position-embedding-mechanism>Position Embedding Mechanism</a></p><h3 id=step-2-input-to-the-transformer-model>Step 2: Input to the Transformer Model<a class=td-heading-self-link href=#step-2-input-to-the-transformer-model aria-label="Heading self-link"></a></h3><p>These embeddings are passed as input to the model.</p><p><strong>Input Matrix</strong>:</p><p>$$
\begin{bmatrix}
0.1 & -0.2 & 0.3 & 0.4 & -0.5 & 0.2 & -0.1 & 0.0 \
-0.3 & 0.1 & 0.2 & -0.4 & 0.5 & -0.2 & 0.3 & -0.1 \
\end{bmatrix}
$$</p><h3 id=step-3-self-attention>Step 3: <a href=https://localhost:1313/dsblog/transformers-demystified-a-step-by-step-guide/#self-attention-mechanism>Self Attention</a><a class=td-heading-self-link href=#step-3-self-attention aria-label="Heading self-link"></a></h3><ul><li><strong>Multi-Head Self-Attention</strong></li><li><strong>Combining Multi-Head Attention</strong></li></ul><p>For Head 2, similar computations will be performed to obtain $$ Q_2 $$, $$ K_2 $$, $$ V_2 $$, and the attention output. The outputs from both heads will be concatenated and then projected back into the original embedding dimension using a weight matrix $$ W^O $$.</p><h3 id=step-4-processing-through-transformer-layers>Step 4. <strong>Processing Through Transformer Layers</strong><a class=td-heading-self-link href=#step-4-processing-through-transformer-layers aria-label="Heading self-link"></a></h3><p>The output of the multi-head attention layer will then pass through the feed-forward neural network (FFNN) and normalization layers, completing the processing for one layer of the transformer. This process is repeated for the remaining layers, in our example 3 layers.</p><ul><li><strong>Feedforward Neural Networks</strong></li><li><strong>Layer Normalization</strong></li><li><strong>Residual Connections</strong></li></ul><p>Each layer refines the embeddings based on the input context.</p><h3 id=step-5-output-logits>Step 5. <strong>Output Logits</strong><a class=td-heading-self-link href=#step-5-output-logits aria-label="Heading self-link"></a></h3><p>After processing through the Transformer layers, we get output vectors (logits) for each token position.</p><p><strong>Output Logits for the Next Token</strong>:
Let&rsquo;s assume our logits for the next token are a 100-dimensional vector (one value per token in the vocabulary). For simplicity:</p><p>$$
\begin{bmatrix}
1.5, & -0.3, & \dots, & 0.8 \
\end{bmatrix}
$$</p><h3 id=step-6-softmax-function>Step 6. <strong>Softmax Function</strong><a class=td-heading-self-link href=#step-6-softmax-function aria-label="Heading self-link"></a></h3><p>The logits are converted to probabilities using the softmax function.</p><p><strong>Softmax Output</strong>:</p><p>$$
\begin{bmatrix}
0.1, & 0.05, & \dots, & 0.15 \
\end{bmatrix}
$$</p><h3 id=step-7-token-selection>Step 7. <strong>Token Selection</strong><a class=td-heading-self-link href=#step-7-token-selection aria-label="Heading self-link"></a></h3><p>A token is selected based on the probabilities. Using sampling, top-k, or greedy decoding:</p><ul><li>Let&rsquo;s say token ID 75 is selected, corresponding to the token &ldquo;everyone&rdquo;.</li></ul><h3 id=step-8-generating-the-next-token>Step 8. <strong>Generating the Next Token</strong><a class=td-heading-self-link href=#step-8-generating-the-next-token aria-label="Heading self-link"></a></h3><p>The process repeats. The input now includes the previous tokens &ldquo;Hello world&rdquo; and the new token &ldquo;everyone&rdquo;. The model generates the next token based on this updated context.</p><h3 id=generating-text>Generating Text<a class=td-heading-self-link href=#generating-text aria-label="Heading self-link"></a></h3><p>To generate text, the model uses these layers iteratively, predicting the next token in the sequence based on the previously generated tokens and the context provided by the input sequence. The output is passed through a final linear layer and softmax to produce probabilities for the next token, from which the next token is sampled or chosen.</p><p>By repeating this process, the model generates text token by token until a specified end condition is met.</p><h2 id=what-different-parameters-are-learned-during-transformer-training>What different parameters are learned during transformer training?<a class=td-heading-self-link href=#what-different-parameters-are-learned-during-transformer-training aria-label="Heading self-link"></a></h2><p>Transformer models like GPT3, GPT3.5, GPT4.0, Gemma, PaLM, Llama etc has billions of parameters. What are these parameters which are learned during training?</p><p>In large language models like ChatGPT, weights and biases are integral to the model&rsquo;s operation, especially within the transformer architecture. Here’s a detailed breakdown of different weights and biases used in such models, including those related to attention mechanisms:</p><h3 id=1-weights-and-biases-in-transformer-architecture>1. <strong>Weights and Biases in Transformer Architecture</strong><a class=td-heading-self-link href=#1-weights-and-biases-in-transformer-architecture aria-label="Heading self-link"></a></h3><h4 id=attention-mechanism-weights><strong>Attention Mechanism Weights</strong><a class=td-heading-self-link href=#attention-mechanism-weights aria-label="Heading self-link"></a></h4><ul><li><p><strong>Query Weights (W_q)</strong>: These weights transform the input embeddings or hidden states into query vectors. In the attention mechanism, the query vector is compared against keys to compute attention scores.</p></li><li><p><strong>Key Weights (W_k)</strong>: These weights transform the input embeddings or hidden states into key vectors. The attention scores are computed by comparing these keys with the query vectors.</p></li><li><p><strong>Value Weights (W_v)</strong>: These weights transform the input embeddings or hidden states into value vectors. The output of the attention mechanism is a weighted sum of these value vectors, based on the attention scores.</p></li><li><p><strong>Output Weights (W_o)</strong>: After applying the attention mechanism, the output vectors are transformed by these weights before being passed to subsequent layers.</p></li></ul><h4 id=feed-forward-network-weights><strong>Feed-Forward Network Weights</strong><a class=td-heading-self-link href=#feed-forward-network-weights aria-label="Heading self-link"></a></h4><ul><li><p><strong>Weights (W_ff)</strong>: The feed-forward network within each transformer block has its own set of weights for transforming the hidden states. This usually includes two sets of weights:</p><ul><li><strong>Weight Matrices for Linear Transformations</strong>: These weights perform linear transformations in the feed-forward network, often involving two layers with an activation function (e.g., ReLU) in between.</li></ul></li><li><p><strong>Biases (b_ff)</strong>: Biases are used along with the weights in the feed-forward network to adjust the activation values.</p></li></ul><h4 id=layer-normalization-weights><strong>Layer Normalization Weights</strong><a class=td-heading-self-link href=#layer-normalization-weights aria-label="Heading self-link"></a></h4><ul><li><strong>Gamma (γ)</strong>: Scaling parameter used in layer normalization to adjust the normalized output.</li><li><strong>Beta (β)</strong>: Shifting parameter used in layer normalization to adjust the mean of the normalized output.</li></ul><h3 id=2-overall-model-weights-and-biases>2. <strong>Overall Model Weights and Biases</strong><a class=td-heading-self-link href=#2-overall-model-weights-and-biases aria-label="Heading self-link"></a></h3><h4 id=embedding-weights><strong>Embedding Weights</strong><a class=td-heading-self-link href=#embedding-weights aria-label="Heading self-link"></a></h4><ul><li><p><strong>Token Embedding Weights</strong>: These weights map input tokens (words or subwords) to continuous vector representations (embeddings).</p></li><li><p><strong>Position Embedding Weights</strong>: These weights add positional information to the embeddings to encode the order of tokens in the sequence.</p></li></ul><h4 id=layer-weights><strong>Layer Weights</strong><a class=td-heading-self-link href=#layer-weights aria-label="Heading self-link"></a></h4><ul><li><p><strong>Weights in Each Transformer Layer</strong>: Each layer of the transformer model has its own set of weights and biases for both the attention mechanism and the feed-forward network.</p></li><li><p><strong>Residual Connection Weights</strong>: Residual connections (or skip connections) within each transformer layer often involve weights for combining the input and output of the layer.</p></li></ul><h3 id=summary-1>Summary<a class=td-heading-self-link href=#summary-1 aria-label="Heading self-link"></a></h3><p>In summary, the different weights and biases in a model like ChatGPT are:</p><ul><li><p><strong>Attention Mechanism</strong>:</p><ul><li>Query Weights (W_q)</li><li>Key Weights (W_k)</li><li>Value Weights (W_v)</li><li>Output Weights (W_o)</li></ul></li><li><p><strong>Feed-Forward Network</strong>:</p><ul><li>Weights (W_ff)</li><li>Biases (b_ff)</li></ul></li><li><p><strong>Layer Normalization</strong>:</p><ul><li>Gamma (γ)</li><li>Beta (β)</li></ul></li><li><p><strong>Embedding Weights</strong>:</p><ul><li>Token Embedding Weights</li><li>Position Embedding Weights</li></ul></li></ul><p>These weights and biases are learned during the training phase and are used during inference to generate responses based on the input data. Each component of the model contributes to its ability to understand and generate human-like text.</p><h2 id=self-attention-mechanism>Self Attention Mechanism<a class=td-heading-self-link href=#self-attention-mechanism aria-label="Heading self-link"></a></h2><h3 id=compute-q-k-v-matrices-for-multi-head-attention-2-heads>Compute Q, K, V Matrices for Multi-Head Attention (2 heads)<a class=td-heading-self-link href=#compute-q-k-v-matrices-for-multi-head-attention-2-heads aria-label="Heading self-link"></a></h3><p>We&rsquo;ll compute Q, K, and V matrices for each head.</p><p><strong>Weight Matrices for Q, K, V for Head 1</strong>:</p><p>$$
W^Q_1, W^K_1, W^V_1 \in \mathbb{R}^{8 \times 4}$$</p><p><strong>Weight Matrices for Q, K, V for Head 2</strong>:</p><p>$$W^Q_2, W^K_2, W^V_2 \in \mathbb{R}^{8 \times 4}
$$</p><p>For simplicity, let&rsquo;s use random matrices. In practice, these are learned during training.</p><p><strong>Embeddings</strong>:</p><p>$$
X = \begin{bmatrix}
0.1 & -0.2 & 0.3 & 0.4 & -0.5 & 0.2 & -0.1 & 0.0 \
-0.3 & 0.1 & 0.2 & -0.4 & 0.5 & -0.2 & 0.3 & -0.1 \
\end{bmatrix}
$$</p><p><strong>Weight Matrices</strong> (randomly initialized for this example):</p><p>Head 1:</p><p>$$
W^Q_1 = \begin{bmatrix}
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
\end{bmatrix}</p><p>W^K_1 = \begin{bmatrix}
0.2 & 0.1 & 0.4 & 0.3 \
0.2 & 0.1 & 0.4 & 0.3 \
0.2 & 0.1 & 0.4 & 0.3 \
0.2 & 0.1 & 0.4 & 0.3 \
0.2 & 0.1 & 0.4 & 0.3 \
0.2 & 0.1 & 0.4 & 0.3 \
0.2 & 0.1 & 0.4 & 0.3 \
0.2 & 0.1 & 0.4 & 0.3 \
\end{bmatrix}</p><p>W^V_1 = \begin{bmatrix}
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
\end{bmatrix}
$$</p><p>Head 2:</p><p>$$
W^Q_2 = \begin{bmatrix}
0.4 & 0.3 & 0.2 & 0.1 \
0.4 & 0.3 & 0.2 & 0.1 \
0.4 & 0.3 & 0.2 & 0.1 \
0.4 & 0.3 & 0.2 & 0.1 \
0.4 & 0.3 & 0.2 & 0.1 \
0.4 & 0.3 & 0.2 & 0.1 \
0.4 & 0.3 & 0.2 & 0.1 \
0.4 & 0.3 & 0.2 & 0.1 \
\end{bmatrix}</p><p>W^K_2 = \begin{bmatrix}
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
\end{bmatrix}</p><p>W^V_2 = \begin{bmatrix}
0.2 & 0.1 & 0.3 & 0.4 \
0.2 & 0.1 & 0.3 & 0.4 \
0.2 & 0.1 & 0.3 & 0.4 \
0.2 & 0.1 & 0.3 & 0.4 \
0.2 & 0.1 & 0.3 & 0.4 \
0.2 & 0.1 & 0.3 & 0.4 \
0.2 & 0.1 & 0.3 & 0.4 \
0.2 & 0.1 & 0.3 & 0.4 \
\end{bmatrix}
$$</p><p><strong>Compute Q, K, V for each token for each head</strong>:</p><p><strong>Head 1</strong>:</p><p>$$
Q_1 = X \cdot W^Q_1 = \begin{bmatrix}
0.1 & -0.2 & 0.3 & 0.4 & -0.5 & 0.2 & -0.1 & 0.0 \
-0.3 & 0.1 & 0.2 & -0.4 & 0.5 & -0.2 & 0.3 & -0.1 \
\end{bmatrix} \cdot W^Q_1</p><p>K_1 = X \cdot W^K_1</p><p>V_1 = X \cdot W^V_1
$$</p><p><strong>Head 2</strong>:</p><p>$$
Q_2 = X \cdot W^Q_2</p><p>K_2 = X \cdot W^K_2</p><p>V_2 = X \cdot W^V_2
$$</p><p>Let&rsquo;s compute these step by step.</p><h3 id=compute-q-k-v-for-head-1>Compute Q, K, V for Head 1<a class=td-heading-self-link href=#compute-q-k-v-for-head-1 aria-label="Heading self-link"></a></h3><p><strong>Input Embeddings</strong>:</p><p>$$
X = \begin{bmatrix}
0.1 & -0.2 & 0.3 & 0.4 & -0.5 & 0.2 & -0.1 & 0.0 \
-0.3 & 0.1 & 0.2 & -0.4 & 0.5 & -0.2 & 0.3 & -0.1 \
\end{bmatrix}
$$</p><p><strong>Weight Matrices for Head 1</strong>:</p><p>$$
W^Q_1 = \begin{bmatrix}
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
\end{bmatrix}</p><p>W^K_1 = \begin{bmatrix}
0.2 & 0.1 & 0.4 & 0.3 \
0.2 & 0.1 & 0.4 & 0.3 \
0.2 & 0.1 & 0.4 & 0.3 \
0.2 & 0.1 & 0.4 & 0.3 \
0.2 & 0.1 & 0.4 & 0.3 \
0.2 & 0.1 & 0.4 & 0.3 \
0.2 & 0.1 & 0.4 & 0.3 \
0.2 & 0.1 & 0.4 & 0.3 \
\end{bmatrix}</p><p>W^V_1 = \begin{bmatrix}
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
0.3 & 0.4 & 0.1 & 0.2 \
\end{bmatrix}
$$</p><h4 id=compute-q-k-v-for-each-token-for-head-1>Compute Q, K, V for each token for Head 1:<a class=td-heading-self-link href=#compute-q-k-v-for-each-token-for-head-1 aria-label="Heading self-link"></a></h4><p>$$
Q_1 = X \cdot W^Q_1 = \begin{bmatrix}
0.1 & -0.2 & 0.3 & 0.4 & -0.5 & 0.2 & -0.1 & 0.0 \
-0.3 & 0.1 & 0.2 & -0.4 & 0.5 & -0.2 & 0.3 & -0.1 \
\end{bmatrix} \cdot \begin{bmatrix}
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
0.1 & 0.2 & 0.3 & 0.4 \
\end{bmatrix}
$$</p><p>Performing the matrix multiplication:</p><p>$$
Q_1 = \begin{bmatrix}
(0.1 \times 0.1) + (-0.2 \times 0.1) + (0.3 \times 0.1) + (0.4 \times 0.1) + (-0.5 \times 0.1) + (0.2 \times 0.1) + (-0.1 \times 0.1) + (0 \times 0.1) & \dots & \
(-0.3 \times 0.1) + (0.1 \times 0.1) + (0.2 \times 0.1) + (-0.4 \times 0.1) + (0.5 \times 0.1) + (-0.2 \times 0.1) + (0.3 \times 0.1) + (-0.1 \times 0.1) & \dots & \
\end{bmatrix}
$$</p><p>Simplifying:</p><p>$$
Q_1 = \begin{bmatrix}
-0.01 & -0.02 & -0.03 & -0.04 \
0.02 & 0.04 & 0.06 & 0.08 \
\end{bmatrix}
$$</p><p>Following the same steps for $$ K_1 $$ and $$ V_1 $$:</p><p>$$
K_1 = X \cdot W^K_1</p><p>= \begin{bmatrix}
0.15 & 0.30 & 0.45 & 0.60 \
0.10 & 0.20 & 0.30 & 0.40 \
\end{bmatrix}</p><p>V_1 = X \cdot W^V_1</p><p>= \begin{bmatrix}
0.28 & 0.56 & 0.84 & 1.12 \
0.24 & 0.48 & 0.72 & 0.96 \
\end{bmatrix}
$$</p><p><strong>Similarly you compute for head 2. Finally you concatenate both vectors and get 2x8 size matrix (same size which was input for the self attention)</strong></p><h3 id=step-3-self-attention-calculation>Step 3: Self-Attention Calculation<a class=td-heading-self-link href=#step-3-self-attention-calculation aria-label="Heading self-link"></a></h3><h4 id=compute-attention-scores>Compute attention scores:<a class=td-heading-self-link href=#compute-attention-scores aria-label="Heading self-link"></a></h4><p>$$
\text{Scores} = Q_1 \cdot K_1^T = \begin{bmatrix}
-0.01 & -0.02 & -0.03 & -0.04 \
0.02 & 0.04 & 0.06 & 0.08 \
\end{bmatrix} \cdot \begin{bmatrix}
0.15 & 0.10 \
0.30 & 0.20 \
0.45 & 0.30 \
0.60 & 0.40 \
\end{bmatrix}
$$</p><p>Performing the matrix multiplication:</p><p>$$
\text{Scores} = \begin{bmatrix}
-0.01 \times 0.15 + -0.02 \times 0.30 + -0.03 \times 0.45 + -0.04 \times 0.60 & -0.01 \times 0.10 + -0.02 \times 0.20 + -0.03 \times 0.30 + -0.04 \times 0.40 \
0.02 \times 0.15 + 0.04 \times 0.30 + 0.06 \times 0.45 + 0.08 \times 0.60 & 0.02 \times 0.10 + 0.04 \times 0.20 + 0.06 \times 0.30 + 0.08 \times 0.40 \
\end{bmatrix}
$$</p><p>Simplifying:</p><p>$$
\text{Scores} = \begin{bmatrix}
-0.015 - 0.006 - 0.0135 - 0.024 & -0.01 - 0.004 - 0.009 - 0.016 \
0.03 + 0.012 + 0.027 + 0.048 & 0.02 + 0.008 + 0.018 + 0.032 \
\end{bmatrix}</p><p>\text{Scores} = \begin{bmatrix}
-0.0585 & -0.039 \
0.117 & 0.078 \
\end{bmatrix}
$$</p><h4 id=apply-softmax-to-obtain-attention-weights>Apply softmax to obtain attention weights:<a class=td-heading-self-link href=#apply-softmax-to-obtain-attention-weights aria-label="Heading self-link"></a></h4><p>$$
\text{Attention Weights} = \text{softmax}(\text{Scores})</p><p>\text{Attention Weights} = \begin{bmatrix}
\frac{e^{-0.0585}}{e^{-0.0585} + e^{-0.039}} & \frac{e^{-0.039}}{e^{-0.0585} + e^{-0.039}} \
\frac{e^{0.117}}{e^{0.117} + e^{0.078}} & \frac{e^{0.078}}{e^{0.117} + e^{0.078}} \
\end{bmatrix}
$$</p><p>Simplifying:</p><p>$$
\text{Attention Weights} = \begin{bmatrix}
\frac{1}{1 + e^{0.0195}} & \frac{e^{0.0195}}{1 + e^{0.0195}} \
\frac{1}{1 + e^{-0.039}} & \frac{e^{-0.039}}{1 + e^{-0.039}} \
\end{bmatrix}
$$</p><p>Approximating the values:</p><p>$$
\text{Attention Weights} = \begin{bmatrix}
0.495 & 0.505 \
0.510 & 0.490 \
\end{bmatrix}
$$</p><h4 id=compute-the-weighted-sum-of-the-values>Compute the weighted sum of the values:<a class=td-heading-self-link href=#compute-the-weighted-sum-of-the-values aria-label="Heading self-link"></a></h4><p>$$
\text{Output} = \text{Attention Weights} \cdot V_1 = \begin{bmatrix}
0.495 & 0.505 \
0.510 & 0.490 \
\end{bmatrix} \cdot \begin{bmatrix}
0.28 & 0.56 & 0.84 & 1.12 \
0.24 & 0.48 & 0.72 & 0.96 \
\end{bmatrix}
$$</p><p>Performing the matrix multiplication:</p><p>$$
\text{Output} = \begin{bmatrix}
0.495 \times 0.28 + 0.505 \times 0.24 & 0.495 \times 0.56 + 0.505 \times 0.48 & 0.495 \times 0.84 + 0.505 \times 0.72 & 0.495 \times 1.12 + 0.505 \times 0.96 \
0.510 \times 0.28 + 0.490 \times 0.24 & 0.510 \times 0.56 + 0.490 \times 0.48 & 0.510 \times 0.84 + 0.490 \times 0.72 & 0.510 \times 1.12 + 0.490 \times 0.96 \
\end{bmatrix}
$$</p><p>Simplifying:</p><p>$$
\text{Output} = \begin{bmatrix}
0.1386 + 0.1212 & 0.2772 + 0.2424 & 0.4158 + 0.3636 & 0.5544 + 0.4848 \
0.1428 + 0.1176 & 0.2856 + 0.2304 & 0.4284 + 0.3432 & 0.5712 + 0.456 \
\end{bmatrix}</p><p>\text{Output} = \begin{bmatrix}
0.2598 & 0.5196 & 0.7794 & 1.0392 \
0.2604 & 0.516 & 0.7716 & 1.0272 \
\end{bmatrix}
$$</p><h2 id=position-embedding-mechanism>Position Embedding Mechanism<a class=td-heading-self-link href=#position-embedding-mechanism aria-label="Heading self-link"></a></h2><p>Position embeddings are used in transformers to provide information about the order of tokens in a sequence. The most common method for computing position embeddings is using sine and cosine functions of different frequencies. This method was introduced in the original transformer paper &ldquo;Attention Is All You Need&rdquo;. The formulas for the position embeddings are as follows:</p><h3 id=formulas-for-position-embedding>Formulas for Position Embedding<a class=td-heading-self-link href=#formulas-for-position-embedding aria-label="Heading self-link"></a></h3><p>For a given position $$ pos $$ and embedding dimension $$ i $$:</p><ol><li><strong>Sine Function for Even Indices:</strong></li></ol><p>$$
PE(pos, 2i) = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
$$</p><ol start=2><li><strong>Cosine Function for Odd Indices:</strong></li></ol><p>$$ PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right) $$</p><p>Where:</p><ul><li>pos is the position of the token in the sequence (starting from 0).</li><li>i is the dimension index (also starting from 0).</li><li>$$d_{\text{model}}$$ is the dimensionality of the embeddings.</li></ul><h3 id=explanation>Explanation<a class=td-heading-self-link href=#explanation aria-label="Heading self-link"></a></h3><ul><li><strong>Even Index:</strong> For even values of i, the position embedding is computed using the sine function.</li><li><strong>Odd Index:</strong> For odd values of i, the position embedding is computed using the cosine function.</li><li><strong>Frequency:</strong> The denominator $$10000^{\frac{2i}{d_{\text{model}}}}$$ ensures that different dimensions have different frequencies. The values for sine and cosine vary more slowly for larger dimensions, capturing different levels of granularity.</li></ul><h3 id=example>Example<a class=td-heading-self-link href=#example aria-label="Heading self-link"></a></h3><p>Let&rsquo;s assume $$ d_{\text{model}} = 8 $$ (for simplicity), and calculate the position embeddings for $$ pos = 1 $$.</p><p>For $$ i = 0 $$:
$$ PE(1, 0) = \sin\left(\frac{1}{10000^{\frac{0}{8}}}\right) = \sin\left(1\right) $$</p><p>For $$ i = 1 $$:
$$ PE(1, 1) = \cos\left(\frac{1}{10000^{\frac{0}{8}}}\right) = \cos\left(1\right) $$</p><p>For $$ i = 2 $$:
$$ PE(1, 2) = \sin\left(\frac{1}{10000^{\frac{2}{8}}}\right) = \sin\left(\frac{1}{10}\right) $$</p><p>For $$ i = 3 $$:
$$ PE(1, 3) = \cos\left(\frac{1}{10000^{\frac{2}{8}}}\right) = \cos\left(\frac{1}{10}\right) $$</p><p>For $$ i = 4 $$:
$$ PE(1, 4) = \sin\left(\frac{1}{10000^{\frac{4}{8}}}\right) = \sin\left(\frac{1}{100}\right) $$</p><p>For $$ i = 5 $$:
$$ PE(1, 5) = \cos\left(\frac{1}{10000^{\frac{4}{8}}}\right) = \cos\left(\frac{1}{100}\right) $$</p><p>For $$ i = 6 $$:
$$ PE(1, 6) = \sin\left(\frac{1}{10000^{\frac{6}{8}}}\right) = \sin\left(\frac{1}{1000}\right) $$</p><p>For $$ i = 7 $$:
$$ PE(1, 7) = \cos\left(\frac{1}{10000^{\frac{6}{8}}}\right) = \cos\left(\frac{1}{1000}\right) $$</p><p>These values are then added to the corresponding token embeddings to provide the model with information about the position of each token in the sequence.</p><p><strong>Author</strong><br>Dr Hari Thapliyaal<br>dasarpai.com<br>linkedin.com/in/harithapliyal</p><div class=category-section><h4 class=category-section__title>Categories:</h4><div class=category-badges><a href=https://localhost:1313/categories/dsblog class=category-badge>dsblog</a></div></div><div class=td-tags><h4 class=td-tags__title>Tags:</h4><div class=category-badges><a href=https://localhost:1313/tags/transformers class=category-badge>Transformers</a><a href=https://localhost:1313/tags/deep-learning class=category-badge>Deep Learning</a><a href=https://localhost:1313/tags/natural-language-processing class=category-badge>Natural Language Processing</a><a href=https://localhost:1313/tags/neural-networks class=category-badge>Neural Networks</a><a href=https://localhost:1313/tags/machine-learning class=category-badge>Machine Learning</a><a href=https://localhost:1313/tags/ai-architecture class=category-badge>AI Architecture</a><a href=https://localhost:1313/tags/attention-mechanism class=category-badge>Attention Mechanism</a></div></div><div class=td-author-box><div class=td-author-box__info><h4 class=td-author-box__name>Blowfish</h4><p class=td-author-box__bio>A powerful, lightweight theme for Hugo.</p><div class=td-author-box__links><a href target=_blank rel=noopener aria-label></a><a href target=_blank rel=noopener aria-label></a></div></div></div><div class=td-social-share><h4 class=td-social-share__title>Share this article:</h4><ul class=td-social-share__list><div class=social-share><a href="https://twitter.com/intent/tweet?text=Transformers%20Demystified%20A%20Step-by-Step%20Guide&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2ftransformers-demystified-a-step-by-step-guide%2f" target=_blank rel=noopener aria-label="Share on Twitter"><i class="fab fa-twitter"></i>
</a><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2flocalhost%3a1313%2fdsblog%2ftransformers-demystified-a-step-by-step-guide%2f" target=_blank rel=noopener aria-label="Share on Facebook"><i class="fab fa-facebook"></i>
</a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2ftransformers-demystified-a-step-by-step-guide%2f&title=Transformers%20Demystified%20A%20Step-by-Step%20Guide" target=_blank rel=noopener aria-label="Share on LinkedIn"><i class="fab fa-linkedin"></i>
</a><a href="https://www.reddit.com/submit?url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2ftransformers-demystified-a-step-by-step-guide%2f&title=Transformers%20Demystified%20A%20Step-by-Step%20Guide" target=_blank rel=noopener aria-label="Share on Reddit"><i class="fab fa-reddit"></i>
</a><a href="mailto:?subject=Transformers%20Demystified%20A%20Step-by-Step%20Guide&body=https%3a%2f%2flocalhost%3a1313%2fdsblog%2ftransformers-demystified-a-step-by-step-guide%2f" aria-label="Share via Email"><i class="fas fa-envelope"></i></a></div></ul></div><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href=https://localhost:1313/dsblog/Dimensionality-Reduction-and-Visualization/ aria-label="Previous page"><div class=td-pager__meta><i class="fa-solid fa-angle-left"></i>
<span class=td-pager__meta-label><b>Previous:</b></span>
<span class=td-pager__meta-title>Dimensionality Reduction and Visualization</span></div></a><a class="td-pager__link td-pager__link--next" href=https://localhost:1313/dsblog/Understanding-LLM-GAN-and-Transformers/ aria-label="Next page"><div class=td-pager__meta><span class=td-pager__meta-label><b>Next:</b></span>
<span class=td-pager__meta-title>Understanding LLM GAN and Transformers</span>
<i class="fa-solid fa-angle-right"></i></div></a></ul></div><div class=col-md-2></div></div></main><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class=col-2><a href=https://dasarpai.com target=_blank rel=noopener><img src=https://localhost:1313/assets/images/site-logo.png alt=dasarpAI width=100 style=border-radius:12px></a></div><div class=col-8><div class=row></div><div class=row></div></div><div class=col-2></div><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"></div></div></div></footer></div><script src=https://localhost:1313/js/main.min.7c31f98e62c36b0c9c834ce3f8260a0e21895dd6aa0773e81a64b104eae3b2e8.js integrity="sha256-fDH5jmLDawycg0zj+CYKDiGJXdaqB3PoGmSxBOrjsug=" crossorigin=anonymous></script><script defer src=https://localhost:1313/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=https://localhost:1313/js/tabpane-persist.js></script><script src=https://localhost:1313/js/asciinema-player.js></script><script>(function(){var e=document.querySelector("#td-section-nav");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></body></html>