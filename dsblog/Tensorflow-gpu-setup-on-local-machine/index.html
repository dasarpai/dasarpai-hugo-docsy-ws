<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.147.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href="https://localhost:1313/favicons/favicon.ico?v=1"><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-16x16.png?v=1" sizes=16x16><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-32x32.png?v=1" sizes=32x32><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><title>Tensorflow GPU Setup on Local Machine | Blowfish</title><meta property="og:url" content="https://localhost:1313/dsblog/Tensorflow-gpu-setup-on-local-machine/">
<meta property="og:site_name" content="Blowfish"><meta property="og:title" content="Tensorflow GPU Setup on Local Machine"><meta property="og:description" content="Tensorflow GPU Setup on Local Machine Introduction Tensorflow, pytorch are deep learning libraries or packages. Tensorflow is developed by google and pytorch is developed by Meta. There are some other but these are the most popular one among Machine Learning and Deep Learning Engineers. If you are doing anything significant in NLP, computer vision, voice processing you must have used this library. But the power of the these libraries lies in parallel metrics/tensor computation. For that they use hardwardes like GPU or TPU which has thousands of core and they designed purely for metrics/tensor processing. Intially they were used for gaming purpose but with the surge of AI these machines are in high use and used for model training and inference purpose."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="dsblog"><meta property="article:published_time" content="2024-08-28T00:00:00+00:00"><meta property="article:modified_time" content="2024-08-28T00:00:00+00:00"><meta property="article:tag" content="TensorFlow"><meta property="article:tag" content="GPU Computing"><meta property="article:tag" content="CUDA"><meta property="article:tag" content="Deep Learning Setup"><meta property="article:tag" content="Docker Containers"><meta property="article:tag" content="Development Environment"><meta itemprop=name content="Tensorflow GPU Setup on Local Machine"><meta itemprop=description content="Tensorflow GPU Setup on Local Machine Introduction Tensorflow, pytorch are deep learning libraries or packages. Tensorflow is developed by google and pytorch is developed by Meta. There are some other but these are the most popular one among Machine Learning and Deep Learning Engineers. If you are doing anything significant in NLP, computer vision, voice processing you must have used this library. But the power of the these libraries lies in parallel metrics/tensor computation. For that they use hardwardes like GPU or TPU which has thousands of core and they designed purely for metrics/tensor processing. Intially they were used for gaming purpose but with the surge of AI these machines are in high use and used for model training and inference purpose."><meta itemprop=datePublished content="2024-08-28T00:00:00+00:00"><meta itemprop=dateModified content="2024-08-28T00:00:00+00:00"><meta itemprop=wordCount content="3156"><meta itemprop=keywords content="TensorFlow GPU Setup,CUDA Installation,Deep Learning Environment,Docker Configuration,GPU Computing,NVIDIA Drivers,Development Setup,Machine Learning Tools"><meta name=twitter:card content="summary"><meta name=twitter:title content="Tensorflow GPU Setup on Local Machine"><meta name=twitter:description content="Tensorflow GPU Setup on Local Machine Introduction Tensorflow, pytorch are deep learning libraries or packages. Tensorflow is developed by google and pytorch is developed by Meta. There are some other but these are the most popular one among Machine Learning and Deep Learning Engineers. If you are doing anything significant in NLP, computer vision, voice processing you must have used this library. But the power of the these libraries lies in parallel metrics/tensor computation. For that they use hardwardes like GPU or TPU which has thousands of core and they designed purely for metrics/tensor processing. Intially they were used for gaming purpose but with the surge of AI these machines are in high use and used for model training and inference purpose."><link rel=preload href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css as=style integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css rel=stylesheet integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link rel=stylesheet type=text/css href=https://localhost:1313/css/asciinema-player.css><script src=https://code.jquery.com/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><link rel=stylesheet href=https://localhost:1313/css/custom.css><script src=https://localhost:1313/js/lunr.js></script></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-light nav-shadow flex-column flex-md-row td-navbar"><a id=agones-top class=navbar-brand href=https://localhost:1313/><svg id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg> <span class="text-uppercase fw-bold">Blowfish</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=https://localhost:1313/><span class=active>Data Science</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Docs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/community/><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/shortcodes/><span>Short Codes</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/blog/><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://github.com/nunocoracao/blowfish><span></span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href>GitHub</a></li><li class="nav-item dropdown d-none d-lg-block"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Release</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://development.agones.dev>Development</a>
<a class=dropdown-item href=https://agones.dev></a><a class=dropdown-item href=https://1-47-0.agones.dev>1.47.0</a>
<a class=dropdown-item href=https://1-46-0.agones.dev>1.46.0</a>
<a class=dropdown-item href=https://1-45-0.agones.dev>1.45.0</a>
<a class=dropdown-item href=https://1-44-0.agones.dev>1.44.0</a>
<a class=dropdown-item href=https://1-43-0.agones.dev>1.43.0</a>
<a class=dropdown-item href=https://1-42-0.agones.dev>1.42.0</a>
<a class=dropdown-item href=https://1-41-0.agones.dev>1.41.0</a>
<a class=dropdown-item href=https://1-40-0.agones.dev>1.40.0</a>
<a class=dropdown-item href=https://1-39-0.agones.dev>1.39.0</a>
<a class=dropdown-item href=https://1-38-0.agones.dev>1.38.0</a>
<a class=dropdown-item href=https://1-37-0.agones.dev>1.37.0</a>
<a class=dropdown-item href=https://1-36-0.agones.dev>1.36.0</a>
<a class=dropdown-item href=https://1-35-0.agones.dev>1.35.0</a>
<a class=dropdown-item href=https://1-34-0.agones.dev>1.34.0</a>
<a class=dropdown-item href=https://1-33-0.agones.dev>1.33.0</a>
<a class=dropdown-item href=https://1-32-0.agones.dev>1.32.0</a>
<a class=dropdown-item href=https://1-31-0.agones.dev>1.31.0</a></div></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><ul class=dropdown-menu><li><a class=dropdown-item href=https://localhost:1313/it/>Italiano</a></li></ul></div></li></ul></div><div class="navbar-nav mx-lg-2 d-none d-lg-block"></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><div class=row><div class=col-md-2></div><div class=col-md-8><p><img src=https://localhost:1313/assets/images/dspost/dsp6140-Tensorflow-gpu-setup-on-local-machine.jpg alt="Tensorflow GPU Setup on Local Machine"></p><h1 id=tensorflow-gpu-setup-on-local-machine>Tensorflow GPU Setup on Local Machine<a class=td-heading-self-link href=#tensorflow-gpu-setup-on-local-machine aria-label="Heading self-link"></a></h1><h2 id=introduction>Introduction<a class=td-heading-self-link href=#introduction aria-label="Heading self-link"></a></h2><p>Tensorflow, pytorch are deep learning libraries or packages. Tensorflow is developed by google and pytorch is developed by Meta. There are some other but these are the most popular one among Machine Learning and Deep Learning Engineers. If you are doing anything significant in NLP, computer vision, voice processing you must have used this library. But the power of the these libraries lies in parallel metrics/tensor computation. For that they use hardwardes like GPU or TPU which has thousands of core and they designed purely for metrics/tensor processing. Intially they were used for gaming purpose but with the surge of AI these machines are in high use and used for model training and inference purpose.</p><p>To install any python libraries we can use pip or conda commands so we can use the same for these libraries. But the problem is many of learners do not have the GPU hardware machines. So even if you install it on cpu machine processing is very slow even with powerful machines. That is where google colab and kaggle come as handy tool for use. Google colab and kaggle provides us free hardware with limited hours GPU which can use used for experimenting or model training. Because these tools are designed for deep learning model training therefore by default they have tensorflow and pytorch installed. But these machines cannot be used for serious business purpose unless you pay them good money. The solution for that is you have your own dedicated GPU machine.</p><p>Now the actually trouble starts, you cannot install tensorflow or pytorch on the gpu machine as you were doing non-gpu machine. You need to make sure gpu hardware is available and being used within the development environemnt. But how do you do that? This article is about that!</p><h2 id=enabling-gpu-for-tensorflow>Enabling GPU for tensorflow<a class=td-heading-self-link href=#enabling-gpu-for-tensorflow aria-label="Heading self-link"></a></h2><p>Question: When I am using packages like tensorflow or pytorch on my local GPU machine then how to install it so that it is available for the development or deployment environment?</p><h3 id=highlevel-instructions>Highlevel instructions<a class=td-heading-self-link href=#highlevel-instructions aria-label="Heading self-link"></a></h3><p>When installing packages that support GPU, ensuring that the GPU is available and utilized in the development environment during model training involves several key steps. These steps ensure that your setup can take full advantage of the GPU for faster computations, especially in deep learning and machine learning tasks</p><ul><li>First you can decide which version of tensorflow or pytorch you want to use.</li><li>You need to use gpu compatable version of these libraries NOT the cpu version of the library.</li><li>Then check compatibility of tensorflow. <a href=https://www.tensorflow.org/guide/versions>https://www.tensorflow.org/guide/versions</a></li><li>Install GPU drivers, CUDA, cuDNN, and the deep learning library (tensorflow, pytorch etc) are compatible. Mismatched versions can lead to errors or inefficient GPU usage. It is time consuming and frustrating if you are just playing with random versions, therefore to avoid this pain do your research and make decision.</li><li>Use nvidia-smi to check whether GPU is being used or not.</li><li>Launch jupyter notebook and write small code to check whether gpu is available within the jupyter environment or not.</li></ul><h2 id=basic-terms>Basic Terms<a class=td-heading-self-link href=#basic-terms aria-label="Heading self-link"></a></h2><p><strong>What is GPU?</strong><br>The term &ldquo;GPU&rdquo; traditionally stands for &ldquo;Graphics Processing Unit,&rdquo; primarily used for rendering graphics. However, modern GPUs, especially those from NVIDIA, have evolved to handle more than just graphics—they can perform general-purpose computing tasks as well, which is referred to as GPGPU (General-Purpose computing on Graphics Processing Units). With the help of GPGPU, CUDA extends the capabilities of GPUs beyond graphics. It allows developers to write programs that can run on the GPU, making use of its parallel processing power for tasks such as scientific calculations, machine learning, and deep learning.</p><p><strong>What is the difference between Library and API?</strong><br>A library is a collection of pre-written code that provides specific functionalities you can directly use in your programs, like functions or classes. An API (Application Programming Interface) is a set of rules and protocols that defines how software components should interact, specifying how to use the functions and classes in a library without providing the actual implementation. Pandas, Numpy are library. In the context of NVIDIA cuBLAS (CUDA Basic Linear Algebra Subroutines) or cuDNN (CUDA Deep Neural Network) are libraries.</p><p><strong>What is NVIDIA Driver</strong>?<br>It acts as a bridge between the operating system and the NVIDIA GPU hardware. It allows the OS and applications to utilize the GPU for computing and rendering tasks.
Both CUDA and cuDNN require the NVIDIA driver to function, as it provides the basic communication and control capabilities with the GPU.</p><p><strong>What is CUDA (Compute Unified Device Architecture)</strong><br>A parallel computing platform and programming model developed by NVIDIA. It enables developers to use NVIDIA GPUs for general-purpose processing (GPGPU), which significantly accelerates tasks like deep learning. It requires the NVIDIA driver. CUDA provides the necessary tools and libraries for interacting with the GPU and forms the foundation for libraries like cuDNN.</p><p><strong>What is cuDNN (CUDA Deep Neural Network Library)</strong>?<br>A GPU-accelerated library for deep learning, optimized for running operations common in neural networks, such as convolutions, pooling, normalization, and activation functions. It is created by NVIDIA. It is built on top of CUDA, it requires CUDA to function. cuDNN is specifically tailored to accelerate deep learning workloads and is used by many deep learning frameworks.</p><p><strong>What is TensorFlow</strong>?<br>An open-source deep learning framework developed by Google. It provides a comprehensive ecosystem for developing, training, and deploying machine learning models, including neural networks. There is another popular deep learning framework PyTorch, it is developed by Meta. TensorFlow can use cuDNN and CUDA for GPU acceleration, which allows TensorFlow to run computations on the GPU for faster processing. TensorFlow relies on cuDNN for optimized deep learning operations and on CUDA for GPU capabilities.</p><p><strong>What is the relationship between NVIDIA Driver, CUDA, cuDNN, TensorFlow?</strong></p><ul><li><strong>NVIDIA Driver</strong> is the base layer, enabling communication with the GPU.</li><li><strong>CUDA</strong> leverages the NVIDIA Driver to provide a platform for GPU-accelerated computing.</li><li><strong>cuDNN</strong> is built on top of CUDA to optimize deep learning tasks.</li><li><strong>TensorFlow</strong> uses both cuDNN and CUDA to perform efficient and accelerated deep learning operations on NVIDIA GPUs.</li></ul><h2 id=what-are-different-options-for-tensorflow-installation>What are different options for Tensorflow Installation?<a class=td-heading-self-link href=#what-are-different-options-for-tensorflow-installation aria-label="Heading self-link"></a></h2><ol><li>Docker : There are dozens of images on dockerhub. Based on your need you can select a docker image from <a href=https://hub.docker.com/r/tensorflow/tensorflow/tags>the Link</a>.</li></ol><ul><li>Docker in windows. You can install and run the docker in Windows OS.</li><li>Docker in wsl/linux. You can install and run the docker in WSL/Linux.</li></ul><ol start=2><li>pip : Another way to use tensorflow is install the binary (compiled program or wheel) of tensorflow on your machine. For this purpose you need to create your own virtual environment first, then activate that environment and then do the pip installation. Which tensorlfow version you want to install you need to decide that first.</li></ol><ul><li>Windows: This virtual env setting and pip installation can be done on windows os.</li><li>WSL/ Linux: You can also create virtual env in WSL/linux and do pip instllation in that environment.</li></ul><ol start=3><li>Source : For this you can clone the tensorflow repository and build the wheel on your local machine and then install. For build you need other program on your machine which can compile the source code. Refer <a href=https://www.tensorflow.org/install/source>Link</a>. You need to install Bazel, Bazelisk is an easy way to install Bazel and automatically downloads the correct Bazel version for TensorFlow. Then Install Clang. Clang is a C/C++/Objective-C compiler that is compiled in C++ based on LLVM. If LLVM is not installed you need to install that as well. Some of these installation are just download, unzip, copy, paste and sometime you may need to install them properly.</li></ol><ul><li>Windows</li><li>WSL/Linux</li></ul><h2 id=installation-using-docker>Installation using Docker<a class=td-heading-self-link href=#installation-using-docker aria-label="Heading self-link"></a></h2><ol><li>Install docker desktop.</li><li>Install GPU support</li><li>Install NVIDIA Container Toolket. <a href=https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html>https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html</a></li><li>Configure docker engine</li><li>Configure containerd (for kubernetes)
1. Configure container runtime
2. Configuring podman</li><li>Run docker</li><li>sudo systemctl restart docker</li><li>Pull image from dockerhug
Let&rsquo;s assume you select tensorflow:latest-gpu-jupyter you can pull that image from dockerhub into your docker image, using following command.</li></ol><pre tabindex=0><code>docker pull tensorflow/tensorflow:latest-gpu-jupyter
</code></pre><ol start=4><li>Run image in a container</li></ol><h2 id=installation-on-wsllinux>Installation on WSL/Linux<a class=td-heading-self-link href=#installation-on-wsllinux aria-label="Heading self-link"></a></h2><p>When installing packages that support GPU, ensuring that the GPU is available and utilized in the development environment during model training involves several key steps. These steps ensure that your setup can take full advantage of the GPU for faster computations, especially in deep learning and machine learning tasks. Here’s what you need to consider:</p><h3 id=1-ensure-compatible-gpu-drivers-are-installed>1. <strong>Ensure Compatible GPU Drivers are Installed</strong><a class=td-heading-self-link href=#1-ensure-compatible-gpu-drivers-are-installed aria-label="Heading self-link"></a></h3><p>Before using GPU-enabled packages, make sure that the appropriate GPU drivers are installed. For NVIDIA GPUs, you need the NVIDIA driver that matches your GPU hardware.</p><ul><li><p><strong>Check Your GPU</strong>: Identify your GPU using:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>nvidia-smi
</span></span></code></pre></div><p>This command should list the GPU details if the drivers are correctly installed.</p><p>Example output</p></li></ul><pre tabindex=0><code>hari@Hari-MSI:/mnt/c/Users/hari_$ nvidia-smi

Tue Aug 27 17:21:37 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.81         CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4070 ...    On  |   00000000:01:00.0 Off |                  N/A |
| N/A   53C    P8              2W /   80W |    7926MiB /   8188MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A        75      C   /python3.11                                 N/A      |
|    0   N/A  N/A       315      C   /python3.11                                 N/A      |
+-----------------------------------------------------------------------------------------+
</code></pre><ul><li><strong>Install the Latest Drivers</strong>: Download and install the latest NVIDIA drivers from the <a href=https://www.nvidia.com/Download/index.aspx>NVIDIA website</a>. Make sure to select the correct driver version for your GPU model.</li></ul><p><strong>This will ask you type of NVIDIA GPU (GeForce, Titan, GRID, Networking, NVIDIA RTX/Quadro, NVS, ION etc). It also you Notebooks series (RTX20, RTX30, RTX40, MX500 etc), laptop GPU (RTX 4050, RTX4060, RTX 4070 etc), OS (Windows 11, linux 64bit, FreeBDS x64 etc). After it will show available GeForce Game Ready Driver and NVIDIA Studio Driver page. You can select and download NVIDIA graphics driver.</strong></p><h3 id=2-install-cuda-toolkit>2. <strong>Install CUDA Toolkit</strong><a class=td-heading-self-link href=#2-install-cuda-toolkit aria-label="Heading self-link"></a></h3><p>CUDA (Compute Unified Device Architecture) is a parallel computing platform and API model created by NVIDIA. Many deep learning libraries use CUDA to access GPU capabilities.</p><ul><li><strong>Download CUDA</strong>: Install the CUDA toolkit that matches your NVIDIA driver version. This can be done from the <a href=https://developer.nvidia.com/cuda-toolkit-archive>CUDA Toolkit Archive</a>.</li><li><strong>Set Environment Variables</strong>: Ensure that CUDA paths are set in your environment variables. For example, on Linux:</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span>/usr/local/cuda-11.2/bin<span class=si>${</span><span class=nv>PATH</span><span class=p>:+:</span><span class=si>${</span><span class=nv>PATH</span><span class=si>}}</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>LD_LIBRARY_PATH</span><span class=o>=</span>/usr/local/cuda-11.2/lib64<span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>                        <span class=si>${</span><span class=nv>LD_LIBRARY_PATH</span><span class=p>:+:</span><span class=si>${</span><span class=nv>LD_LIBRARY_PATH</span><span class=si>}}</span>
</span></span></code></pre></div><h3 id=3-install-cudnn-cuda-deep-neural-network-library>3. <strong>Install cuDNN (CUDA Deep Neural Network Library)</strong><a class=td-heading-self-link href=#3-install-cudnn-cuda-deep-neural-network-library aria-label="Heading self-link"></a></h3><p>cuDNN is a GPU-accelerated library for deep neural networks, providing highly tuned implementations for standard routines such as forward and backward convolution, pooling, normalization, and activation layers.</p><ul><li><p><strong>Download cuDNN</strong>: You can download it from the <a href=https://developer.nvidia.com/cudnn>NVIDIA cuDNN page</a>.</p></li><li><p><strong>Install cuDNN</strong>: Follow the installation instructions for your operating system. This usually involves copying certain files to the CUDA toolkit directories.</p></li></ul><h3 id=4-install-gpu-compatible-python-libraries>4. <strong>Install GPU-Compatible Python Libraries</strong><a class=td-heading-self-link href=#4-install-gpu-compatible-python-libraries aria-label="Heading self-link"></a></h3><p>When using deep learning frameworks like TensorFlow, PyTorch, or others, you need to install the GPU-compatible versions of these libraries. Here&rsquo;s how to do this for some common libraries:</p><ul><li><p><strong>TensorFlow</strong>: Install the GPU version of TensorFlow using:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install tensorflow-gpu
</span></span></code></pre></div></li><li><p><strong>PyTorch</strong>: Visit the <a href=https://pytorch.org/get-started/locally/>PyTorch website</a> and select your preferences (OS, package manager, Python version, CUDA version). For example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117
</span></span></code></pre></div><p>This command installs PyTorch with CUDA 11.7 support.</p></li></ul><h3 id=5-verify-gpu-availability-in-your-environment>5. <strong>Verify GPU Availability in Your Environment</strong><a class=td-heading-self-link href=#5-verify-gpu-availability-in-your-environment aria-label="Heading self-link"></a></h3><p>Once all installations are complete, it’s crucial to verify that the GPU is detected and utilized by your deep learning framework:</p><ul><li><p><strong>TensorFlow</strong>: Check if TensorFlow can access the GPU with:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Num GPUs Available: &#34;</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>list_physical_devices</span><span class=p>(</span><span class=s1>&#39;GPU&#39;</span><span class=p>)))</span>
</span></span></code></pre></div></li><li><p><strong>PyTorch</strong>: Verify GPU availability in PyTorch:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Is CUDA available?&#34;</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;CUDA Device Name:&#34;</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>get_device_name</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span>
</span></span></code></pre></div></li></ul><h3 id=6-common-troubleshooting-tips>6. <strong>Common Troubleshooting Tips</strong><a class=td-heading-self-link href=#6-common-troubleshooting-tips aria-label="Heading self-link"></a></h3><ul><li><p><strong>Version Compatibility</strong>: Ensure the versions of your GPU drivers, CUDA, cuDNN, and the deep learning library are compatible. Mismatched versions can lead to errors or inefficient GPU usage.</p></li><li><p><strong>Environment Variables</strong>: Double-check that CUDA and cuDNN environment variables are correctly set and point to the appropriate directories.</p></li><li><p><strong>Check Dependencies</strong>: Use <code>nvidia-smi</code> to monitor GPU usage and ensure that your application is utilizing the GPU during model training.</p></li></ul><h3 id=example-setting-up-tensorflow-with-gpu>Example: Setting Up TensorFlow with GPU<a class=td-heading-self-link href=#example-setting-up-tensorflow-with-gpu aria-label="Heading self-link"></a></h3><p>Here’s a step-by-step example for setting up TensorFlow with GPU:</p><ol><li><p><strong>Install NVIDIA Driver</strong>:</p><ul><li>Download and install the latest driver from the NVIDIA website suitable for your GPU model.</li></ul></li><li><p><strong>Install CUDA Toolkit</strong>:</p><ul><li>Download CUDA 11.2 (if using TensorFlow 2.6) and follow the installation instructions.</li></ul></li><li><p><strong>Install cuDNN</strong>:</p><ul><li>Download cuDNN for CUDA 11.2, extract the files, and copy them to the CUDA directories (e.g., <code>/usr/local/cuda-11.2/lib64</code>).</li></ul></li><li><p><strong>Install TensorFlow-GPU</strong>:</p><ul><li>Install TensorFlow with GPU support using <code>pip</code>:<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install tensorflow-gpu<span class=o>==</span>2.6.0
</span></span></code></pre></div></li></ul></li><li><p><strong>Verify Setup</strong>:</p><ul><li>Run a simple TensorFlow script to check if the GPU is recognized:<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Num GPUs Available: &#34;</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>list_physical_devices</span><span class=p>(</span><span class=s1>&#39;GPU&#39;</span><span class=p>)))</span>
</span></span></code></pre></div></li></ul></li></ol><p>By following these steps, you can ensure that your development environment is correctly set up to leverage the GPU for model training, thereby speeding up computations and improving performance.</p><h2 id=important-commands>Important Commands<a class=td-heading-self-link href=#important-commands aria-label="Heading self-link"></a></h2><h1 id=how-to-know-which-ubunto-version-on-my-wsl>How to know which ubunto version on my wsl?<a class=td-heading-self-link href=#how-to-know-which-ubunto-version-on-my-wsl aria-label="Heading self-link"></a></h1><pre tabindex=0><code>hari@Hari-MSI:/mnt/c/Users/hari_$ lsb_release -a

Output
	No LSB modules are available.
	Distributor draft: false
id: Ubuntu
	Description:    Ubuntu 22.04.3 LTS
	Release:        22.04
	Codename:       jammy
	
hari@Hari-MSI:/mnt/c/Users/hari_$ cat /etc/os-release

output
	PRETTY_NAME=&#34;Ubuntu 22.04.3 LTS&#34;
	NAME=&#34;Ubuntu&#34;
	VERSION_ID=&#34;22.04&#34;
	VERSION=&#34;22.04.3 LTS (Jammy Jellyfish)&#34;
	VERSION_CODENAME=jammy
	ID=ubuntu
	ID_LIKE=debian
	HOME_URL=&#34;https://www.ubuntu.com/&#34;
	SUPPORT_URL=&#34;https://help.ubuntu.com/&#34;
	BUG_REPORT_URL=&#34;https://bugs.launchpad.net/ubuntu/&#34;
	PRIVACY_POLICY_URL=&#34;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&#34;
	UBUNTU_CODENAME=jammy
</code></pre><h2 id=how-to-install-cuda-118-and-cudnn-81-for-tensorflow-2170-gpu>How to install CUDA 11.8 and cuDNN 8.1 for tensorflow-2.17.0-gpu?<a class=td-heading-self-link href=#how-to-install-cuda-118-and-cudnn-81-for-tensorflow-2170-gpu aria-label="Heading self-link"></a></h2><h3 id=1-use-a-compatible-tensorflow-docker-image>1. Use a Compatible TensorFlow Docker Image<a class=td-heading-self-link href=#1-use-a-compatible-tensorflow-docker-image aria-label="Heading self-link"></a></h3><p>To ensure compatibility, it&rsquo;s easiest to start with a TensorFlow Docker image that already includes the correct versions of CUDA and cuDNN. TensorFlow provides pre-built Docker images with specific versions of CUDA and cuDNN.</p><ul><li><strong>Pull the TensorFlow Docker Image</strong>: Use a TensorFlow Docker image that is known to support CUDA 11.8 and cuDNN 8.1. The <code>tensorflow/tensorflow:2.17.0-gpu</code> tag typically comes with a compatible version of CUDA, but to specifically get CUDA 11.8, you might need to specify a tag like <code>tensorflow/tensorflow:2.17.0-gpu-cuda11.8</code>.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker pull tensorflow/tensorflow:2.17.0-gpu-cuda11.8
</span></span></code></pre></div><h3 id=2-verify-cuda-and-cudnn-versions-inside-the-container>2. Verify CUDA and cuDNN Versions Inside the Container<a class=td-heading-self-link href=#2-verify-cuda-and-cudnn-versions-inside-the-container aria-label="Heading self-link"></a></h3><p>After pulling the Docker image, run the container and verify the CUDA and cuDNN versions:</p><ul><li><strong>Run the Docker Container</strong>:</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker run --gpus all -it --rm -p 9999:8888 tensorflow/tensorflow:2.17.0-gpu-cuda11.8 bash
</span></span></code></pre></div><pre><code>This command starts an interactive bash session inside the container.
</code></pre><ul><li><strong>Check CUDA Version</strong>:</li></ul><p>Inside the container, check the CUDA version:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>nvcc --version
</span></span></code></pre></div><p>This should show CUDA 11.8.</p><p>hari@Hari-MSI:/mnt/c/Users/hari_$ nvcc &ndash;version</p><p>nvcc: NVIDIA (R) Cuda compiler driver<br>Copyright (c) 2005-2022 NVIDIA Corporation<br>Built on Wed_Sep_21_10:33:58_PDT_2022<br>Cuda compilation tools, release 11.8, V11.8.89<br>Build cuda_11.8.r11.8/compiler.31833905_0</p><p>This was the default cuda with my docker.<br>nvcc: NVIDIA (R) Cuda compiler driver<br>Copyright (c) 2005-2023 NVIDIA Corporation<br>Built on Wed_Nov_22_10:17:15_PST_2023<br>Cuda compilation tools, release 12.3, V12.3.107<br>Build cuda_12.3.r12.3/compiler.33567101_0</p><ul><li><p><strong>Check cuDNN Version</strong>:</p><p>To check the installed cuDNN version, you can use the following commands inside the container:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>cat /usr/local/cuda/include/cudnn_version.h <span class=p>|</span> grep CUDNN_MAJOR -A <span class=m>2</span>
</span></span></code></pre></div><p>This should confirm cuDNN 8.1.</p></li></ul><h3 id=3-manually-install-cudnn-81-if-needed>3. Manually Install cuDNN 8.1 (if needed)<a class=td-heading-self-link href=#3-manually-install-cudnn-81-if-needed aria-label="Heading self-link"></a></h3><p>If you find that the cuDNN version isn&rsquo;t 8.1 (though it should be with the right Docker image), you can manually install it:</p><ol><li><strong>Download cuDNN 8.1</strong>: Go to the <a href=https://developer.nvidia.com/cudnn>NVIDIA cuDNN download page</a> and download the cuDNN version 8.1 for CUDA 11.8.</li></ol><pre tabindex=0><code># following the above link to download cuDNN I got this link commands below.
https://developer.nvidia.com/cudnn-downloads?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=20.04&amp;target_type=deb_local

	wget https://developer.download.nvidia.com/compute/cudnn/9.3.0/local_installers/cudnn-local-repo-ubuntu2004-9.3.0_1.0-1_amd64.deb
	sudo dpkg -i cudnn-local-repo-ubuntu2004-9.3.0_1.0-1_amd64.deb
	sudo cp /var/cudnn-local-repo-ubuntu2004-9.3.0/cudnn-*-keyring.gpg /usr/share/keyrings/
	sudo apt-get update
	sudo apt-get -y install cudnn
</code></pre><ol start=2><li><strong>Transfer the cuDNN File into the Docker Container</strong>:</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker cp /path/to/cudnn-linux-x86_64-8.1.x.x-cuda11.8-archive.tar.xz &lt;container_id&gt;:/root/
</span></span></code></pre></div><ol start=3><li><strong>Install cuDNN Inside the Docker Container</strong>:</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>tar -xzvf cudnn-linux-x86_64-8.1.x.x-cuda11.8-archive.tar.xz
</span></span><span class=line><span class=cl>cp cudnn-linux-x86_64-8.1.x.x-cuda11.8-archive/include/* /usr/local/cuda/include/
</span></span><span class=line><span class=cl>cp cudnn-linux-x86_64-8.1.x.x-cuda11.8-archive/lib/* /usr/local/cuda/lib64/
</span></span><span class=line><span class=cl>ldconfig
</span></span></code></pre></div><h3 id=4-run-jupyter-notebook>4. Run Jupyter Notebook<a class=td-heading-self-link href=#4-run-jupyter-notebook aria-label="Heading self-link"></a></h3><p>Finally, start Jupyter Notebook in the container:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>jupyter notebook --ip<span class=o>=</span>0.0.0.0 --allow-root
</span></span></code></pre></div><p>You should be able to access Jupyter at <code>http://localhost:9999</code>.</p><h3 id=summary>Summary<a class=td-heading-self-link href=#summary aria-label="Heading self-link"></a></h3><ul><li>Use a TensorFlow Docker image with CUDA 11.8 support.</li><li>Verify the CUDA and cuDNN versions inside the container.</li><li>Manually install cuDNN 8.1 if it&rsquo;s not already included.</li><li>Run Jupyter Notebook to start your development.</li></ul><h1 id=nvcc-version>nvcc &ndash;version<a class=td-heading-self-link href=#nvcc-version aria-label="Heading self-link"></a></h1><pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Wed_Nov_22_10:17:15_PST_2023
Cuda compilation tools, release 12.3, V12.3.107
Build cuda_12.3.r12.3/compiler.33567101_0
</code></pre><p><strong>Note:</strong> Check TensorFlow&rsquo;s Compatibility: Visit the official TensorFlow compatibility guide to verify which CUDA and cuDNN versions are supported by TensorFlow 2.17.0. As of recent releases, TensorFlow 2.17.0 is generally compatible with:</p><p>CUDA: 11.2, 11.3, 11.4, 11.5, 11.6, 11.7, 11.8
cuDNN: 8.1 and later versions</p><h2 id=how-to-install-jupyter-in-docker-container>How to install Jupyter in docker container?<a class=td-heading-self-link href=#how-to-install-jupyter-in-docker-container aria-label="Heading self-link"></a></h2><h3 id=steps-to-ensure-jupyter-is-installed-and-accessible>Steps to Ensure Jupyter is Installed and Accessible:<a class=td-heading-self-link href=#steps-to-ensure-jupyter-is-installed-and-accessible aria-label="Heading self-link"></a></h3><ol><li><p><strong>Start the TensorFlow Container in Interactive Mode:</strong>
Run the container with a bash shell so you can install Jupyter:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker run --gpus all -it --rm -p 9999:8888 tensorflow/tensorflow:2.17.0-gpu bash
</span></span></code></pre></div></li><li><p><strong>Install Jupyter:</strong>
Once inside the container, install Jupyter Notebook using <code>pip</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install jupyter
</span></span></code></pre></div></li><li><p><strong>Verify Jupyter Installation:</strong>
After installation, check if Jupyter is in the PATH by running:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>which jupyter
</span></span></code></pre></div><p>If this command returns a path (e.g., <code>/usr/local/bin/jupyter</code>), Jupyter is successfully installed.</p></li><li><p><strong>Run Jupyter Notebook:</strong>
Start the Jupyter Notebook server:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>jupyter notebook --ip<span class=o>=</span>0.0.0.0 --allow-root
</span></span></code></pre></div><p>If you see the Jupyter server starting and showing URLs (as before), this means it is working correctly.</p></li><li><p><strong>Access Jupyter Notebook:</strong></p><ul><li>Open your browser and go to <a href=http://localhost:9999>http://localhost:9999</a>.</li><li>Use the token provided by Jupyter to log in.</li></ul></li></ol><h3 id=troubleshooting>Troubleshooting:<a class=td-heading-self-link href=#troubleshooting aria-label="Heading self-link"></a></h3><ul><li><p><strong>Container Restart:</strong> If you exit the container, any installed packages will be lost because of the <code>--rm</code> flag. For persistent changes, consider building a custom Docker image or running the container without <code>--rm</code> and then committing the changes.</p></li><li><p><strong>Creating a Custom Docker Image (Optional):</strong>
If you want a persistent environment, you can create a custom Docker image that includes Jupyter:</p><p>Create a <code>Dockerfile</code> with the following content:</p></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-Dockerfile data-lang=Dockerfile><span class=line><span class=cl><span class=k>FROM</span><span class=s> tensorflow/tensorflow:2.17.0-gpu</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> pip install jupyter<span class=err>
</span></span></span></code></pre></div><p>Build and run the Docker image:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker build -t my-tf-jupyter .
</span></span><span class=line><span class=cl>docker run --gpus all -it --rm -p 9999:8888 my-tf-jupyter jupyter notebook --ip<span class=o>=</span>0.0.0.0 --allow-root
</span></span></code></pre></div><p>Above command will show one url on the screen, along with port and token. You can go on browser use this information to access the jupyter notebook.</p><p>You can access this from visual code as well. For that purpose on the kernel when IDE ask you for jupyter server you can give this url information.</p><h2 id=test-gpu-utilization>Test GPU Utilization<a class=td-heading-self-link href=#test-gpu-utilization aria-label="Heading self-link"></a></h2><pre tabindex=0><code>import tensorflow as tf
print(&#34;Num GPUs Available: &#34;, len(tf.config.list_physical_devices(&#39;GPU&#39;)))

# Simple matrix multiplication to test GPU
with tf.device(&#39;/GPU:0&#39;):
    a = tf.constant([[1.0, 2.0], [3.0, 4.0]])
    b = tf.constant([[1.0, 1.0], [0.0, 1.0]])
    c = tf.matmul(a, b)
print(c)
</code></pre><h2 id=if-you-want-to-disable-onednn-custom-operations-for-consistency>If you want to disable oneDNN custom operations for consistency:<a class=td-heading-self-link href=#if-you-want-to-disable-onednn-custom-operations-for-consistency aria-label="Heading self-link"></a></h2><pre tabindex=0><code>export TF_ENABLE_ONEDNN_OPTS=0
</code></pre><p>You can set this in your Docker container or your host environment if you&rsquo;re running scripts directly</p><h2 id=where-should-i-install-cuda-and-cudnn-libaries>Where should I install CUDA and cuDNN libaries<a class=td-heading-self-link href=#where-should-i-install-cuda-and-cudnn-libaries aria-label="Heading self-link"></a></h2><ul><li>If you are using tensorflow from docker then<ul><li>The CUDA and cuDNN libraries must be installed inside the Docker container for TensorFlow to use them.</li></ul></li><li>If you are using tensorflow on your windows then<ul><li>The CUDA and CuDNN libraries must be installed inside windows environment.</li></ul></li><li>If you are using tensorflow on linux/wsl then<ul><li>it must installed on liux/wsl</li></ul></li></ul><h2 id=how-to-build-new-docker-or-update-existing-docker>How to build new docker? Or update existing docker.<a class=td-heading-self-link href=#how-to-build-new-docker-or-update-existing-docker aria-label="Heading self-link"></a></h2><p>Create a folder and go into that folder<br>Create a Dockerfile. Write all the command in this.<br>go on the command prompt and execute command: docker build -t my-tf-jupyter .</p><h2 id=what-should-be-the-content-of-dockerfile>What should be the content of Dockerfile?<a class=td-heading-self-link href=#what-should-be-the-content-of-dockerfile aria-label="Heading self-link"></a></h2><p>FROM tensorflow/tensorflow:2.17.0-gpu<br>RUN pip install jupyter</p><p><strong>Author</strong><br>Dr Hari Thapliyaal<br>dasarpai.com<br>linkedin.com/in/harithapliyal</p><div class=category-section><h4 class=category-section__title>Categories:</h4><div class=category-badges><a href=https://localhost:1313/categories/dsblog class=category-badge>dsblog</a></div></div><div class=td-tags><h4 class=td-tags__title>Tags:</h4><div class=category-badges><a href=https://localhost:1313/tags/tensorflow class=category-badge>TensorFlow</a><a href=https://localhost:1313/tags/gpu-computing class=category-badge>GPU Computing</a><a href=https://localhost:1313/tags/cuda class=category-badge>CUDA</a><a href=https://localhost:1313/tags/deep-learning-setup class=category-badge>Deep Learning Setup</a><a href=https://localhost:1313/tags/docker-containers class=category-badge>Docker Containers</a><a href=https://localhost:1313/tags/development-environment class=category-badge>Development Environment</a><a href=https://localhost:1313/tags/machine-learning-infrastructure class=category-badge>Machine Learning Infrastructure</a></div></div><div class=td-author-box><div class=td-author-box__info><h4 class=td-author-box__name>Blowfish</h4><p class=td-author-box__bio>A powerful, lightweight theme for Hugo.</p><div class=td-author-box__links><a href target=_blank rel=noopener aria-label></a><a href target=_blank rel=noopener aria-label></a></div></div></div><div class=td-social-share><h4 class=td-social-share__title>Share this article:</h4><ul class=td-social-share__list><div class=social-share><a href="https://twitter.com/intent/tweet?text=Tensorflow%20GPU%20Setup%20on%20Local%20Machine&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fTensorflow-gpu-setup-on-local-machine%2f" target=_blank rel=noopener aria-label="Share on Twitter"><i class="fab fa-twitter"></i>
</a><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fTensorflow-gpu-setup-on-local-machine%2f" target=_blank rel=noopener aria-label="Share on Facebook"><i class="fab fa-facebook"></i>
</a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fTensorflow-gpu-setup-on-local-machine%2f&title=Tensorflow%20GPU%20Setup%20on%20Local%20Machine" target=_blank rel=noopener aria-label="Share on LinkedIn"><i class="fab fa-linkedin"></i>
</a><a href="https://www.reddit.com/submit?url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fTensorflow-gpu-setup-on-local-machine%2f&title=Tensorflow%20GPU%20Setup%20on%20Local%20Machine" target=_blank rel=noopener aria-label="Share on Reddit"><i class="fab fa-reddit"></i>
</a><a href="mailto:?subject=Tensorflow%20GPU%20Setup%20on%20Local%20Machine&body=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fTensorflow-gpu-setup-on-local-machine%2f" aria-label="Share via Email"><i class="fas fa-envelope"></i></a></div></ul></div><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href=https://localhost:1313/dsblog/All-About-ai-Hype/ aria-label="Previous page"><div class=td-pager__meta><i class="fa-solid fa-angle-left"></i>
<span class=td-pager__meta-label><b>Previous:</b></span>
<span class=td-pager__meta-title>All About AI Hype</span></div></a><a class="td-pager__link td-pager__link--next" href=https://localhost:1313/dsblog/What-is-Package-Manager/ aria-label="Next page"><div class=td-pager__meta><span class=td-pager__meta-label><b>Next:</b></span>
<span class=td-pager__meta-title>What is Package Manager?</span>
<i class="fa-solid fa-angle-right"></i></div></a></ul></div><div class=col-md-2></div></div></main><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class=col-2><a href=https://dasarpai.com target=_blank rel=noopener><img src=https://localhost:1313/assets/images/site-logo.png alt=dasarpAI width=100 style=border-radius:12px></a></div><div class=col-8><div class=row></div><div class=row></div></div><div class=col-2></div><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"></div></div></div></footer></div><script src=https://localhost:1313/js/main.min.7c31f98e62c36b0c9c834ce3f8260a0e21895dd6aa0773e81a64b104eae3b2e8.js integrity="sha256-fDH5jmLDawycg0zj+CYKDiGJXdaqB3PoGmSxBOrjsug=" crossorigin=anonymous></script><script defer src=https://localhost:1313/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=https://localhost:1313/js/tabpane-persist.js></script><script src=https://localhost:1313/js/asciinema-player.js></script><script>(function(){var e=document.querySelector("#td-section-nav");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></body></html>