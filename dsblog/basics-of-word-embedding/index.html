<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.147.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href="https://localhost:1313/favicons/favicon.ico?v=1"><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-16x16.png?v=1" sizes=16x16><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-32x32.png?v=1" sizes=32x32><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><title>Basics of Word Embedding | Blowfish</title><meta property="og:url" content="https://localhost:1313/dsblog/basics-of-word-embedding/">
<meta property="og:site_name" content="Blowfish"><meta property="og:title" content="Basics of Word Embedding"><meta property="og:description" content="Basics of Word Embedding What is Context, target and window? The “context” word is the surrounding word. The “target” word is the middle word. The “window distance” is number of words (including) between context words and target word. Window distance 1 means, one word surronding the target, one left side context word, one right context word. Two window distance means 2 words left and 2 words right. Let’s take a sentence"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="dsblog"><meta property="article:published_time" content="2023-11-11T00:00:00+00:00"><meta property="article:modified_time" content="2023-11-11T00:00:00+00:00"><meta property="article:tag" content="Word Embedding"><meta property="article:tag" content="NLP"><meta property="article:tag" content="Vector Representation"><meta property="article:tag" content="Text Processing"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Neural Networks"><meta itemprop=name content="Basics of Word Embedding"><meta itemprop=description content="Basics of Word Embedding What is Context, target and window? The “context” word is the surrounding word. The “target” word is the middle word. The “window distance” is number of words (including) between context words and target word. Window distance 1 means, one word surronding the target, one left side context word, one right context word. Two window distance means 2 words left and 2 words right. Let’s take a sentence"><meta itemprop=datePublished content="2023-11-11T00:00:00+00:00"><meta itemprop=dateModified content="2023-11-11T00:00:00+00:00"><meta itemprop=wordCount content="2212"><meta itemprop=keywords content="Word Embeddings,Text Vectorization,Natural Language Processing,Word2Vec,GloVe,BERT Embeddings,Neural Language Models,Text Analysis"><meta name=twitter:card content="summary"><meta name=twitter:title content="Basics of Word Embedding"><meta name=twitter:description content="Basics of Word Embedding What is Context, target and window? The “context” word is the surrounding word. The “target” word is the middle word. The “window distance” is number of words (including) between context words and target word. Window distance 1 means, one word surronding the target, one left side context word, one right context word. Two window distance means 2 words left and 2 words right. Let’s take a sentence"><link rel=preload href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css as=style integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css rel=stylesheet integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link rel=stylesheet type=text/css href=https://localhost:1313/css/asciinema-player.css><script src=https://code.jquery.com/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><link rel=stylesheet href=https://localhost:1313/css/custom.css><script src=https://localhost:1313/js/lunr.js></script></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-light nav-shadow flex-column flex-md-row td-navbar"><a id=agones-top class=navbar-brand href=https://localhost:1313/><svg id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg> <span class="text-uppercase fw-bold">Blowfish</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=https://localhost:1313/><span class=active>Data Science</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Docs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/community/><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/shortcodes/><span>Short Codes</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/blog/><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://github.com/nunocoracao/blowfish><span></span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href>GitHub</a></li><li class="nav-item dropdown d-none d-lg-block"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Release</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://development.agones.dev>Development</a>
<a class=dropdown-item href=https://agones.dev></a><a class=dropdown-item href=https://1-47-0.agones.dev>1.47.0</a>
<a class=dropdown-item href=https://1-46-0.agones.dev>1.46.0</a>
<a class=dropdown-item href=https://1-45-0.agones.dev>1.45.0</a>
<a class=dropdown-item href=https://1-44-0.agones.dev>1.44.0</a>
<a class=dropdown-item href=https://1-43-0.agones.dev>1.43.0</a>
<a class=dropdown-item href=https://1-42-0.agones.dev>1.42.0</a>
<a class=dropdown-item href=https://1-41-0.agones.dev>1.41.0</a>
<a class=dropdown-item href=https://1-40-0.agones.dev>1.40.0</a>
<a class=dropdown-item href=https://1-39-0.agones.dev>1.39.0</a>
<a class=dropdown-item href=https://1-38-0.agones.dev>1.38.0</a>
<a class=dropdown-item href=https://1-37-0.agones.dev>1.37.0</a>
<a class=dropdown-item href=https://1-36-0.agones.dev>1.36.0</a>
<a class=dropdown-item href=https://1-35-0.agones.dev>1.35.0</a>
<a class=dropdown-item href=https://1-34-0.agones.dev>1.34.0</a>
<a class=dropdown-item href=https://1-33-0.agones.dev>1.33.0</a>
<a class=dropdown-item href=https://1-32-0.agones.dev>1.32.0</a>
<a class=dropdown-item href=https://1-31-0.agones.dev>1.31.0</a></div></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><ul class=dropdown-menu><li><a class=dropdown-item href=https://localhost:1313/it/>Italiano</a></li></ul></div></li></ul></div><div class="navbar-nav mx-lg-2 d-none d-lg-block"></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><div class=row><div class=col-md-2></div><div class=col-md-8><p><img src=https://localhost:1313/assets/images/dspost/dsp6101-Basics-of-Word-Embedding.jpg alt="Basics of Word Embedding"></p><h1 id=basics-of-word-embedding>Basics of Word Embedding<a class=td-heading-self-link href=#basics-of-word-embedding aria-label="Heading self-link"></a></h1><h2 id=what-is-context-target-and-window>What is Context, target and window?<a class=td-heading-self-link href=#what-is-context-target-and-window aria-label="Heading self-link"></a></h2><ul><li>The &ldquo;context&rdquo; word is the surrounding word.</li><li>The &ldquo;target&rdquo; word is the middle word.</li><li>The &ldquo;window distance&rdquo; is number of words (including) between context words and target word. Window distance 1 means, one word surronding the target, one left side context word, one right context word. Two window distance means 2 words left and 2 words right.</li></ul><p>Let&rsquo;s take a sentence</p><blockquote><p>The quick brown fox jump over a lazy dog.</p></blockquote><p>R- Right, L - Left</p><table><thead><tr><th>target</th><th>context 1 window</th><th>context 2 window</th></tr></thead><tbody><tr><td>the</td><td>quick (R)</td><td>quick(R), brown(R)</td></tr><tr><td>quick</td><td>the(L), brown(R)</td><td>the(L), brown(R), fox(R)</td></tr><tr><td>brown</td><td>quick(L), fox(R)</td><td>the(L), quick(L), fox(R), jump(R)</td></tr><tr><td>fox</td><td>brown(L), jump(R)</td><td>quick(L), brown(L), jump(R), over(R)</td></tr></tbody></table><p>When creating dataset you don&rsquo;t write multiple words in one row, but you create multiple rows, as below.</p><table><thead><tr><th>target</th><th>context 2 window</th></tr></thead><tbody><tr><td>the</td><td>quick</td></tr><tr><td>the</td><td>brown</td></tr><tr><td>quick</td><td>the</td></tr><tr><td>quick</td><td>brown</td></tr><tr><td>quick</td><td>fox</td></tr></tbody></table><h2 id=what-is-skipgram>What is Skipgram?<a class=td-heading-self-link href=#what-is-skipgram aria-label="Heading self-link"></a></h2><p>Skipgram: <strong>With the help of target word</strong> we want to predict the context/surrounding word. From above example predicting &ldquo;quick&rdquo;, &ldquo;brown&rdquo;, &ldquo;the&rdquo;, &ldquo;brown&rdquo; etc with target word &ldquo;the&rdquo;, &ldquo;quick&rdquo;</p><h2 id=what-is-cbow-continuous-bag-of-words>What is CBOW (Continuous Bag of Words)<a class=td-heading-self-link href=#what-is-cbow-continuous-bag-of-words aria-label="Heading self-link"></a></h2><p>CBOW : <strong>With the help of context</strong> we want to predict target. From above example, predicting &ldquo;the&rdquo;, &ldquo;quick&rdquo; when context words are &ldquo;quick&rdquo; or &ldquo;brown&rdquo;, &ldquo;the&rdquo;, &ldquo;fox&rdquo;.</p><h2 id=how-cbow-works>How CBOW works?<a class=td-heading-self-link href=#how-cbow-works aria-label="Heading self-link"></a></h2><p>For both, CBOW and Skipgram networks works in the same way as mentioned below. Only difference is when we are using CBOW we want to predict target word from context word. If you are using Skipgram then we want to predict context word from a target word.</p><h3 id=finalize-the-corpus-step-1>Finalize the corpus (Step 1)<a class=td-heading-self-link href=#finalize-the-corpus-step-1 aria-label="Heading self-link"></a></h3><p>In reality corpus is extremely huge size, it is like entire wikipedia text or entire stakeoverflow text or entire quora text. For the illustration of skipgram we are taking a small example.</p><p><strong>Corpus</strong> : The quick brown fox jump over the dog</p><h3 id=create-skipgram-step-234->Create Skipgram (Step 2+3+4 )<a class=td-heading-self-link href=#create-skipgram-step-234- aria-label="Heading self-link"></a></h3><p>As discussed earlier created 1 or 2 or 3 window skipgram from the corpus.</p><table><thead><tr><th>Word</th><th>(Step 3) onehot encoding for each word in the corpus</th><th>(Step 4) random initial embedding, 4 dimensional</th></tr></thead><tbody><tr><td>the</td><td>[1,0,0,0,0,0,0,0]</td><td>[0.11,0.12,0.14,0.15]</td></tr><tr><td>quick</td><td>[0,1,0,0,0,0,0,0]</td><td>[0.21,0.23,0.24,0.26]</td></tr><tr><td>brown</td><td>[0,0,1,0,0,0,0,0]</td><td>[0.31,0.34,0.36,0.38]</td></tr><tr><td>fox</td><td>[0,0,0,1,0,0,0,0]</td><td>[0.51,0.12,0.14,0.15]</td></tr><tr><td>jump</td><td>[0,0,0,0,1,0,0,0]</td><td>[0.21,0.63,0.24,0.26]</td></tr><tr><td>over</td><td>[0,0,0,0,0,1,0,0]</td><td>[0.31,0.34,0.86,0.38]</td></tr><tr><td>the</td><td>[0,0,0,0,0,0,1,0]</td><td>[0.71,0.12,0.14,0.15]</td></tr><tr><td>dog</td><td>[0,0,0,0,0,0,0,1]</td><td>[0.21,0.93,0.24,0.26]</td></tr></tbody></table><h3 id=create-neural-network-step-5>Create Neural Network (Step 5)<a class=td-heading-self-link href=#create-neural-network-step-5 aria-label="Heading self-link"></a></h3><p>Create a neural network for learning embedding.</p><ul><li>One input layer which can accept token/words. Convert token (context and target words) into onehot encoding</li><li>One embedding layer, for example sake we are taking 4 dimensional embedding of words. These embedding are randomingly intiated number initally (there are other ways also).</li><li>One dense layer of 5 neuron (example)</li><li>Softmax function</li><li>Output layer (to predict the probability of the predicted word. If vocabulary size of the corpus is 10,000 words, then softmax will predict 10,000 probabilities)</li><li>Loss function - Cross entropy loss function. L = $$- \sum_{i=1}^{N} y_{i} \cdot \log(p_{i})$$, N is vocab size.</li><li>4 numbers from embedding will go to each of the 5 neuron, Each neuron will have 4 weights to embedding layer. 5*4 = 20 weights are learned + 5 biases learned</li><li>Learning Rate LR = .0002</li></ul><h3 id=training---forward-propagation-step-678910>Training - Forward propagation (Step 6+7+8+9+10)<a class=td-heading-self-link href=#training---forward-propagation-step-678910 aria-label="Heading self-link"></a></h3><ul><li>Randomly initialize all the weights and biases of the network.</li><li>Pass target and context word to the network.</li></ul><table><thead><tr><th>Step 6</th><th>-</th><th>-</th><th>Step 7</th><th>Step 8</th><th>-</th><th>Step 9</th></tr></thead><tbody><tr><td>Input layer</td><td>Embedding layer</td><td>Hidden layer (5 neuron, random init w&amp;b), dense layer</td><td>matmul between weights and inputs (embedding)</td><td>softmax (8 vocab size)</td><td>actual vector for &ldquo;quick&rdquo;</td><td>cross entropy loss</td></tr><tr><td>The (context), quick (target)</td><td>context (The) = [.11,.12,.14,.15]</td><td>n1=[.11,.12,.13,.14]</td><td>0.0657</td><td>0.1867</td><td>0</td><td>0.0897</td></tr><tr><td>target (quick) = [.21,.23,.24,.26]</td><td>n2=[.13,.14,.15,.16]</td><td>0.0761</td><td>0.1886</td><td>1</td><td>0.7244</td><td></td></tr><tr><td></td><td>n3=[.21..22,.23,.24]</td><td>0.1177</td><td>0.1966</td><td>0</td><td>0.0951</td><td></td></tr><tr><td></td><td>n4=[.32,.33,.34,.35]</td><td>0.1749</td><td>0.2082</td><td>0</td><td>0.1014</td><td></td></tr><tr><td></td><td>n5=[.42,.43,.45,.46]</td><td>0.2298</td><td>0.2199</td><td>0</td><td>0.1079</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td><strong>Step 10</strong></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td>Total Loss</td><td>1.1185</td><td></td></tr></tbody></table><h3 id=training---backward-propagation-step-1112>Training - backward propagation (Step 11+12)<a class=td-heading-self-link href=#training---backward-propagation-step-1112 aria-label="Heading self-link"></a></h3><p>Updating weights of network neurons</p><table><thead><tr><th>Step 11 (Gradient Calculation for 20 weights)</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>dL/dw1</td><td>10.17</td><td>9.32</td><td>8.60</td><td>7.99</td></tr><tr><td>dL/dw2</td><td>8.60</td><td>7.99</td><td>7.46</td><td>6.99</td></tr><tr><td>dL/dw3</td><td>5.33</td><td>5.08</td><td>4.66</td><td>4.66</td></tr><tr><td>dL/dw4</td><td>3.50</td><td>3.39</td><td>3.20</td><td>3.20</td></tr><tr><td>dL/dw5</td><td>2.66</td><td>2.60</td><td>2.43</td><td>2.43</td></tr></tbody></table><table><thead><tr><th>Step 12 Updated Weights</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>new w1</td><td>0.11</td><td>0.12</td><td>0.13</td><td>0.14</td></tr><tr><td>new w2</td><td>0.13</td><td>0.14</td><td>0.15</td><td>0.16</td></tr><tr><td>new w3</td><td>0.21</td><td>0.22</td><td>0.23</td><td>0.24</td></tr><tr><td>new d4</td><td>0.32</td><td>0.33</td><td>0.34</td><td>0.35</td></tr><tr><td>new w5</td><td>0.42</td><td>0.43</td><td>0.45</td><td>0.46</td></tr></tbody></table><p>new weight = old weight - Learning Rate * DL/dW</p><h3 id=update-embedding-step-1314->Update Embedding (Step 13+14 )<a class=td-heading-self-link href=#update-embedding-step-1314- aria-label="Heading self-link"></a></h3><table><thead><tr><th>Old Embedding (Vector)</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>context (The) = [.11,.12,.14,.15]</td><td>0.110</td><td>0.120</td><td>0.140</td><td>0.150</td></tr><tr><td>target (quick) = [.21,.23,.24,.26]</td><td>0.210</td><td>0.230</td><td>0.240</td><td>0.260</td></tr></tbody></table><table><thead><tr><th>Step 13 (Gradient of Old Embedding)</th><th></th><th></th><th></th></tr></thead><tbody><tr><td>dL/context</td><td>10.17</td><td>9.32</td><td>7.99</td></tr><tr><td>dL/target</td><td>5.33</td><td>4.86</td><td>4.66</td></tr></tbody></table><table><thead><tr><th>Step 14 (Updated Embedding)</th><th></th><th></th><th></th></tr></thead><tbody><tr><td>context (The)</td><td>0.108</td><td>0.118</td><td>0.138</td></tr><tr><td>target (quick)</td><td>0.209</td><td>0.229</td><td>0.239</td></tr></tbody></table><h3 id=complete-the-training>Complete the Training<a class=td-heading-self-link href=#complete-the-training aria-label="Heading self-link"></a></h3><ul><li>Perform training forward and backword propagation in batch, multiple words at a time.</li><li>Everytime update w&amp;b and also update embedding.</li><li>Trained embedding can be used in future without these training steps.</li><li>Let entire dataset of paired words go through this network. One it goes through it is called one epoch.</li><li>Let embedding get updated over multiple epoch say 50 or 100. More epoch, will cause better embedding. It will cost more money.</li><li>More dimentional vector will have better represenation but will cost more computation and more money.</li></ul><h2 id=other-methods-of-embedding>Other Methods of Embedding<a class=td-heading-self-link href=#other-methods-of-embedding aria-label="Heading self-link"></a></h2><h3 id=tf-idf>TF-IDF<a class=td-heading-self-link href=#tf-idf aria-label="Heading self-link"></a></h3><p>TF-IDF - Term Frequency - Inverse Document Frequency, is an old, traditional, frequency based text embedding technique. It is not based on neural network architecture therefore does not need expensive hardware to create these embedding and use TF-IDF embedding. Like skipgram or CBOW it is not vector based but frequency based, therefore understandign symantic of the text is not possible with TF-IDF. There is no use of pretrained embedding, everytime we have a corpus we need to create embedding for that and it is used only for that. We cannot use TF-IDF embedding, which was created using news text for something else, say history or enterainment. Thus, embedding transfer is meaninless but task transfer can be done. It means TF-IDF embedding which is used for classficittion purpose can be used for other task like topic modelling, sentiment analysis etc. Obviously there is a limit, we cannot use it for other task like translation or summarization.</p><h4 id=how-tf-idf-works>How TF-IDF works?<a class=td-heading-self-link href=#how-tf-idf-works aria-label="Heading self-link"></a></h4><ul><li>Term frequency (TF): The number of times a word appears in a document.</li><li>Inverse document frequency (IDF): The logarithm of the number of documents in the collection divided by the number of documents that contain the word.</li><li>The TF-IDF score for a word in a document is calculated as follows:</li><li>TF-IDF = TF * IDF (The higher the TF-IDF score, the more important the word is to the document.)</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>Document_1</span><span class=p>:</span> <span class=s2>&#34;The quick brown fox jumps over the lazy dog.&#34;</span>
</span></span><span class=line><span class=cl><span class=n>Document_2</span><span class=p>:</span> <span class=s2>&#34;The dog is lazy, but the fox is quick.&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Term frequency for the word &#34;quick&#34; in Document 1</span>
</span></span><span class=line><span class=cl><span class=n>TF</span><span class=p>(</span><span class=n>quick</span><span class=p>,</span> <span class=n>Document_1</span><span class=p>)</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Inverse document frequency for the word &#34;quick&#34;</span>
</span></span><span class=line><span class=cl><span class=n>IDF</span><span class=p>(</span><span class=n>quick</span><span class=p>)</span> <span class=o>=</span> <span class=n>log</span><span class=p>(</span><span class=mi>2</span> <span class=o>/</span> <span class=mi>1</span><span class=p>)</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># TF-IDF score for the word &#34;quick&#34; in Document 1</span>
</span></span><span class=line><span class=cl><span class=n>TF</span><span class=o>-</span><span class=n>IDF</span><span class=p>(</span><span class=n>quick</span><span class=p>,</span> <span class=n>Document_1</span><span class=p>)</span> <span class=o>=</span> <span class=n>TF</span><span class=p>(</span><span class=n>quick</span><span class=p>,</span> <span class=n>Document_1</span><span class=p>)</span> <span class=o>*</span> <span class=n>IDF</span><span class=p>(</span><span class=n>quick</span><span class=p>)</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>*</span> <span class=mi>0</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Term frequency for the word &#34;quick&#34; in Document 2</span>
</span></span><span class=line><span class=cl><span class=n>TF</span><span class=p>(</span><span class=n>quick</span><span class=p>,</span> <span class=n>Document_2</span><span class=p>)</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Inverse document frequency for the word &#34;quick&#34;</span>
</span></span><span class=line><span class=cl><span class=n>IDF</span><span class=p>(</span><span class=n>quick</span><span class=p>)</span> <span class=o>=</span> <span class=n>log</span><span class=p>(</span><span class=mi>2</span> <span class=o>/</span> <span class=mi>1</span><span class=p>)</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># TF-IDF score for the word &#34;quick&#34; in Document 2</span>
</span></span><span class=line><span class=cl><span class=n>TF</span><span class=o>-</span><span class=n>IDF</span><span class=p>(</span><span class=n>quick</span><span class=p>,</span> <span class=n>Document_2</span><span class=p>)</span> <span class=o>=</span> <span class=n>TF</span><span class=p>(</span><span class=n>quick</span><span class=p>,</span> <span class=n>Document_2</span><span class=p>)</span> <span class=o>*</span> <span class=n>IDF</span><span class=p>(</span><span class=n>quick</span><span class=p>)</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>*</span> <span class=mi>0</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Term frequency for the word &#34;lazy&#34; in Document 1</span>
</span></span><span class=line><span class=cl><span class=n>TF</span><span class=p>(</span><span class=n>lazy</span><span class=p>,</span> <span class=n>Document_1</span><span class=p>)</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Inverse document frequency for the word &#34;lazy&#34;</span>
</span></span><span class=line><span class=cl><span class=n>IDF</span><span class=p>(</span><span class=n>lazy</span><span class=p>)</span> <span class=o>=</span> <span class=n>log</span><span class=p>(</span><span class=mi>2</span> <span class=o>/</span> <span class=mi>1</span><span class=p>)</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># TF-IDF score for the word &#34;lazy&#34; in Document 1</span>
</span></span><span class=line><span class=cl><span class=n>TF</span><span class=o>-</span><span class=n>IDF</span><span class=p>(</span><span class=n>lazy</span><span class=p>,</span> <span class=n>Document_1</span><span class=p>)</span> <span class=o>=</span> <span class=n>TF</span><span class=p>(</span><span class=n>lazy</span><span class=p>,</span> <span class=n>Document_1</span><span class=p>)</span> <span class=o>*</span> <span class=n>IDF</span><span class=p>(</span><span class=n>lazy</span><span class=p>)</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>*</span> <span class=mi>0</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Term frequency for the word &#34;lazy&#34; in Document 2</span>
</span></span><span class=line><span class=cl><span class=n>TF</span><span class=p>(</span><span class=n>lazy</span><span class=p>,</span> <span class=n>Document_2</span><span class=p>)</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Inverse document frequency for the word &#34;lazy&#34;</span>
</span></span><span class=line><span class=cl><span class=n>IDF</span><span class=p>(</span><span class=n>lazy</span><span class=p>)</span> <span class=o>=</span> <span class=n>log</span><span class=p>(</span><span class=mi>2</span> <span class=o>/</span> <span class=mi>1</span><span class=p>)</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># TF-IDF score for the word &#34;lazy&#34; in Document 2</span>
</span></span><span class=line><span class=cl><span class=n>TF</span><span class=o>-</span><span class=n>IDF</span><span class=p>(</span><span class=n>lazy</span><span class=p>,</span> <span class=n>Document_2</span><span class=p>)</span> <span class=o>=</span> <span class=n>TF</span><span class=p>(</span><span class=n>lazy</span><span class=p>,</span> <span class=n>Document_2</span><span class=p>)</span> <span class=o>*</span> <span class=n>IDF</span><span class=p>(</span><span class=n>lazy</span><span class=p>)</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>*</span> <span class=mi>0</span> <span class=o>=</span> <span class=mi>0</span>
</span></span></code></pre></div><h3 id=glove-global-vectors>GloVe (Global Vectors)<a class=td-heading-self-link href=#glove-global-vectors aria-label="Heading self-link"></a></h3><p>GloVe is a method that learns word embeddings from global word-word co-occurrence statistics. It is similar to Skipgram and CBOW, but it is better at capturing long-range semantic relationships between words. GloVe embedding is good for text classification, and machine translation (MT).</p><h4 id=how-glove-embedding-works>How GloVe embedding works?<a class=td-heading-self-link href=#how-glove-embedding-works aria-label="Heading self-link"></a></h4><ul><li>Tokenize the corpus: Split the corpus into individual words and punctuation marks.</li><li>Count word co-occurrences: For each word in the vocabulary, count how many times it co-occurs with other words in a given window size.</li><li>Build a word-word co-occurrence matrix: The word-word co-occurrence matrix is a square matrix, where each row and column represents a word in the vocabulary. The value at each cell in the matrix represents the number of times the two corresponding words co-occur in the corpus.</li><li>Factorize the word-word co-occurrence matrix: Factorize the word-word co-occurrence matrix into two lower-dimensional matrices, one for <strong>word embeddings</strong> (relationship between words) and one for <strong>context embeddings</strong> (relationship between words in the context). We can factorize the word-word co-occurrence matrix using a variety of matrix factorization techniques, such as singular value decomposition (SVD) or nonnegative matrix factorization (NMF).</li><li>Normalize the word embeddings: Normalize the word embeddings so that they have a unit length. We can normalize the word embeddings by dividing each embedding by its L2 norm. This will ensure that all of the embeddings have a unit length.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>gensim.models</span> <span class=kn>import</span> <span class=n>KeyedVectors</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Load the corpus</span>
</span></span><span class=line><span class=cl><span class=n>corpus</span> <span class=o>=</span> <span class=nb>open</span><span class=p>(</span><span class=s2>&#34;corpus.txt&#34;</span><span class=p>,</span> <span class=s2>&#34;r&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>read</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Tokenize the corpus</span>
</span></span><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=n>corpus</span><span class=o>.</span><span class=n>split</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Count word co-occurrences</span>
</span></span><span class=line><span class=cl><span class=n>word_co_occurrences</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>tokens</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>!=</span> <span class=n>tokens</span><span class=p>[</span><span class=n>j</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>            <span class=n>word_co_occurrences</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=n>tokens</span><span class=o>.</span><span class=n>count</span><span class=p>(</span><span class=n>tokens</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=s2>&#34; &#34;</span> <span class=o>+</span> <span class=n>tokens</span><span class=p>[</span><span class=n>j</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Factorize the word-word co-occurrence matrix</span>
</span></span><span class=line><span class=cl><span class=n>glove_model</span> <span class=o>=</span> <span class=n>KeyedVectors</span><span class=p>(</span><span class=n>word_vectors</span><span class=o>=</span><span class=n>word_co_occurrences</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Save the word embeddings</span>
</span></span><span class=line><span class=cl><span class=n>glove_model</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>&#34;glove_embeddings.txt&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>How SVD (Singular Value Decomposition) works?</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create a word-word co-occurrence matrix</span>
</span></span><span class=line><span class=cl><span class=n>word_co_occurrences</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Perform SVD</span>
</span></span><span class=line><span class=cl><span class=n>U</span><span class=p>,</span> <span class=n>S</span><span class=p>,</span> <span class=n>Vh</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>svd</span><span class=p>(</span><span class=n>word_co_occurrences</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Truncate the singular values</span>
</span></span><span class=line><span class=cl><span class=n>S_truncated</span> <span class=o>=</span> <span class=n>S</span><span class=p>[:</span><span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Reconstruct the word-word co-occurrence matrix</span>
</span></span><span class=line><span class=cl><span class=n>word_co_occurrences_reconstructed</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>U</span><span class=p>[:,</span> <span class=p>:</span><span class=mi>2</span><span class=p>],</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>S_truncated</span><span class=p>,</span> <span class=n>Vh</span><span class=p>[:,</span> <span class=p>:</span><span class=mi>2</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Print the reconstructed word-word co-occurrence matrix</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>word_co_occurrences_reconstructed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Results</span>
</span></span><span class=line><span class=cl><span class=p>[[</span><span class=o>-</span><span class=mf>0.50578521</span> <span class=o>-</span><span class=mf>0.25523155</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=o>-</span><span class=mf>0.58437383</span> <span class=o>-</span><span class=mf>0.60130182</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=o>-</span><span class=mf>0.63457746</span>  <span class=mf>0.75716113</span><span class=p>]]</span>
</span></span><span class=line><span class=cl><span class=p>[[</span><span class=o>-</span><span class=mf>0.50578521</span> <span class=o>-</span><span class=mf>0.58437383</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span> <span class=mf>0.25523155</span>  <span class=mf>0.60130182</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span> <span class=mf>0.82403773</span> <span class=o>-</span><span class=mf>0.54492509</span><span class=p>]]</span>
</span></span></code></pre></div><h3 id=bert-bidirectional-encoder-representations-from-transformers>BERT (Bidirectional Encoder Representations from Transformers)<a class=td-heading-self-link href=#bert-bidirectional-encoder-representations-from-transformers aria-label="Heading self-link"></a></h3><p>BERT is a transformer-based language model that can learn word embeddings from unlabeled text, we need not to create skipgram pairs. BERT embeddings are particularly good at capturing <strong>contextual information</strong>. BERT embedding is good for MT, QA, Classification tasks.</p><h4 id=how-bert-does-embedding>How BERT does embedding?<a class=td-heading-self-link href=#how-bert-does-embedding aria-label="Heading self-link"></a></h4><ul><li>Tokenization: The first step is to tokenize the sentence into words. This means splitting the sentence into individual words, including punctuation marks. The tokenized sentence is then represented as a sequence of integers (we create ids), where each integer represents a word in the vocabulary.</li><li>Word embedding lookup: BERT uses a pre-trained word embedding table to convert each word in the sequence into a vector of numbers. This vector represents the meaning of the word in a distributed manner.</li><li>Segment embedding lookup: BERT also uses a segment embedding table to encode the position of each word in the sentence. This is necessary because BERT is a bidirectional language model, and it needs to know the context of each word in order to learn meaningful embeddings.</li><li>Positional embedding lookup: BERT also uses a positional embedding table to encode the absolute position of each word in the sentence. This is necessary because BERT needs to know the order of the words in the sentence in order to learn meaningful embeddings.</li><li>Transformer encoding: The encoded sequence of word embeddings, segment embeddings, and positional embeddings is then passed to the transformer encoder. The transformer encoder is a neural network architecture that learns long-range dependencies between words in a sentence.</li><li>Output embedding: The output of the transformer encoder is a sequence of vectors, where each vector represents the embedding of the corresponding word in the sentence. These embeddings are then used for downstream natural language processing tasks, such as machine translation, text classification, and question answering.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Tokenize the sentence</span>
</span></span><span class=line><span class=cl><span class=n>sentence</span> <span class=o>=</span> <span class=s2>&#34;The quick brown fox jump over the lazy fox&#34;</span>
</span></span><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=n>sentence</span><span class=o>.</span><span class=n>split</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Convert each word to a word embedding vector</span>
</span></span><span class=line><span class=cl><span class=n>word_embeddings</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>token</span> <span class=ow>in</span> <span class=n>tokens</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>word_embeddings</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>bert_model</span><span class=o>.</span><span class=n>get_word_embedding</span><span class=p>(</span><span class=n>token</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create segment embeddings</span>
</span></span><span class=line><span class=cl><span class=n>segment_embeddings</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span> <span class=o>//</span> <span class=mi>2</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>segment_embeddings</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>bert_model</span><span class=o>.</span><span class=n>get_segment_embedding</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>segment_embeddings</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>bert_model</span><span class=o>.</span><span class=n>get_segment_embedding</span><span class=p>(</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create positional embeddings</span>
</span></span><span class=line><span class=cl><span class=n>positional_embeddings</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>    <span class=n>positional_embeddings</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>bert_model</span><span class=o>.</span><span class=n>get_positional_embedding</span><span class=p>(</span><span class=n>i</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Encode the sentence</span>
</span></span><span class=line><span class=cl><span class=n>encoded_sentence</span> <span class=o>=</span> <span class=n>bert_model</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>word_embeddings</span><span class=p>,</span> <span class=n>segment_embeddings</span><span class=p>,</span> <span class=n>positional_embeddings</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Output embeddings</span>
</span></span><span class=line><span class=cl><span class=n>output_embeddings</span> <span class=o>=</span> <span class=n>encoded_sentence</span>
</span></span></code></pre></div><h3 id=fasttext-fast-text>FastText (Fast Text)<a class=td-heading-self-link href=#fasttext-fast-text aria-label="Heading self-link"></a></h3><p>FastText is a modification of Skipgram that can learn embeddings for words and subwords. This makes it better at representing rare words and out-of-vocabulary words. FastText is good for name-entity-recognition (NER) & Question Answering (QA) tasks.</p><h3 id=elmo-embeddings-from-language-models>ELMo (Embeddings from Language Models)<a class=td-heading-self-link href=#elmo-embeddings-from-language-models aria-label="Heading self-link"></a></h3><p>ELMo (Embeddings from Language Models) is a deep contextual word embedding technique that uses a bidirectional language model (biLM) to learn word representations. A biLM is a type of neural network that can learn to predict the next word in a sentence, as well as the previous word. Unlike skipgram, which predicts next words, biLM is bidirectional. From a target word biLM can predict next and previous words.</p><h1 id=resources>Resources<a class=td-heading-self-link href=#resources aria-label="Heading self-link"></a></h1><p>If you want to understand all skipgram/cbow caluclation with excel and then you can use this <a href="https://docs.google.com/spreadsheets/d/1eU4EVtUzD1w_ILcpJVTc6oK2KH9vEDK7OuXFtyv1_gU/edit?usp=sharing">calculation sheet</a></p><div class=category-section><h4 class=category-section__title>Categories:</h4><div class=category-badges><a href=https://localhost:1313/categories/dsblog class=category-badge>dsblog</a></div></div><div class=td-tags><h4 class=td-tags__title>Tags:</h4><div class=category-badges><a href=https://localhost:1313/tags/word-embedding class=category-badge>Word Embedding</a><a href=https://localhost:1313/tags/nlp class=category-badge>NLP</a><a href=https://localhost:1313/tags/vector-representation class=category-badge>Vector Representation</a><a href=https://localhost:1313/tags/text-processing class=category-badge>Text Processing</a><a href=https://localhost:1313/tags/machine-learning class=category-badge>Machine Learning</a><a href=https://localhost:1313/tags/neural-networks class=category-badge>Neural Networks</a><a href=https://localhost:1313/tags/language-models class=category-badge>Language Models</a></div></div><div class=td-author-box><div class=td-author-box__info><h4 class=td-author-box__name>Blowfish</h4><p class=td-author-box__bio>A powerful, lightweight theme for Hugo.</p><div class=td-author-box__links><a href target=_blank rel=noopener aria-label></a><a href target=_blank rel=noopener aria-label></a></div></div></div><div class=td-social-share><h4 class=td-social-share__title>Share this article:</h4><ul class=td-social-share__list><div class=social-share><a href="https://twitter.com/intent/tweet?text=Basics%20of%20Word%20Embedding&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fbasics-of-word-embedding%2f" target=_blank rel=noopener aria-label="Share on Twitter"><i class="fab fa-twitter"></i>
</a><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fbasics-of-word-embedding%2f" target=_blank rel=noopener aria-label="Share on Facebook"><i class="fab fa-facebook"></i>
</a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fbasics-of-word-embedding%2f&title=Basics%20of%20Word%20Embedding" target=_blank rel=noopener aria-label="Share on LinkedIn"><i class="fab fa-linkedin"></i>
</a><a href="https://www.reddit.com/submit?url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fbasics-of-word-embedding%2f&title=Basics%20of%20Word%20Embedding" target=_blank rel=noopener aria-label="Share on Reddit"><i class="fab fa-reddit"></i>
</a><a href="mailto:?subject=Basics%20of%20Word%20Embedding&body=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fbasics-of-word-embedding%2f" aria-label="Share via Email"><i class="fas fa-envelope"></i></a></div></ul></div><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href=https://localhost:1313/dsblog/graph-of-thoughts/ aria-label="Previous page"><div class=td-pager__meta><i class="fa-solid fa-angle-left"></i>
<span class=td-pager__meta-label><b>Previous:</b></span>
<span class=td-pager__meta-title>Graph of Thoughts</span></div></a><a class="td-pager__link td-pager__link--next" href=https://localhost:1313/dsblog/topic-modeling-with-bert/ aria-label="Next page"><div class=td-pager__meta><span class=td-pager__meta-label><b>Next:</b></span>
<span class=td-pager__meta-title>Topic Modeling with BERT</span>
<i class="fa-solid fa-angle-right"></i></div></a></ul></div><div class=col-md-2></div></div></main><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class=col-2><a href=https://dasarpai.com target=_blank rel=noopener><img src=https://localhost:1313/assets/images/site-logo.png alt=dasarpAI width=100 style=border-radius:12px></a></div><div class=col-8><div class=row></div><div class=row></div></div><div class=col-2></div><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"></div></div></div></footer></div><script src=https://localhost:1313/js/main.min.7c31f98e62c36b0c9c834ce3f8260a0e21895dd6aa0773e81a64b104eae3b2e8.js integrity="sha256-fDH5jmLDawycg0zj+CYKDiGJXdaqB3PoGmSxBOrjsug=" crossorigin=anonymous></script><script defer src=https://localhost:1313/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=https://localhost:1313/js/tabpane-persist.js></script><script src=https://localhost:1313/js/asciinema-player.js></script><script>(function(){var e=document.querySelector("#td-section-nav");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></body></html>