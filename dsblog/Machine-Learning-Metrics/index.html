<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.147.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href="../../favicons/favicon.ico?v=1"><link rel=apple-touch-icon href="../../favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><link rel=icon type=image/png href="../../favicons/favicon-16x16.png?v=1" sizes=16x16><link rel=icon type=image/png href="../../favicons/favicon-32x32.png?v=1" sizes=32x32><link rel=apple-touch-icon href="../../favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><title>Machine Learning Metrics | Blowfish</title><meta property="og:url" content="https://dasarpai.github.io/dsblog/Machine-Learning-Metrics/">
<meta property="og:site_name" content="Blowfish"><meta property="og:title" content="Machine Learning Metrics"><meta property="og:description" content="Machine Learning Metrics Introduction In Machine Learning projects whether classical machine learning, deep learning, computer vision, speech processing, NLP, or any other ML project we keep building different models with different datasets. But how to know that for a particular problem model X is the best one? For that, we need to evaluate these models against certain metrics. What metrics we pick, depends upon the problem statement, data imbalance, type of data, etc. In this article, we will explore an exhaustive list of ML Metrics."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="dsblog"><meta property="article:published_time" content="2023-08-21T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-21T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Model Evaluation"><meta property="article:tag" content="Performance Metrics"><meta property="article:tag" content="Data Science"><meta property="article:tag" content="ML Metrics"><meta property="article:tag" content="Model Assessment"><meta itemprop=name content="Machine Learning Metrics"><meta itemprop=description content="Machine Learning Metrics Introduction In Machine Learning projects whether classical machine learning, deep learning, computer vision, speech processing, NLP, or any other ML project we keep building different models with different datasets. But how to know that for a particular problem model X is the best one? For that, we need to evaluate these models against certain metrics. What metrics we pick, depends upon the problem statement, data imbalance, type of data, etc. In this article, we will explore an exhaustive list of ML Metrics."><meta itemprop=datePublished content="2023-08-21T00:00:00+00:00"><meta itemprop=dateModified content="2023-08-21T00:00:00+00:00"><meta itemprop=wordCount content="3954"><meta itemprop=keywords content="Machine Learning Metrics,Model Evaluation,Performance Measurement,ML Model Assessment,Evaluation Metrics,Model Performance,Statistical Metrics,ML Evaluation"><meta name=twitter:card content="summary"><meta name=twitter:title content="Machine Learning Metrics"><meta name=twitter:description content="Machine Learning Metrics Introduction In Machine Learning projects whether classical machine learning, deep learning, computer vision, speech processing, NLP, or any other ML project we keep building different models with different datasets. But how to know that for a particular problem model X is the best one? For that, we need to evaluate these models against certain metrics. What metrics we pick, depends upon the problem statement, data imbalance, type of data, etc. In this article, we will explore an exhaustive list of ML Metrics."><link rel=preload href=../../scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css as=style integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link href=../../scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css rel=stylesheet integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link rel=stylesheet type=text/css href=https://dasarpai.github.io/css/asciinema-player.css><script src=https://code.jquery.com/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><link rel=stylesheet href=../../css/custom.css><script src=../../js/lunr.js></script></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-light nav-shadow flex-column flex-md-row td-navbar"><a id=agones-top class=navbar-brand href=../../><svg id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg> <span class="text-uppercase fw-bold">Blowfish</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=../../><span class=active>Data Science</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/docs/><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/docs/><span>Docs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../community/><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/docs/shortcodes/><span>Short Codes</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/blog/><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://github.com/nunocoracao/blowfish><span></span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href>GitHub</a></li><li class="nav-item dropdown d-none d-lg-block"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Release</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://development.agones.dev>Development</a>
<a class=dropdown-item href=https://agones.dev></a><a class=dropdown-item href=https://1-47-0.agones.dev>1.47.0</a>
<a class=dropdown-item href=https://1-46-0.agones.dev>1.46.0</a>
<a class=dropdown-item href=https://1-45-0.agones.dev>1.45.0</a>
<a class=dropdown-item href=https://1-44-0.agones.dev>1.44.0</a>
<a class=dropdown-item href=https://1-43-0.agones.dev>1.43.0</a>
<a class=dropdown-item href=https://1-42-0.agones.dev>1.42.0</a>
<a class=dropdown-item href=https://1-41-0.agones.dev>1.41.0</a>
<a class=dropdown-item href=https://1-40-0.agones.dev>1.40.0</a>
<a class=dropdown-item href=https://1-39-0.agones.dev>1.39.0</a>
<a class=dropdown-item href=https://1-38-0.agones.dev>1.38.0</a>
<a class=dropdown-item href=https://1-37-0.agones.dev>1.37.0</a>
<a class=dropdown-item href=https://1-36-0.agones.dev>1.36.0</a>
<a class=dropdown-item href=https://1-35-0.agones.dev>1.35.0</a>
<a class=dropdown-item href=https://1-34-0.agones.dev>1.34.0</a>
<a class=dropdown-item href=https://1-33-0.agones.dev>1.33.0</a>
<a class=dropdown-item href=https://1-32-0.agones.dev>1.32.0</a>
<a class=dropdown-item href=https://1-31-0.agones.dev>1.31.0</a></div></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><ul class=dropdown-menu><li><a class=dropdown-item href=../../it/>Italiano</a></li></ul></div></li></ul></div><div class="navbar-nav mx-lg-2 d-none d-lg-block"></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><div class=row><div class=col-md-2></div><div class=col-md-8><p><img src=../../assets/images/dspost/dsp6092-Machine-Learning-Metrics.jpg alt="Comprehensive Glossary of LLM"></p><h1 id=machine-learning-metrics>Machine Learning Metrics<a class=td-heading-self-link href=#machine-learning-metrics aria-label="Heading self-link"></a></h1><h2 id=introduction>Introduction<a class=td-heading-self-link href=#introduction aria-label="Heading self-link"></a></h2><p>In Machine Learning projects whether classical machine learning, deep learning, computer vision, speech processing, NLP, or any other ML project we keep building different models with different datasets. But how to know that for a particular problem model X is the best one? For that, we need to evaluate these models against certain metrics. What metrics we pick, depends upon the problem statement, data imbalance, type of data, etc. In this article, we will explore an exhaustive list of ML Metrics.</p><p>From various sources, benchmarking platforms, and research papers, I have noted 330+ metrics for evaluating Machine Learning models. I keep working on this page, expanding and defining these terms on a regular basis. Some of these metrics go over my head but I am keeping it here on this page because they have been used by the practitioners in some of their projects. After experimenting with those I will expand them here. Some of the metrics are obvious to Data Scientists. But, even if you are new to Data science it gives you an idea about these metrics.</p><hr><h2 id=mrr>MRR<a class=td-heading-self-link href=#mrr aria-label="Heading self-link"></a></h2><p>MRR stands for &ldquo;Mean Reciprocal Rank,&rdquo; and it is a metric commonly used in information retrieval and evaluation tasks, including those in natural language processing (NLP). MRR is used to assess the effectiveness of ranking algorithms or systems in presenting relevant information to users. MRR is often applied to tasks such as question answering, search engines, and recommendation systems. A higher MRR indicates that relevant results tend to appear higher in the ranked lists, which suggests better user experience.</p><p>Step1: Ranking of Results: Imagine you have a system that retrieves a list of possible answers or documents in response to a user&rsquo;s query or question. These results are usually ranked based on their perceived relevance to the query.</p><p>Step2: Reciprocal Rank: For each query or question, the reciprocal rank of the first correct (relevant) result in the ranked list is calculated as 1 divided by the position of that correct result. If the correct result is the second item, the reciprocal rank would be 1/2; if it&rsquo;s the fifth, the reciprocal rank would be 1/5, and so on.</p><p>Step3: Mean Reciprocal Rank (MRR): To calculate the MRR, you take the average of the reciprocal ranks across all queries or questions in your evaluation dataset. The formula for MRR is:</p><p>MRR = (1/N) * ∑(1/rank_i)</p><p>N is the total number of queries or questions.<br>rank_i is the position of the correct result for the i-th query.</p><p>Commonly used MRR@n metrics are MRR@1, MRR@3, MRR@5, MRR@10, MRR@100, MRR@1000</p><hr><h2 id=mapn>MAP@n<a class=td-heading-self-link href=#mapn aria-label="Heading self-link"></a></h2><p>MAP stands for &ldquo;Mean Average Precision&rdquo;. MAP focuses on evaluating the effectiveness of ranking algorithms or systems</p><p>Step1: Ranking of Results: You have a system that retrieves a ranked list of possible answers or documents in response to a user&rsquo;s query or question.</p><p>Step2: Precision and Recall: Precision is the ratio of relevant items retrieved to the total number of items retrieved. Recall is the ratio of relevant items retrieved to the total number of relevant items in the dataset. These two metrics are often in tension with each other; increasing precision might result in lower recall and vice versa.</p><p>Step3: Average Precision (AP) for Each Query: For each query or question, you calculate the precision at each position in the ranked list where a relevant item is retrieved. You then calculate the average of these precision values, resulting in the Average Precision (AP) for that query.</p><p>Step4: Mean Average Precision (MAP): To calculate the MAP, you take the average of the Average Precision values across all queries or questions in your evaluation dataset. The formula for MAP is:</p><p>MAP = (1/N) * ∑(AP_i)</p><p>N is the total number of queries or questions.<br>AP_i is the Average Precision for the i-th query.</p><p>Commonly used MAP@n metrics are map@1, map@3, map@5, map@10, map@100, map@1000</p><h2 id=mean-average-precision-map>Mean average precision (mAP):<a class=td-heading-self-link href=#mean-average-precision-map aria-label="Heading self-link"></a></h2><p>A measure of the average precision over a range of IoU thresholds. It is also used in object detection and segmentation tasks.</p><h2 id=mapiou-thresholds>mAP@IoU thresholds<a class=td-heading-self-link href=#mapiou-thresholds aria-label="Heading self-link"></a></h2><p>The mean average precision over a range of intersection over union (IoU) thresholds.</p><hr><h2 id=accuracy>Accuracy:<a class=td-heading-self-link href=#accuracy aria-label="Heading self-link"></a></h2><p>The percentage of test samples that are correctly classified.</p><h3 id=acc_norm>acc_norm<a class=td-heading-self-link href=#acc_norm aria-label="Heading self-link"></a></h3><h3 id=accuracy_cardiffnlptweet_topic_multi>accuracy_cardiffnlp/tweet_topic_multi<a class=td-heading-self-link href=#accuracy_cardiffnlptweet_topic_multi aria-label="Heading self-link"></a></h3><h3 id=accuracy_cardiffnlptweet_topic_single>accuracy_cardiffnlp/tweet_topic_single<a class=td-heading-self-link href=#accuracy_cardiffnlptweet_topic_single aria-label="Heading self-link"></a></h3><h3 id=accuracy_cosinus>accuracy_cosinus<a class=td-heading-self-link href=#accuracy_cosinus aria-label="Heading self-link"></a></h3><h3 id=accuracy_euclidean>accuracy_euclidean<a class=td-heading-self-link href=#accuracy_euclidean aria-label="Heading self-link"></a></h3><h3 id=accuracy_manhattan>accuracy_manhattan<a class=td-heading-self-link href=#accuracy_manhattan aria-label="Heading self-link"></a></h3><h3 id=accuracy_tweet_evalemoji>accuracy_tweet_eval/emoji<a class=td-heading-self-link href=#accuracy_tweet_evalemoji aria-label="Heading self-link"></a></h3><h3 id=accuracy_tweet_evalemotion>accuracy_tweet_eval/emotion<a class=td-heading-self-link href=#accuracy_tweet_evalemotion aria-label="Heading self-link"></a></h3><h3 id=accuracy_tweet_evalhate>accuracy_tweet_eval/hate<a class=td-heading-self-link href=#accuracy_tweet_evalhate aria-label="Heading self-link"></a></h3><h3 id=accuracy_tweet_evalirony>accuracy_tweet_eval/irony<a class=td-heading-self-link href=#accuracy_tweet_evalirony aria-label="Heading self-link"></a></h3><h3 id=accuracy_tweet_evaloffensive>accuracy_tweet_eval/offensive<a class=td-heading-self-link href=#accuracy_tweet_evaloffensive aria-label="Heading self-link"></a></h3><h3 id=accuracy_tweet_evalsentiment>accuracy_tweet_eval/sentiment<a class=td-heading-self-link href=#accuracy_tweet_evalsentiment aria-label="Heading self-link"></a></h3><hr><h2 id=precision>Precision:<a class=td-heading-self-link href=#precision aria-label="Heading self-link"></a></h2><p>The fraction of predicted positive samples that are actually positive.</p><h3 id=precision_entity_span>precision_entity_span<a class=td-heading-self-link href=#precision_entity_span aria-label="Heading self-link"></a></h3><h3 id=precisionn>Precision@n<a class=td-heading-self-link href=#precisionn aria-label="Heading self-link"></a></h3><p>Commonly used precision@n metrics are Recall@1, Precision@3, Precision@5, Precision@10, Precision@100, Precision@1000</p><h3 id=precision_macro>precision_macro<a class=td-heading-self-link href=#precision_macro aria-label="Heading self-link"></a></h3><hr><h2 id=recall>Recall:<a class=td-heading-self-link href=#recall aria-label="Heading self-link"></a></h2><p>The fraction of actual positive samples that are predicted positive.</p><h3 id=recall_entity_span>recall_entity_span<a class=td-heading-self-link href=#recall_entity_span aria-label="Heading self-link"></a></h3><h3 id=recalln>recall@n<a class=td-heading-self-link href=#recalln aria-label="Heading self-link"></a></h3><p>Commonly used recall@n metrics are recall@1, recall@3, recall@5, recall@10, recall@100, recall@1000</p><h3 id=recall_macro>recall_macro<a class=td-heading-self-link href=#recall_macro aria-label="Heading self-link"></a></h3><hr><h2 id=rp>R@P<a class=td-heading-self-link href=#rp aria-label="Heading self-link"></a></h2><p>&ldquo;R@P&rdquo; stands for &ldquo;Recall at Precision.&rdquo; It&rsquo;s a metric used to evaluate the performance of information retrieval systems, such as search engines or question-answering models, in terms of their ability to retrieve relevant documents or answers.</p><p>R@P is a combination of recall and precision. It evaluates how well a system can maintain a specified precision level while retrieving relevant items. It&rsquo;s typically expressed as &ldquo;R@X,&rdquo; where &ldquo;X&rdquo; represents the desired precision level.</p><p>For example, R@5 measures the recall when precision is equal to 0.5 (50%). It calculates the percentage of relevant items retrieved when the system is limited to a precision level of 50%.</p><p>Mathematically, R@X is calculated by finding the recall at the point where precision reaches the specified level X. This is done by examining the retrieved items in descending order of relevance until the desired precision level is achieved.</p><p>R@P is a useful metric in tasks where both recall (finding all relevant items) and precision (avoiding irrelevant items) are important. It provides insights into how well a system balances these two aspects when retrieving information.</p><h3 id=rp50>R@P=50<a class=td-heading-self-link href=#rp50 aria-label="Heading self-link"></a></h3><h3 id=rp75>R@P=75<a class=td-heading-self-link href=#rp75 aria-label="Heading self-link"></a></h3><h3 id=rp90>R@P=90<a class=td-heading-self-link href=#rp90 aria-label="Heading self-link"></a></h3><hr><h2 id=f1-score>F1 score:<a class=td-heading-self-link href=#f1-score aria-label="Heading self-link"></a></h2><p>The harmonic mean of precision and recall.</p><h3 id=f1_entity_span>f1_entity_span<a class=td-heading-self-link href=#f1_entity_span aria-label="Heading self-link"></a></h3><h3 id=f1_macro>f1_macro<a class=td-heading-self-link href=#f1_macro aria-label="Heading self-link"></a></h3><h3 id=f1_micro>f1_micro<a class=td-heading-self-link href=#f1_micro aria-label="Heading self-link"></a></h3><h3 id=f1_weighted>f1_weighted<a class=td-heading-self-link href=#f1_weighted aria-label="Heading self-link"></a></h3><h3 id=f1-seqeval>f1 (seqeval)<a class=td-heading-self-link href=#f1-seqeval aria-label="Heading self-link"></a></h3><h3 id=f1-macro>f1 macro<a class=td-heading-self-link href=#f1-macro aria-label="Heading self-link"></a></h3><h3 id=f1m>f1@m<a class=td-heading-self-link href=#f1m aria-label="Heading self-link"></a></h3><h3 id=f1m-absent>f1@m (absent)<a class=td-heading-self-link href=#f1m-absent aria-label="Heading self-link"></a></h3><h3 id=f1m-present>f1@m (present)<a class=td-heading-self-link href=#f1m-present aria-label="Heading self-link"></a></h3><h3 id=f1o-absent>f1@o (absent)<a class=td-heading-self-link href=#f1o-absent aria-label="Heading self-link"></a></h3><h3 id=f1o-present>f1@o (present)<a class=td-heading-self-link href=#f1o-present aria-label="Heading self-link"></a></h3><h3 id=f1neg>f1neg<a class=td-heading-self-link href=#f1neg aria-label="Heading self-link"></a></h3><h3 id=f1pos>f1pos<a class=td-heading-self-link href=#f1pos aria-label="Heading self-link"></a></h3><hr><h2 id=hitn>Hit@n<a class=td-heading-self-link href=#hitn aria-label="Heading self-link"></a></h2><h3 id=hit1>Hit@1:<a class=td-heading-self-link href=#hit1 aria-label="Heading self-link"></a></h3><ul><li>The percentage of queries for which the correct document is ranked first.</li><li>The percentage of images for which the correct class is in the top k predictions. It is also used in image classification tasks.</li></ul><h3 id=hit5>Hit@5:<a class=td-heading-self-link href=#hit5 aria-label="Heading self-link"></a></h3><p>The percentage of queries for which the correct document is ranked among the top 5 documents.</p><hr><h2 id=log-loss>Log-loss:<a class=td-heading-self-link href=#log-loss aria-label="Heading self-link"></a></h2><p>The negative log likelihood of the predicted labels.</p><h2 id=brier-score>Brier score:<a class=td-heading-self-link href=#brier-score aria-label="Heading self-link"></a></h2><p>The mean squared difference between the predicted probabilities and the actual labels.</p><h2 id=confusion-matrix>Confusion matrix:<a class=td-heading-self-link href=#confusion-matrix aria-label="Heading self-link"></a></h2><p>A table that shows the true and predicted labels for each class.</p><h2 id=mse>MSE<a class=td-heading-self-link href=#mse aria-label="Heading self-link"></a></h2><h2 id=root-mean-squared-error-rmse>Root mean squared error (RMSE):<a class=td-heading-self-link href=#root-mean-squared-error-rmse aria-label="Heading self-link"></a></h2><p>A measure of the average squared difference between the predicted and actual values.</p><h2 id=mean-absolute-error-mae>Mean absolute error (MAE):<a class=td-heading-self-link href=#mean-absolute-error-mae aria-label="Heading self-link"></a></h2><p>A measure of the average absolute difference between the predicted and actual values.</p><h2 id=r-squared>R-squared:<a class=td-heading-self-link href=#r-squared aria-label="Heading self-link"></a></h2><p>A measure of the proportion of the variance in the target variable that is explained by the model.</p><h2 id=cohens-kappa>Cohen&rsquo;s kappa:<a class=td-heading-self-link href=#cohens-kappa aria-label="Heading self-link"></a></h2><p>A measure of agreement between two raters, taking into account chance agreement.</p><h2 id=matthews-correlation-coefficient-mcc>Matthews correlation coefficient (MCC):<a class=td-heading-self-link href=#matthews-correlation-coefficient-mcc aria-label="Heading self-link"></a></h2><p>A measure of the accuracy of a binary classifier that takes into account both the true positive rate and the false positive rate.</p><h2 id=area-under-the-curve-auc>Area under the curve (AUC):<a class=td-heading-self-link href=#area-under-the-curve-auc aria-label="Heading self-link"></a></h2><p>A measure of the performance of a binary classifier at all possible thresholds.</p><h2 id=precision-recall-curve>Precision-recall curve:<a class=td-heading-self-link href=#precision-recall-curve aria-label="Heading self-link"></a></h2><p>A graphical representation of the precision and recall of a binary classifier at all possible thresholds.</p><h2 id=roc-curve>ROC curve:<a class=td-heading-self-link href=#roc-curve aria-label="Heading self-link"></a></h2><p>A graphical representation of the true positive rate and the false positive rate of a binary classifier at all possible thresholds.</p><h2 id=mean-squared-logarithmic-error-msle>Mean squared logarithmic error (MSLE):<a class=td-heading-self-link href=#mean-squared-logarithmic-error-msle aria-label="Heading self-link"></a></h2><p>A measure of the average squared difference between the logarithms of the predicted and actual values.</p><h2 id=mean-absolute-percentage-error-mape>Mean absolute percentage error (MAPE):<a class=td-heading-self-link href=#mean-absolute-percentage-error-mape aria-label="Heading self-link"></a></h2><p>A measure of the average percentage difference between the predicted and actual values.</p><h2 id=root-mean-square-logarithmic-error-rmsle>Root mean square logarithmic error (RMSLE):<a class=td-heading-self-link href=#root-mean-square-logarithmic-error-rmsle aria-label="Heading self-link"></a></h2><p>A measure of the average squared difference between the logarithms of the predicted and actual values, after taking the square root.</p><h2 id=precision-at-k-pk>Precision at k (P@k):<a class=td-heading-self-link href=#precision-at-k-pk aria-label="Heading self-link"></a></h2><p>The percentage of the top k predictions that are correct.</p><p>The percentage of predicted positive samples that are ranked among the top k predictions.<br>Precision@k = (# of relevant items retrieved among top-k recommendations) / (# of total predicted relevant items)</p><p>For example, if a system retrieves 5 relevant items among the top-10 recommendations out of a total of 25 predicted relevant items, then the recall@10 would be 0.20 or 20%.</p><p>In other words, precision@k measures how well the system performs in predicting relevant items among the top-k recommendations. It is useful when we are interested in evaluating the ranking quality of recommendations rather than their absolute number.</p><h2 id=recall-at-k-rk>Recall at k (R@k):<a class=td-heading-self-link href=#recall-at-k-rk aria-label="Heading self-link"></a></h2><p>The percentage of actual positive samples that are ranked among the top k predictions.<br>Recall@k = (# of relevant items retrieved among top-k recommendations) / (# of total relevant items)</p><p>For example, if a system retrieves 5 relevant items among the top-10 recommendations out of a total of 20 relevant items, then the recall@10 would be 0.25 or 25%.</p><p>In other words, recall@k measures how well the system performs in retrieving relevant items among the top-k recommendations. It is useful when we are interested in evaluating the ranking quality of recommendations rather than their absolute number.</p><h2 id=f1-score-at-k-f1k>F1-score at k (F1@k):<a class=td-heading-self-link href=#f1-score-at-k-f1k aria-label="Heading self-link"></a></h2><p>The harmonic mean of precision at k and recall at k.</p><h2 id=interpretability>Interpretability:<a class=td-heading-self-link href=#interpretability aria-label="Heading self-link"></a></h2><p>A measure of how easy it is to understand and explain the predictions of a model.</p><h2 id=fairness>Fairness:<a class=td-heading-self-link href=#fairness aria-label="Heading self-link"></a></h2><p>A measure of how the model treats different groups of people.</p><h2 id=inception-score>Inception score:<a class=td-heading-self-link href=#inception-score aria-label="Heading self-link"></a></h2><p>A measure of the diversity and quality of the generated images by a generative adversarial network (GAN).</p><h2 id=frechet-inception-distance>Frechet Inception distance:<a class=td-heading-self-link href=#frechet-inception-distance aria-label="Heading self-link"></a></h2><p>A measure of the similarity between the generated images by a GAN and the real images.</p><h2 id=wasserstein-distance>Wasserstein distance:<a class=td-heading-self-link href=#wasserstein-distance aria-label="Heading self-link"></a></h2><p>A measure of the distance between the distributions of the generated images and the real images.</p><h2 id=inception-score-1>Inception score:<a class=td-heading-self-link href=#inception-score-1 aria-label="Heading self-link"></a></h2><p>A measure of the diversity and quality of the generated text by a language model.</p><h2 id=perplexity>Perplexity:<a class=td-heading-self-link href=#perplexity aria-label="Heading self-link"></a></h2><p>A measure of how difficult it is to predict the next word in a sequence.</p><hr><h2 id=bleu-score>BLEU score:<a class=td-heading-self-link href=#bleu-score aria-label="Heading self-link"></a></h2><p>A measure of the similarity between the generated text and the reference text.
<a href=../../dsblog/ml-tasks-and-model-evaluation#what-is-blue-benchmark>BLEU More..</a></p><h3 id=bleu4>BLEU4<a class=td-heading-self-link href=#bleu4 aria-label="Heading self-link"></a></h3><h3 id=bleu4_answer_extraction>BLEU4_answer_extraction<a class=td-heading-self-link href=#bleu4_answer_extraction aria-label="Heading self-link"></a></h3><h3 id=bleu4_question_answer_generation>BLEU4_question_answer_generation<a class=td-heading-self-link href=#bleu4_question_answer_generation aria-label="Heading self-link"></a></h3><h3 id=bleu4_question_answering>BLEU4_question_answering<a class=td-heading-self-link href=#bleu4_question_answering aria-label="Heading self-link"></a></h3><h3 id=bleu4_question_generation>BLEU4_question_generation<a class=td-heading-self-link href=#bleu4_question_generation aria-label="Heading self-link"></a></h3><hr><h2 id=intersection-over-union-iou>Intersection over union (IoU):<a class=td-heading-self-link href=#intersection-over-union-iou aria-label="Heading self-link"></a></h2><p>A measure of the overlap between two regions. It is typically used in object detection and segmentation tasks.</p><h2 id=top-5-error-rate>Top-5 error rate:<a class=td-heading-self-link href=#top-5-error-rate aria-label="Heading self-link"></a></h2><p>The percentage of images for which the correct class is not in the top 5 predictions. It is typically used in image classification tasks.</p><h2 id=em---exact-match>EM - Exact Match<a class=td-heading-self-link href=#em---exact-match aria-label="Heading self-link"></a></h2><p>EM used in NLP task evaluation, particularly in tasks like question answering and text generation. The EM metric measures the percentage of predictions that exactly match the ground truth or reference answers.</p><p>If the model&rsquo;s answer matches the reference answer word-for-word, then the EM score for that particular instance is 1. If the answers do not match exactly, the EM score is 0. The EM score is then calculated as the ratio of instances where the model&rsquo;s answer matches the reference answer exactly to the total number of instances in the evaluation dataset.</p><hr><h2 id=rouge>ROUGE<a class=td-heading-self-link href=#rouge aria-label="Heading self-link"></a></h2><p>ROUGE (Recall-Oriented Understudy for Gisting Evaluation): ROUGE is another metric used for evaluating text summarization and generation tasks. It measures the overlap of n-grams between the generated text and the reference text. <a href=../../dsblog/ml-tasks-and-model-evaluation#what-is-rouge-score>ROUGE More..</a></p><h3 id=rouge-l>rouge-l<a class=td-heading-self-link href=#rouge-l aria-label="Heading self-link"></a></h3><h3 id=rouge-2>rouge-2<a class=td-heading-self-link href=#rouge-2 aria-label="Heading self-link"></a></h3><h3 id=rouge-lsum>rouge-lsum<a class=td-heading-self-link href=#rouge-lsum aria-label="Heading self-link"></a></h3><h3 id=rouge_l_answer_extraction>rouge_l_answer_extraction<a class=td-heading-self-link href=#rouge_l_answer_extraction aria-label="Heading self-link"></a></h3><h3 id=rouge_l_question_answer_generation>rouge_l_question_answer_generation<a class=td-heading-self-link href=#rouge_l_question_answer_generation aria-label="Heading self-link"></a></h3><h3 id=rouge_l_question_answering>rouge_l_question_answering<a class=td-heading-self-link href=#rouge_l_question_answering aria-label="Heading self-link"></a></h3><h3 id=rouge_l_question_generation>rouge_l_question_generation<a class=td-heading-self-link href=#rouge_l_question_generation aria-label="Heading self-link"></a></h3><hr><h2 id=meteor>METEOR<a class=td-heading-self-link href=#meteor aria-label="Heading self-link"></a></h2><p>Metric for Evaluation of Translation with Explicit ORdering: METEOR is a metric that combines multiple measures including precision, recall, stemming, and synonymy. It aims to provide a balanced evaluation of machine translation quality.<a href=../../dsblog/ml-tasks-and-model-evaluation#what-is-meteor-score>METEOR More..</a></p><h3 id=meteor_answer_extraction>meteor_answer_extraction<a class=td-heading-self-link href=#meteor_answer_extraction aria-label="Heading self-link"></a></h3><h3 id=meteor_question_answer_generation>meteor_question_answer_generation<a class=td-heading-self-link href=#meteor_question_answer_generation aria-label="Heading self-link"></a></h3><h3 id=meteor_question_answering>meteor_question_answering<a class=td-heading-self-link href=#meteor_question_answering aria-label="Heading self-link"></a></h3><h3 id=meteor_question_generation>meteor_question_generation<a class=td-heading-self-link href=#meteor_question_generation aria-label="Heading self-link"></a></h3><h2 id=heqd>HEQD<a class=td-heading-self-link href=#heqd aria-label="Heading self-link"></a></h2><p>HEQD stands for Hierarchical Edit Distance. It is a metric used to evaluate the quality of text summarization. It is based on the edit distance between the ground truth summary and the generated summary, taking into account the hierarchical structure of the summary.</p><p>The HEQD metric is calculated as follows:</p><p>HEQD = 1 - (ED / L)
where</p><p>ED is the edit distance between the ground truth summary and the generated summary.<br>L is the length of the ground truth summary.<br>The HEQD metric is a more accurate measure of the quality of text summarization than the BLEU metric, as it takes into account the hierarchical structure of the summary.</p><p>Example:</p><p>Ground truth summary:</p><ul><li>The cat is on the mat.</li><li>The dog is chasing the ball.</li></ul><p>Generated summary:</p><ul><li>The cat and the dog are playing.</li></ul><p>The HEQD metric for this example would be:</p><p>HEQD = 1 - (2 / 2) = 0.5</p><p>This means that the generated summary is 50% similar to the ground truth summary.</p><p>The HEQD metric is a relatively new metric, and it is not yet as widely used as the BLEU metric. However, it is a promising metric for evaluating the quality of text summarization.</p><h2 id=perplexity-1>Perplexity<a class=td-heading-self-link href=#perplexity-1 aria-label="Heading self-link"></a></h2><p>In NLP and machine learning, perplexity is a measure of how well a language model predicts the next word in a sequence. It is calculated as the inverse of the probability of the model&rsquo;s predictions. A lower perplexity indicates that the model is better at predicting the next word.</p><p>The perplexity is calculated as follows:</p><p>Perplexity = $$exp( \frac{-1}{N * \sum(log(p(w_i)))})$$<br>where</p><p>N is the number of words in the test set.<br>$$w_i$$ is the i-th word in the test set.<br>$$p(w_i)$$ is the probability of the model predicting word $$w_i$$.</p><p>The perplexity metric is a more robust measure of the quality of a language model than the accuracy metric. This is because the accuracy metric only measures how well the model predicts the correct word, while the perplexity metric also takes into account the probability of the model&rsquo;s predictions.</p><p>Here is an example of how the perplexity metric can be used to evaluate the quality of a language model:</p><p>Let&rsquo;s say we have a language model that is trained on a corpus of text. We want to evaluate the quality of the model by using the perplexity metric. We take a test set of text and calculate the perplexity of the model&rsquo;s predictions on the test set. If the perplexity is low, then the model is good at predicting the next word. If the perplexity is high, then the model is not good at predicting the next word.</p><p>The perplexity metric is a useful tool for evaluating the quality of language models. It is a more robust measure of quality than the accuracy metric, and it can be used to compare the performance of different language models.</p><hr><h2 id=passn>Pass@n<a class=td-heading-self-link href=#passn aria-label="Heading self-link"></a></h2><p>The Pass@100 metric is calculated by measuring the percentage of queries for which the correct answer is among the top 100 candidates retrieved by the foundation language model (FLM) from a large corpus of documents. It reflects the ability of the FLM to retrieve relevant information from a large-scale knowledge source, which is essential for downstream tasks such as question answering, summarization, and dialogue. The higher the Pass@100 score, the better the FLM is at retrieving knowledge 1.</p><p>If you are interested in calculating Pass@100 for your own data, you can use binomial distribution. The formula for calculating Pass@100 using binomial distribution is as follows:</p><p>$$P(X >= k) = 1 - \sum_{i=0}^{k-1} [nCi * p^i * (1-p)^{n-i}]$$</p><p>where X is a binomial random variable representing the number of correct answers in the top 100 candidates, n is the total number of queries, p is the probability of getting a correct answer in one query, and k is the minimum number of correct answers required to achieve Pass@100.</p><p>Commonly used Pass@n metrics are pass@1, pass@10, Pass@100</p><hr><h2 id=answer-exact>Answer Exact<a class=td-heading-self-link href=#answer-exact aria-label="Heading self-link"></a></h2><h3 id=answer_exact_match_answer_extraction>answer_exact_match_answer_extraction<a class=td-heading-self-link href=#answer_exact_match_answer_extraction aria-label="Heading self-link"></a></h3><h3 id=answer_exact_match_question_answering>answer_exact_match_question_answering<a class=td-heading-self-link href=#answer_exact_match_question_answering aria-label="Heading self-link"></a></h3><hr><h2 id=answer-f1>Answer F1<a class=td-heading-self-link href=#answer-f1 aria-label="Heading self-link"></a></h2><h3 id=answer_f1_score__answer_extraction>answer_f1_score__answer_extraction<a class=td-heading-self-link href=#answer_f1_score__answer_extraction aria-label="Heading self-link"></a></h3><h3 id=answer_f1_score__question_answering>answer_f1_score__question_answering<a class=td-heading-self-link href=#answer_f1_score__question_answering aria-label="Heading self-link"></a></h3><hr><h2 id=bert-score>Bert Score<a class=td-heading-self-link href=#bert-score aria-label="Heading self-link"></a></h2><h3 id=bertscore>bertscore<a class=td-heading-self-link href=#bertscore aria-label="Heading self-link"></a></h3><h3 id=bertscore_answer_extraction>bertscore_answer_extraction<a class=td-heading-self-link href=#bertscore_answer_extraction aria-label="Heading self-link"></a></h3><h3 id=bertscore_question_answer_generation>bertscore_question_answer_generation<a class=td-heading-self-link href=#bertscore_question_answer_generation aria-label="Heading self-link"></a></h3><h3 id=bertscore_question_answering>bertscore_question_answering<a class=td-heading-self-link href=#bertscore_question_answering aria-label="Heading self-link"></a></h3><h3 id=bertscore_question_generation>bertscore_question_generation<a class=td-heading-self-link href=#bertscore_question_generation aria-label="Heading self-link"></a></h3><hr><h2 id=code>Code<a class=td-heading-self-link href=#code aria-label="Heading self-link"></a></h2><h3 id=code_eval>code_eval<a class=td-heading-self-link href=#code_eval aria-label="Heading self-link"></a></h3><h3 id=code_eval_outputs>code_eval_outputs<a class=td-heading-self-link href=#code_eval_outputs aria-label="Heading self-link"></a></h3><hr><h2 id=cosine>Cosine<a class=td-heading-self-link href=#cosine aria-label="Heading self-link"></a></h2><h3 id=cos_sim_accuracy>cos_sim_accuracy<a class=td-heading-self-link href=#cos_sim_accuracy aria-label="Heading self-link"></a></h3><pre tabindex=0><code>threshold = 0.9  # Define your threshold

true_positives = 0
false_positives = 0
false_negatives = 0

for question in questions:
    predicted_answer = model.predict(question)
    reference_answer = get_reference_answer(question)

    cosine_similarity = calculate_cosine_similarity(predicted_answer, reference_answer)

    if cosine_similarity &gt;= threshold:
        if predicted_answer == reference_answer:
            true_positives += 1
        else:
            false_positives += 1
    else:
        if predicted_answer == reference_answer:
            true_negative += 1
		else: 
		    false_negatives += 1 

cos_sim_precision = true_positives / (true_positives + false_positives)
cos_sim_recall = true_positives / (true_positives + false_negatives)
cos_sim_accuracy = (true_positive + true_negative)/len(quetions)
cos_sim_f1 = 2 * cos_sim_precision * cos_sim_recall / (cos_sim_precision +cos_sim_recall)
</code></pre><p><strong>Example Data</strong><br>Threshold : 0.65</p><table><thead><tr><th>Actual</th><th>Predicted Cosine</th><th>Pred>Threshold</th><th>Pred>Threshold =Actual</th><th>Prediction_Type</th><th>Remark</th></tr></thead><tbody><tr><td>TRUE</td><td>0.98</td><td>TRUE</td><td>TRUE</td><td>TP</td><td>Pred Cosine>=Threshold</td></tr><tr><td>TRUE</td><td>0.79</td><td>TRUE</td><td>TRUE</td><td>TP</td><td>Pred Cosine>=Threshold</td></tr><tr><td>FALSE</td><td>0.7</td><td>TRUE</td><td>FALSE</td><td>FP</td><td>Pred Cosine>=Threshold</td></tr><tr><td>TRUE</td><td>0.6</td><td>FALSE</td><td>FALSE</td><td>TN</td><td>Pred Cosine&lt;Threshold</td></tr><tr><td>TRUE</td><td>0.5</td><td>FALSE</td><td>FALSE</td><td>TN</td><td>Pred Cosine&lt;Threshold</td></tr><tr><td>FALSE</td><td>0.2</td><td>FALSE</td><td>TRUE</td><td>FN</td><td>Pred Cosine&lt;Threshold</td></tr><tr><td>FALSE</td><td>0.4</td><td>FALSE</td><td>TRUE</td><td>FN</td><td>Pred Cosine&lt;Threshold</td></tr><tr><td>FALSE</td><td>0.66</td><td>TRUE</td><td>FALSE</td><td>FN</td><td>Pred Cosine&lt;Threshold</td></tr></tbody></table><h3 id=cos_sim_f1>cos_sim_f1<a class=td-heading-self-link href=#cos_sim_f1 aria-label="Heading self-link"></a></h3><p>Check cos_sim_accuracy</p><h3 id=cos_sim_precision>cos_sim_precision<a class=td-heading-self-link href=#cos_sim_precision aria-label="Heading self-link"></a></h3><p>Check cos_sim_accuracy</p><h3 id=cos_sim_recall>cos_sim_recall<a class=td-heading-self-link href=#cos_sim_recall aria-label="Heading self-link"></a></h3><p>Check cos_sim_accuracy</p><h3 id=cos_sim_ap>cos_sim_ap<a class=td-heading-self-link href=#cos_sim_ap aria-label="Heading self-link"></a></h3><h3 id=cos_sim_pearson>cos_sim_pearson<a class=td-heading-self-link href=#cos_sim_pearson aria-label="Heading self-link"></a></h3><h3 id=cos_sim_spearman>cos_sim_spearman<a class=td-heading-self-link href=#cos_sim_spearman aria-label="Heading self-link"></a></h3><hr><h2 id=dot>Dot<a class=td-heading-self-link href=#dot aria-label="Heading self-link"></a></h2><h3 id=dot_accuracy>dot_accuracy<a class=td-heading-self-link href=#dot_accuracy aria-label="Heading self-link"></a></h3><h3 id=dot_ap>dot_ap<a class=td-heading-self-link href=#dot_ap aria-label="Heading self-link"></a></h3><h3 id=dot_f1>dot_f1<a class=td-heading-self-link href=#dot_f1 aria-label="Heading self-link"></a></h3><h3 id=dot_pearson>dot_pearson<a class=td-heading-self-link href=#dot_pearson aria-label="Heading self-link"></a></h3><h3 id=dot_precision>dot_precision<a class=td-heading-self-link href=#dot_precision aria-label="Heading self-link"></a></h3><h3 id=dot_recall>dot_recall<a class=td-heading-self-link href=#dot_recall aria-label="Heading self-link"></a></h3><h3 id=dot_spearman>dot_spearman<a class=td-heading-self-link href=#dot_spearman aria-label="Heading self-link"></a></h3><hr><h2 id=euclidean>Euclidean<a class=td-heading-self-link href=#euclidean aria-label="Heading self-link"></a></h2><h3 id=euclidean_accuracy>euclidean_accuracy<a class=td-heading-self-link href=#euclidean_accuracy aria-label="Heading self-link"></a></h3><h3 id=euclidean_ap>euclidean_ap<a class=td-heading-self-link href=#euclidean_ap aria-label="Heading self-link"></a></h3><h3 id=euclidean_f1>euclidean_f1<a class=td-heading-self-link href=#euclidean_f1 aria-label="Heading self-link"></a></h3><h3 id=euclidean_pearson>euclidean_pearson<a class=td-heading-self-link href=#euclidean_pearson aria-label="Heading self-link"></a></h3><h3 id=euclidean_precision>euclidean_precision<a class=td-heading-self-link href=#euclidean_precision aria-label="Heading self-link"></a></h3><h3 id=euclidean_recall>euclidean_recall<a class=td-heading-self-link href=#euclidean_recall aria-label="Heading self-link"></a></h3><h3 id=euclidean_spearman>euclidean_spearman<a class=td-heading-self-link href=#euclidean_spearman aria-label="Heading self-link"></a></h3><hr><h2 id=eval>Eval<a class=td-heading-self-link href=#eval aria-label="Heading self-link"></a></h2><h3 id=eval_accuracy>eval_accuracy<a class=td-heading-self-link href=#eval_accuracy aria-label="Heading self-link"></a></h3><h3 id=eval_exact>eval_exact<a class=td-heading-self-link href=#eval_exact aria-label="Heading self-link"></a></h3><h3 id=eval_f1>eval_f1<a class=td-heading-self-link href=#eval_f1 aria-label="Heading self-link"></a></h3><h3 id=eval_hasans_exact>eval_hasans_exact<a class=td-heading-self-link href=#eval_hasans_exact aria-label="Heading self-link"></a></h3><h3 id=eval_hasans_f1>eval_hasans_f1<a class=td-heading-self-link href=#eval_hasans_f1 aria-label="Heading self-link"></a></h3><h3 id=eval_noans_exact>eval_noans_exact<a class=td-heading-self-link href=#eval_noans_exact aria-label="Heading self-link"></a></h3><h3 id=eval_noans_f1>eval_noans_f1<a class=td-heading-self-link href=#eval_noans_f1 aria-label="Heading self-link"></a></h3><h3 id=eval_precision>eval_precision<a class=td-heading-self-link href=#eval_precision aria-label="Heading self-link"></a></h3><h3 id=eval_recall>eval_recall<a class=td-heading-self-link href=#eval_recall aria-label="Heading self-link"></a></h3><hr><h2 id=exact>Exact<a class=td-heading-self-link href=#exact aria-label="Heading self-link"></a></h2><h3 id=exact-1>exact<a class=td-heading-self-link href=#exact-1 aria-label="Heading self-link"></a></h3><h3 id=exact_match>exact_match<a class=td-heading-self-link href=#exact_match aria-label="Heading self-link"></a></h3><hr><h2 id=gen>Gen<a class=td-heading-self-link href=#gen aria-label="Heading self-link"></a></h2><h3 id=gen_len>gen_len<a class=td-heading-self-link href=#gen_len aria-label="Heading self-link"></a></h3><h3 id=gen-length>gen-length<a class=td-heading-self-link href=#gen-length aria-label="Heading self-link"></a></h3><hr><h2 id=joint-goal-accuracy>Joint Goal Accuracy<a class=td-heading-self-link href=#joint-goal-accuracy aria-label="Heading self-link"></a></h2><h3 id=joint-goal-accuracy-1>joint goal accuracy<a class=td-heading-self-link href=#joint-goal-accuracy-1 aria-label="Heading self-link"></a></h3><h3 id=joint-goal-expected-calibration-error>joint goal expected calibration error<a class=td-heading-self-link href=#joint-goal-expected-calibration-error aria-label="Heading self-link"></a></h3><hr><h2 id=manhattan>Manhattan<a class=td-heading-self-link href=#manhattan aria-label="Heading self-link"></a></h2><h3 id=manhattan_accuracy>manhattan_accuracy<a class=td-heading-self-link href=#manhattan_accuracy aria-label="Heading self-link"></a></h3><h3 id=manhattan_ap>manhattan_ap<a class=td-heading-self-link href=#manhattan_ap aria-label="Heading self-link"></a></h3><h3 id=manhattan_f1>manhattan_f1<a class=td-heading-self-link href=#manhattan_f1 aria-label="Heading self-link"></a></h3><h3 id=manhattan_precision>manhattan_precision<a class=td-heading-self-link href=#manhattan_precision aria-label="Heading self-link"></a></h3><h3 id=manhattan_recall>manhattan_recall<a class=td-heading-self-link href=#manhattan_recall aria-label="Heading self-link"></a></h3><h3 id=manhattan_spearman>manhattan_spearman<a class=td-heading-self-link href=#manhattan_spearman aria-label="Heading self-link"></a></h3><h3 id=manhattan_pearson>manhattan_pearson<a class=td-heading-self-link href=#manhattan_pearson aria-label="Heading self-link"></a></h3><hr><h2 id=max>Max<a class=td-heading-self-link href=#max aria-label="Heading self-link"></a></h2><h3 id=max_accuracy>max_accuracy<a class=td-heading-self-link href=#max_accuracy aria-label="Heading self-link"></a></h3><h3 id=max_ap>max_ap<a class=td-heading-self-link href=#max_ap aria-label="Heading self-link"></a></h3><h3 id=max_f1>max_f1<a class=td-heading-self-link href=#max_f1 aria-label="Heading self-link"></a></h3><h2 id=mean>mean<a class=td-heading-self-link href=#mean aria-label="Heading self-link"></a></h2><h3 id=mean_reciprocal_rank>mean_reciprocal_rank<a class=td-heading-self-link href=#mean_reciprocal_rank aria-label="Heading self-link"></a></h3><h3 id=mean_reward>mean_reward<a class=td-heading-self-link href=#mean_reward aria-label="Heading self-link"></a></h3><hr><h2 id=macro>Macro<a class=td-heading-self-link href=#macro aria-label="Heading self-link"></a></h2><h3 id=macro_f1>macro_f1<a class=td-heading-self-link href=#macro_f1 aria-label="Heading self-link"></a></h3><h3 id=macro_precision>macro_precision<a class=td-heading-self-link href=#macro_precision aria-label="Heading self-link"></a></h3><h3 id=macro_recall>macro_recall<a class=td-heading-self-link href=#macro_recall aria-label="Heading self-link"></a></h3><hr><h2 id=micro>Micro<a class=td-heading-self-link href=#micro aria-label="Heading self-link"></a></h2><h3 id=micro_precision>micro_precision<a class=td-heading-self-link href=#micro_precision aria-label="Heading self-link"></a></h3><h3 id=micro_recall>micro_recall<a class=td-heading-self-link href=#micro_recall aria-label="Heading self-link"></a></h3><h3 id=micro_f1>micro_f1<a class=td-heading-self-link href=#micro_f1 aria-label="Heading self-link"></a></h3><h3 id=micro_f1_cardiffnlp>micro_f1_cardiffnlp<a class=td-heading-self-link href=#micro_f1_cardiffnlp aria-label="Heading self-link"></a></h3><h3 id=micro_f1_tweet_evalemoji>micro_f1_tweet_eval/emoji<a class=td-heading-self-link href=#micro_f1_tweet_evalemoji aria-label="Heading self-link"></a></h3><h3 id=micro_f1_tweet_evalemotion>micro_f1_tweet_eval/emotion<a class=td-heading-self-link href=#micro_f1_tweet_evalemotion aria-label="Heading self-link"></a></h3><h3 id=micro_f1_tweet_evalhate>micro_f1_tweet_eval/hate<a class=td-heading-self-link href=#micro_f1_tweet_evalhate aria-label="Heading self-link"></a></h3><h3 id=micro_f1_tweet_evalirony>micro_f1_tweet_eval/irony<a class=td-heading-self-link href=#micro_f1_tweet_evalirony aria-label="Heading self-link"></a></h3><h3 id=micro_f1_tweet_evaloffensive>micro_f1_tweet_eval/offensive<a class=td-heading-self-link href=#micro_f1_tweet_evaloffensive aria-label="Heading self-link"></a></h3><h3 id=micro_f1_tweet_evalsentiment>micro_f1_tweet_eval/sentiment<a class=td-heading-self-link href=#micro_f1_tweet_evalsentiment aria-label="Heading self-link"></a></h3><hr><h2 id=mover-score>Mover Score<a class=td-heading-self-link href=#mover-score aria-label="Heading self-link"></a></h2><h3 id=moverscore_answer_extraction>moverscore_answer_extraction<a class=td-heading-self-link href=#moverscore_answer_extraction aria-label="Heading self-link"></a></h3><h3 id=moverscore_question_answer_generation>moverscore_question_answer_generation<a class=td-heading-self-link href=#moverscore_question_answer_generation aria-label="Heading self-link"></a></h3><h3 id=moverscore_question_answering>moverscore_question_answering<a class=td-heading-self-link href=#moverscore_question_answering aria-label="Heading self-link"></a></h3><h3 id=moverscore_question_generation>moverscore_question_generation<a class=td-heading-self-link href=#moverscore_question_generation aria-label="Heading self-link"></a></h3><hr><h2 id=normalized-discounted-cumulative-gain-ndcg>Normalized discounted cumulative gain (NDCG):<a class=td-heading-self-link href=#normalized-discounted-cumulative-gain-ndcg aria-label="Heading self-link"></a></h2><p>NDCG (Normalized Discounted Cumulative Gain) is a widely used metric in information retrieval and recommendation systems to evaluate the quality of ranked search results or recommendations. The &ldquo;NDCG@n&rdquo; metric is a variation of NDCG that focuses on a specific cutoff point, n, which represents the number of items or documents to consider in the evaluation. It assesses how well the top n items or documents in a ranked list match the relevance of the ground truth or expected results.</p><p>Here&rsquo;s how NDCG@n is calculated:</p><ol><li><p><strong>Rank the Items or Documents</strong>: Start by ranking the items or documents based on some relevance score. For example, in a search engine, documents may be ranked based on their relevance to a user&rsquo;s query.</p></li><li><p><strong>Determine Relevance Scores</strong>: Assign relevance scores to each of the items or documents. These scores typically range from 0 (not relevant) to 1 (perfectly relevant). These scores represent how relevant each item or document is to the user&rsquo;s query or the context of the evaluation.</p></li><li><p><strong>Calculate DCG@n (Discounted Cumulative Gain at n)</strong>: Calculate the Discounted Cumulative Gain at the cutoff point n. DCG@n is computed as the sum of the relevance scores of the top n items, with a logarithmic discount applied to the position of each item:</p><pre tabindex=0><code>DCG@n = rel(1) + (rel(2) / log2(2)) + (rel(3) / log2(3)) + ... + (rel(n) / log2(n))
</code></pre><p>Where:</p><ul><li><code>rel(i)</code> is the relevance score of the item at position i.</li><li><code>log2(i)</code> is the logarithm base 2 of i.</li></ul></li><li><p><strong>Calculate IDCG@n (Ideal Discounted Cumulative Gain at n)</strong>: Calculate the Ideal Discounted Cumulative Gain at the cutoff point n. IDCG@n represents the best possible DCG@n that could be achieved if all the items were perfectly ranked. To calculate IDCG@n, sort the items by their true relevance scores and calculate DCG@n using the same formula.</p></li><li><p><strong>Calculate NDCG@n (Normalized Discounted Cumulative Gain at n)</strong>: Calculate NDCG@n by dividing DCG@n by IDCG@n:</p><pre tabindex=0><code>NDCG@n = DCG@n / IDCG@n
</code></pre></li></ol><p>NDCG@n provides a normalized measure of the quality of the ranked list at the specified cutoff point. It ranges from 0 to 1, where higher values indicate better ranking quality. A value of 1 indicates that the top n items are perfectly ranked according to relevance.</p><p>NDCG@n is particularly useful when you want to assess the performance of recommendation systems, search engines, or any system that presents ranked lists to users, and you are interested in evaluating the quality of the top n results.</p><p>Popular NDCG@n metrics are NDCG@1, NDCG@3, NDGC@5, NDGC@10, NDCG@100, NDCG@1000</p><hr><h2 id=pearson>Pearson<a class=td-heading-self-link href=#pearson aria-label="Heading self-link"></a></h2><h3 id=pearson_correlation>pearson_correlation<a class=td-heading-self-link href=#pearson_correlation aria-label="Heading self-link"></a></h3><h3 id=pearsons-r-distress>pearson&rsquo;s r (distress)<a class=td-heading-self-link href=#pearsons-r-distress aria-label="Heading self-link"></a></h3><h3 id=pearsons-r-empathy>pearson&rsquo;s r (empathy)<a class=td-heading-self-link href=#pearsons-r-empathy aria-label="Heading self-link"></a></h3><hr><h2 id=qa-aligned-f1>QA Aligned F1<a class=td-heading-self-link href=#qa-aligned-f1 aria-label="Heading self-link"></a></h2><h3 id=qa_aligned_f1_score_bertscore_question_answer_generation>qa_aligned_f1_score_bertscore_question_answer_generation<a class=td-heading-self-link href=#qa_aligned_f1_score_bertscore_question_answer_generation aria-label="Heading self-link"></a></h3><h3 id=qa_aligned_f1_score_bertscore_question_answer_generation_with_gold_answer>qa_aligned_f1_score_bertscore_question_answer_generation_with_gold_answer<a class=td-heading-self-link href=#qa_aligned_f1_score_bertscore_question_answer_generation_with_gold_answer aria-label="Heading self-link"></a></h3><h3 id=qa_aligned_f1_score_bertscore_question_answer_generation_with_gold_answer_gold_answer>qa_aligned_f1_score_bertscore_question_answer_generation_with_gold_answer_gold_answer<a class=td-heading-self-link href=#qa_aligned_f1_score_bertscore_question_answer_generation_with_gold_answer_gold_answer aria-label="Heading self-link"></a></h3><h3 id=qa_aligned_f1_score_moverscore_question_answer_generation>qa_aligned_f1_score_moverscore_question_answer_generation<a class=td-heading-self-link href=#qa_aligned_f1_score_moverscore_question_answer_generation aria-label="Heading self-link"></a></h3><h3 id=qa_aligned_f1_score_moverscore_question_answer_generation_gold_answer>qa_aligned_f1_score_moverscore_question_answer_generation_gold_answer<a class=td-heading-self-link href=#qa_aligned_f1_score_moverscore_question_answer_generation_gold_answer aria-label="Heading self-link"></a></h3><hr><h2 id=qa-aligned-precision>QA Aligned Precision<a class=td-heading-self-link href=#qa-aligned-precision aria-label="Heading self-link"></a></h2><h3 id=qa_aligned_precision_bertscore_question_answer_generation>qa_aligned_precision_bertscore_question_answer_generation<a class=td-heading-self-link href=#qa_aligned_precision_bertscore_question_answer_generation aria-label="Heading self-link"></a></h3><h3 id=qa_aligned_precision_bertscore_question_answer_generation_with_gold_answer>qa_aligned_precision_bertscore_question_answer_generation_with_gold_answer<a class=td-heading-self-link href=#qa_aligned_precision_bertscore_question_answer_generation_with_gold_answer aria-label="Heading self-link"></a></h3><h3 id=qa_aligned_precision_bertscore_question_answer_generation_with_gold_answer_gold_answer>qa_aligned_precision_bertscore_question_answer_generation_with_gold_answer_gold_answer<a class=td-heading-self-link href=#qa_aligned_precision_bertscore_question_answer_generation_with_gold_answer_gold_answer aria-label="Heading self-link"></a></h3><h3 id=qa_aligned_precision_moverscore_question_answer_generation>qa_aligned_precision_moverscore_question_answer_generation<a class=td-heading-self-link href=#qa_aligned_precision_moverscore_question_answer_generation aria-label="Heading self-link"></a></h3><h3 id=qa_aligned_precision_moverscore_question_answer_generation_with_gold_answer>qa_aligned_precision_moverscore_question_answer_generation_with_gold_answer<a class=td-heading-self-link href=#qa_aligned_precision_moverscore_question_answer_generation_with_gold_answer aria-label="Heading self-link"></a></h3><h3 id=qa_aligned_precision_moverscore_question_answer_generation_with_gold_answer_gold_answer>qa_aligned_precision_moverscore_question_answer_generation_with_gold_answer_gold_answer<a class=td-heading-self-link href=#qa_aligned_precision_moverscore_question_answer_generation_with_gold_answer_gold_answer aria-label="Heading self-link"></a></h3><hr><h2 id=qa-aligned-recall>QA Aligned Recall<a class=td-heading-self-link href=#qa-aligned-recall aria-label="Heading self-link"></a></h2><h3 id=qa_aligned_recall_bertscore_question_answer_generation>qa_aligned_recall_bertscore_question_answer_generation<a class=td-heading-self-link href=#qa_aligned_recall_bertscore_question_answer_generation aria-label="Heading self-link"></a></h3><h3 id=qa_aligned_recall_bertscore_question_answer_generation_with_gold_answer>qa_aligned_recall_bertscore_question_answer_generation_with_gold_answer<a class=td-heading-self-link href=#qa_aligned_recall_bertscore_question_answer_generation_with_gold_answer aria-label="Heading self-link"></a></h3><h3 id=qa_aligned_recall_bertscore_question_answer_generation_with_gold_answer_gold_answer>qa_aligned_recall_bertscore_question_answer_generation_with_gold_answer_gold_answer<a class=td-heading-self-link href=#qa_aligned_recall_bertscore_question_answer_generation_with_gold_answer_gold_answer aria-label="Heading self-link"></a></h3><h3 id=qa_aligned_recall_moverscore_question_answer_generation>qa_aligned_recall_moverscore_question_answer_generation<a class=td-heading-self-link href=#qa_aligned_recall_moverscore_question_answer_generation aria-label="Heading self-link"></a></h3><h3 id=qa_aligned_recall_moverscore_question_answer_generation_with_gold_answer>qa_aligned_recall_moverscore_question_answer_generation_with_gold_answer<a class=td-heading-self-link href=#qa_aligned_recall_moverscore_question_answer_generation_with_gold_answer aria-label="Heading self-link"></a></h3><h3 id=qa_aligned_recall_moverscore_question_answer_generation_with_gold_answer_gold_answer>qa_aligned_recall_moverscore_question_answer_generation_with_gold_answer_gold_answer<a class=td-heading-self-link href=#qa_aligned_recall_moverscore_question_answer_generation_with_gold_answer_gold_answer aria-label="Heading self-link"></a></h3><hr><h2 id=squad>SQUAD<a class=td-heading-self-link href=#squad aria-label="Heading self-link"></a></h2><h3 id=squad-1>squad<a class=td-heading-self-link href=#squad-1 aria-label="Heading self-link"></a></h3><h3 id=squad_v2>squad_v2<a class=td-heading-self-link href=#squad_v2 aria-label="Heading self-link"></a></h3><hr><h2 id=top-n-accuracy>Top-n Accuracy<a class=td-heading-self-link href=#top-n-accuracy aria-label="Heading self-link"></a></h2><h3 id=top-1-accuracy>top-1 accuracy<a class=td-heading-self-link href=#top-1-accuracy aria-label="Heading self-link"></a></h3><h3 id=top-5-accuracy>top-5 accuracy<a class=td-heading-self-link href=#top-5-accuracy aria-label="Heading self-link"></a></h3><hr><h2 id=validation>Validation<a class=td-heading-self-link href=#validation aria-label="Heading self-link"></a></h2><h3 id=validation_accuracy>validation_accuracy<a class=td-heading-self-link href=#validation_accuracy aria-label="Heading self-link"></a></h3><h3 id=validation-loss>validation loss<a class=td-heading-self-link href=#validation-loss aria-label="Heading self-link"></a></h3><hr><h2 id=weighted>Weighted<a class=td-heading-self-link href=#weighted aria-label="Heading self-link"></a></h2><h3 id=weighted_f1>weighted_f1<a class=td-heading-self-link href=#weighted_f1 aria-label="Heading self-link"></a></h3><h3 id=weighted_precision>weighted_precision<a class=td-heading-self-link href=#weighted_precision aria-label="Heading self-link"></a></h3><h3 id=weighted_recall>weighted_recall<a class=td-heading-self-link href=#weighted_recall aria-label="Heading self-link"></a></h3><hr><h2 id=wer>WER<a class=td-heading-self-link href=#wer aria-label="Heading self-link"></a></h2><h3 id=wer_without_norm>wer_without_norm<a class=td-heading-self-link href=#wer_without_norm aria-label="Heading self-link"></a></h3><hr><h2 id=matthews>Matthews<a class=td-heading-self-link href=#matthews aria-label="Heading self-link"></a></h2><h3 id=matthews_correlation>matthews_correlation<a class=td-heading-self-link href=#matthews_correlation aria-label="Heading self-link"></a></h3><hr><h2 id=accuracy-radius-1>Accuracy-radius-1<a class=td-heading-self-link href=#accuracy-radius-1 aria-label="Heading self-link"></a></h2><h2 id=act_dcf-p001>Act_dcf-p=0.01<a class=td-heading-self-link href=#act_dcf-p001 aria-label="Heading self-link"></a></h2><h2 id=avgrank>Avgrank<a class=td-heading-self-link href=#avgrank aria-label="Heading self-link"></a></h2><h2 id=arc>ARC<a class=td-heading-self-link href=#arc aria-label="Heading self-link"></a></h2><p>The <strong>Abstraction and Reasoning Corpus (ARC)</strong> is a benchmark dataset designed to encourage research in advanced question answering and measure a human-like form of general fluid intelligence. It was built upon an explicit set of priors designed to be as close as possible to innate human priors. The ARC dataset requires far more powerful knowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC question set is partitioned into a Challenge Set and an Easy Set, where the Challenge Set contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm. The dataset contains only natural, grade-school science questions (authored for human tests), and is the largest public-domain set of this kind (7,787 questions</p><h2 id=byte_perplexity>Byte_perplexity<a class=td-heading-self-link href=#byte_perplexity aria-label="Heading self-link"></a></h2><h2 id=cer>CER<a class=td-heading-self-link href=#cer aria-label="Heading self-link"></a></h2><h2 id=cher>Cher<a class=td-heading-self-link href=#cher aria-label="Heading self-link"></a></h2><h2 id=chrf>Chrf<a class=td-heading-self-link href=#chrf aria-label="Heading self-link"></a></h2><h2 id=cider>Cider<a class=td-heading-self-link href=#cider aria-label="Heading self-link"></a></h2><h2 id=codebleu>Codebleu<a class=td-heading-self-link href=#codebleu aria-label="Heading self-link"></a></h2><h2 id=conll>Conll<a class=td-heading-self-link href=#conll aria-label="Heading self-link"></a></h2><h2 id=coval>Coval<a class=td-heading-self-link href=#coval aria-label="Heading self-link"></a></h2><h2 id=cver>Cver<a class=td-heading-self-link href=#cver aria-label="Heading self-link"></a></h2><h2 id=der>DER<a class=td-heading-self-link href=#der aria-label="Heading self-link"></a></h2><h2 id=bialog-acts-accuracy>Bialog acts accuracy<a class=td-heading-self-link href=#bialog-acts-accuracy aria-label="Heading self-link"></a></h2><h2 id=dialog-acts-f1>Dialog acts f1<a class=td-heading-self-link href=#dialog-acts-f1 aria-label="Heading self-link"></a></h2><h2 id=diffbleu>Diffbleu<a class=td-heading-self-link href=#diffbleu aria-label="Heading self-link"></a></h2><h2 id=dvitelcodebleu>Dvitel/codebleu<a class=td-heading-self-link href=#dvitelcodebleu aria-label="Heading self-link"></a></h2><h2 id=eer>EER<a class=td-heading-self-link href=#eer aria-label="Heading self-link"></a></h2><h2 id=em>EM<a class=td-heading-self-link href=#em aria-label="Heading self-link"></a></h2><h2 id=empos>Empos<a class=td-heading-self-link href=#empos aria-label="Heading self-link"></a></h2><h2 id=fid>FID<a class=td-heading-self-link href=#fid aria-label="Heading self-link"></a></h2><h2 id=hamming-score>Hamming score<a class=td-heading-self-link href=#hamming-score aria-label="Heading self-link"></a></h2><h2 id=jaccard-error-rate>Jaccard error rate<a class=td-heading-self-link href=#jaccard-error-rate aria-label="Heading self-link"></a></h2><h2 id=lambada>Lambada<a class=td-heading-self-link href=#lambada aria-label="Heading self-link"></a></h2><h2 id=language-model-loss>Language model loss<a class=td-heading-self-link href=#language-model-loss aria-label="Heading self-link"></a></h2><h2 id=las>LAS<a class=td-heading-self-link href=#las aria-label="Heading self-link"></a></h2><h2 id=loss>Loss<a class=td-heading-self-link href=#loss aria-label="Heading self-link"></a></h2><h2 id=mer>MER<a class=td-heading-self-link href=#mer aria-label="Heading self-link"></a></h2><h2 id=mmlu>MMLU<a class=td-heading-self-link href=#mmlu aria-label="Heading self-link"></a></h2><p>The MMLU metric, or Massive Multitask Language Understanding, is a benchmark for evaluating the general knowledge and reasoning capabilities of large language models (LLMs). It was developed by Meta AI and released in 2022.</p><p>The MMLU dataset consists of 15,908 multiple-choice questions covering a wide range of topics, including mathematics, US history, computer science, law, and more. The questions are designed to assess the model&rsquo;s understanding of the world and its ability to reason about complex concepts. The test covers 57 tasks, including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem-solving ability</p><p>The MMLU metric is calculated by averaging the model&rsquo;s accuracy on all of the questions in the dataset. This makes it a comprehensive measure of the model&rsquo;s overall performance.</p><p>The MMLU metric has been used to evaluate a number of different LLMs, including GPT-3, Jurassic-1 Jumbo, and LaMDA. As of October 2023, the highest MMLU score achieved by a public LLM is 86.7%, which was set by GPT-4.</p><p>The MMLU metric is an important tool for evaluating the progress of LLMs and for identifying areas where they need improvement. It is also a useful tool for comparing the performance of different LLMs.</p><p>Here are some of the benefits of using the MMLU metric:</p><ul><li>It is a comprehensive measure of the model&rsquo;s overall performance on a wide range of tasks.</li><li>It is fair and unbiased, as it uses a standardized set of questions.</li><li>It is easy to understand and interpret.</li><li>It is reproducible, meaning that other researchers can easily obtain the same results by evaluating their models on the same dataset.</li></ul><p>Overall, the MMLU metric is a valuable tool for evaluating the performance of LLMs and for tracking their progress over time.</p><h2 id=nuclearity>Nuclearity<a class=td-heading-self-link href=#nuclearity aria-label="Heading self-link"></a></h2><h2 id=ovrl>OVRL<a class=td-heading-self-link href=#ovrl aria-label="Heading self-link"></a></h2><h2 id=per>PER<a class=td-heading-self-link href=#per aria-label="Heading self-link"></a></h2><h2 id=perplexity-2>Perplexity<a class=td-heading-self-link href=#perplexity-2 aria-label="Heading self-link"></a></h2><h2 id=pesq>PESQ<a class=td-heading-self-link href=#pesq aria-label="Heading self-link"></a></h2><h2 id=ppl>PPL<a class=td-heading-self-link href=#ppl aria-label="Heading self-link"></a></h2><h2 id=qwk>QWK<a class=td-heading-self-link href=#qwk aria-label="Heading self-link"></a></h2><h2 id=re-macro-f1>RE+ macro f1<a class=td-heading-self-link href=#re-macro-f1 aria-label="Heading self-link"></a></h2><h2 id=roc-auc>ROC AUC<a class=td-heading-self-link href=#roc-auc aria-label="Heading self-link"></a></h2><h2 id=sacrebleu>Sacrebleu<a class=td-heading-self-link href=#sacrebleu aria-label="Heading self-link"></a></h2><h2 id=sari>Sari<a class=td-heading-self-link href=#sari aria-label="Heading self-link"></a></h2><h2 id=ser>SER<a class=td-heading-self-link href=#ser aria-label="Heading self-link"></a></h2><h2 id=si-sdri>Si-sdri<a class=td-heading-self-link href=#si-sdri aria-label="Heading self-link"></a></h2><h2 id=si-snri>Si-snri<a class=td-heading-self-link href=#si-snri aria-label="Heading self-link"></a></h2><h2 id=sig>SIG<a class=td-heading-self-link href=#sig aria-label="Heading self-link"></a></h2><h2 id=slot-error-rate>Slot error rate<a class=td-heading-self-link href=#slot-error-rate aria-label="Heading self-link"></a></h2><h2 id=slot-f1>Slot f1<a class=td-heading-self-link href=#slot-f1 aria-label="Heading self-link"></a></h2><h2 id=span>Span<a class=td-heading-self-link href=#span aria-label="Heading self-link"></a></h2><h2 id=spearmanr>Spearmanr<a class=td-heading-self-link href=#spearmanr aria-label="Heading self-link"></a></h2><h2 id=spice>Spice<a class=td-heading-self-link href=#spice aria-label="Heading self-link"></a></h2><h2 id=spider>Spider<a class=td-heading-self-link href=#spider aria-label="Heading self-link"></a></h2><h2 id=hellaswag>HellaSwag<a class=td-heading-self-link href=#hellaswag aria-label="Heading self-link"></a></h2><p>HellaSwag is a dataset for studying grounded commonsense inference. It consists of <strong>70k multiple choice questions</strong> about grounded situations, each question comes from one of two domains &ndash; activitynet or wikihow &ndash; with four answer choices about what might happen next in the scene. The dataset was designed to test commonsense natural language inference (NLI) about physical situations.</p><p>The name <strong>HellaSwag</strong> is an acronym for Harder Endings, Longer contexts, and Low-shot Activities for Situations With Adversarial Generations ¹. The dataset was created by Zellers et al. in 2019 to evaluate common-sense reasoning in large language models (LLMs) ¹.</p><p>The dataset is used to measure the performance of LLMs on commonsense reasoning tasks. The questions are segments of video captions describing some event in the physical world. A video caption segment provides an initial context for an LLM. Each context is then followed by four options for completing that context, with only one option being correct.</p><h2 id=ter>TER<a class=td-heading-self-link href=#ter aria-label="Heading self-link"></a></h2><h2 id=text-image-similarity>Text-image-similarity<a class=td-heading-self-link href=#text-image-similarity aria-label="Heading self-link"></a></h2><h2 id=training-loss>Training loss<a class=td-heading-self-link href=#training-loss aria-label="Heading self-link"></a></h2><h2 id=trueskill>Trueskill<a class=td-heading-self-link href=#trueskill aria-label="Heading self-link"></a></h2><h2 id=uas>UAS<a class=td-heading-self-link href=#uas aria-label="Heading self-link"></a></h2><h2 id=wikitext>Wikitext<a class=td-heading-self-link href=#wikitext aria-label="Heading self-link"></a></h2><h2 id=wil>WIL<a class=td-heading-self-link href=#wil aria-label="Heading self-link"></a></h2><h2 id=wip>WIP<a class=td-heading-self-link href=#wip aria-label="Heading self-link"></a></h2><h2 id=zero-shot-transfer>Zero-shot transfer<a class=td-heading-self-link href=#zero-shot-transfer aria-label="Heading self-link"></a></h2><h2 id=resources>Resources<a class=td-heading-self-link href=#resources aria-label="Heading self-link"></a></h2><ul><li><a href=https://huggingface.co/spaces/autoevaluate/leaderboards>Metrics at Huggingface</a></li><li><a href=https://paperswithcode.com/sota>SOTA Paperwithcode</a></li><li><a href=https://paperswithcode.com/task/question-answering>Question Answering Paperwithcode</a></li><li><a href=https://opus.nlpl.eu/leaderboard/>BLEU Score</a></li><li><a href=https://gluebenchmark.com/leaderboard>GLUE Score</a></li><li><a href=https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard>Open LLM Leaderboard</a></li><li><a href=https://leaderboard.allenai.org/>AllanAI Leaderboards</a></li><li><a href=https://ai.google.com/research/NaturalQuestions/leaderboard>AI Google - NaturalQuestions/leaderboard</a></li><li><a href=https://eval.ai/web/challenges/challenge-page/514/leaderboard/1386>Eval AI</a></li><li><a href=https://quac.ai/>QuAC- Question Answering in Context</a></li></ul><p><strong>Author</strong><br>Dr Hari Thapliyaal<br>dasarpai.com<br>linkedin.com/in/harithapliyal</p><div class=category-section><h4 class=category-section__title>Categories:</h4><div class=category-badges><a href=../../categories/dsblog class=category-badge>dsblog</a></div></div><div class=td-tags><h4 class=td-tags__title>Tags:</h4><div class=category-badges><a href=../../tags/machine-learning class=category-badge>Machine Learning</a><a href=../../tags/model-evaluation class=category-badge>Model Evaluation</a><a href=../../tags/performance-metrics class=category-badge>Performance Metrics</a><a href=../../tags/data-science class=category-badge>Data Science</a><a href=../../tags/ml-metrics class=category-badge>ML Metrics</a><a href=../../tags/model-assessment class=category-badge>Model Assessment</a><a href=../../tags/statistical-analysis class=category-badge>Statistical Analysis</a></div></div><div class=td-author-box><div class=td-author-box__info><h4 class=td-author-box__name>Blowfish</h4><p class=td-author-box__bio>A powerful, lightweight theme for Hugo.</p><div class=td-author-box__links><a href target=_blank rel=noopener aria-label></a><a href target=_blank rel=noopener aria-label></a></div></div></div><div class=td-social-share><h4 class=td-social-share__title>Share this article:</h4><ul class=td-social-share__list><div class=social-share><a href="https://twitter.com/intent/tweet?text=Machine%20Learning%20Metrics&url=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fMachine-Learning-Metrics%2f" target=_blank rel=noopener aria-label="Share on Twitter"><i class="fab fa-twitter"></i>
</a><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fMachine-Learning-Metrics%2f" target=_blank rel=noopener aria-label="Share on Facebook"><i class="fab fa-facebook"></i>
</a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fMachine-Learning-Metrics%2f&title=Machine%20Learning%20Metrics" target=_blank rel=noopener aria-label="Share on LinkedIn"><i class="fab fa-linkedin"></i>
</a><a href="https://www.reddit.com/submit?url=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fMachine-Learning-Metrics%2f&title=Machine%20Learning%20Metrics" target=_blank rel=noopener aria-label="Share on Reddit"><i class="fab fa-reddit"></i>
</a><a href="mailto:?subject=Machine%20Learning%20Metrics&body=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fMachine-Learning-Metrics%2f" aria-label="Share via Email"><i class="fas fa-envelope"></i></a></div></ul></div><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href=../../dsblog/rps-Pretrained-Language-Models-for-Text-Generation/ aria-label="Previous page"><div class=td-pager__meta><i class="fa-solid fa-angle-left"></i>
<span class=td-pager__meta-label><b>Previous:</b></span>
<span class=td-pager__meta-title>Paper-Summary- A Survey Paper# Pretrained Language Models for Text Generation</span></div></a><a class="td-pager__link td-pager__link--next" href=../../dsblog/Comprehensive-Glossary-of-LLM/ aria-label="Next page"><div class=td-pager__meta><span class=td-pager__meta-label><b>Next:</b></span>
<span class=td-pager__meta-title>Comprehensive Glossary of LLM, Deep Learning, NLP, and CV Terminology</span>
<i class="fa-solid fa-angle-right"></i></div></a></ul></div><div class=col-md-2></div></div></main><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class=col-2><a href=https://dasarpai.com target=_blank rel=noopener><img src=https://dasarpai.github.io/assets/images/site-logo.png alt=dasarpAI width=100 style=border-radius:12px></a></div><div class=col-8><div class=row></div><div class=row></div></div><div class=col-2></div><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"></div></div></div></footer></div><script src=../../js/main.min.4e728f6d1777a74bef81c238fd7b573f42f1848a910503e07f877f49f1f442cb.js integrity="sha256-TnKPbRd3p0vvgcI4/XtXP0LxhIqRBQPgf4d/SfH0Qss=" crossorigin=anonymous></script><script defer src=../../js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=../../js/tabpane-persist.js></script><script src=https://dasarpai.github.io/js/asciinema-player.js></script><script>(function(){var e=document.querySelector("#td-section-nav");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></body></html>