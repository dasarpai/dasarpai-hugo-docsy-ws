<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.147.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href="https://localhost:1313/favicons/favicon.ico?v=1"><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-16x16.png?v=1" sizes=16x16><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-32x32.png?v=1" sizes=32x32><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><title>Understanding the Working of CNN | Blowfish</title><meta property="og:url" content="https://localhost:1313/dsblog/understanding-working-of-cnn/">
<meta property="og:site_name" content="Blowfish"><meta property="og:title" content="Understanding the Working of CNN"><meta property="og:description" content="Understanding the Working of CNN In this article, we aim to delve deeper into the working of CNNs. This article is intended for readers who have a basic understanding of CNNs and have computation-related questions. If you have any other questions about CNNs, feel free to ask in the comments."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="dsblog"><meta property="article:published_time" content="2025-01-29T00:00:00+00:00"><meta property="article:modified_time" content="2025-01-29T00:00:00+00:00"><meta property="article:tag" content="CNN"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="AI"><meta itemprop=name content="Understanding the Working of CNN"><meta itemprop=description content="Understanding the Working of CNN In this article, we aim to delve deeper into the working of CNNs. This article is intended for readers who have a basic understanding of CNNs and have computation-related questions. If you have any other questions about CNNs, feel free to ask in the comments."><meta itemprop=datePublished content="2025-01-29T00:00:00+00:00"><meta itemprop=dateModified content="2025-01-29T00:00:00+00:00"><meta itemprop=wordCount content="3758"><meta itemprop=keywords content="Understanding Convolutional Neural Networks,How CNN Works,Working of Convolutional Neural Networks,How CNN is used in AI,Convolutional Neural Networks in AI,How CNN is used in Deep Learning,Working of Convolutional Neural Networks in Deep Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Understanding the Working of CNN"><meta name=twitter:description content="Understanding the Working of CNN In this article, we aim to delve deeper into the working of CNNs. This article is intended for readers who have a basic understanding of CNNs and have computation-related questions. If you have any other questions about CNNs, feel free to ask in the comments."><link rel=preload href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css as=style integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css rel=stylesheet integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link rel=stylesheet type=text/css href=https://localhost:1313/css/asciinema-player.css><script src=https://code.jquery.com/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><link rel=stylesheet href=https://localhost:1313/css/custom.css><script src=https://localhost:1313/js/lunr.js></script></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-light nav-shadow flex-column flex-md-row td-navbar"><a id=agones-top class=navbar-brand href=https://localhost:1313/><svg id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg> <span class="text-uppercase fw-bold">Blowfish</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=https://localhost:1313/><span class=active>Data Science</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Docs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/community/><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/shortcodes/><span>Short Codes</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/blog/><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://github.com/nunocoracao/blowfish><span></span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href>GitHub</a></li><li class="nav-item dropdown d-none d-lg-block"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Release</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://development.agones.dev>Development</a>
<a class=dropdown-item href=https://agones.dev></a><a class=dropdown-item href=https://1-47-0.agones.dev>1.47.0</a>
<a class=dropdown-item href=https://1-46-0.agones.dev>1.46.0</a>
<a class=dropdown-item href=https://1-45-0.agones.dev>1.45.0</a>
<a class=dropdown-item href=https://1-44-0.agones.dev>1.44.0</a>
<a class=dropdown-item href=https://1-43-0.agones.dev>1.43.0</a>
<a class=dropdown-item href=https://1-42-0.agones.dev>1.42.0</a>
<a class=dropdown-item href=https://1-41-0.agones.dev>1.41.0</a>
<a class=dropdown-item href=https://1-40-0.agones.dev>1.40.0</a>
<a class=dropdown-item href=https://1-39-0.agones.dev>1.39.0</a>
<a class=dropdown-item href=https://1-38-0.agones.dev>1.38.0</a>
<a class=dropdown-item href=https://1-37-0.agones.dev>1.37.0</a>
<a class=dropdown-item href=https://1-36-0.agones.dev>1.36.0</a>
<a class=dropdown-item href=https://1-35-0.agones.dev>1.35.0</a>
<a class=dropdown-item href=https://1-34-0.agones.dev>1.34.0</a>
<a class=dropdown-item href=https://1-33-0.agones.dev>1.33.0</a>
<a class=dropdown-item href=https://1-32-0.agones.dev>1.32.0</a>
<a class=dropdown-item href=https://1-31-0.agones.dev>1.31.0</a></div></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><ul class=dropdown-menu><li><a class=dropdown-item href=https://localhost:1313/it/>Italiano</a></li></ul></div></li></ul></div><div class="navbar-nav mx-lg-2 d-none d-lg-block"></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><div class=row><div class=col-md-2></div><div class=col-md-8><p><img src=https://localhost:1313/assets/images/dspost/dsp6213-Understanding-Working-of-CNN.jpg alt="Understanding the Working of CNN"></p><h1 id=understanding-the-working-of-cnn>Understanding the Working of CNN<a class=td-heading-self-link href=#understanding-the-working-of-cnn aria-label="Heading self-link"></a></h1><p>In this article, we aim to delve deeper into the working of CNNs. This article is intended for readers who have a basic understanding of CNNs and have computation-related questions. If you have any other questions about CNNs, feel free to ask in the comments.</p><p><strong>Questions we are looking into.</strong></p><ul><li>What is the meaning of convolution in neural network?</li><li>If there is some convolution layer with 64 kernel (filter) and filter size is 3x3 then does the filter get updated during training process?</li><li>I heard filters has only 0 and 1 value. Depending upon what we want to extract we use the pattern of 0 and 1 on the filter, like for edge detection, contras detection etc.</li><li>If a layer with 64 filter has 3x3 filter then how many weights are there?</li><li>There is very famous 1x1 filter. How many weights are there if it layer has 64 neuron? Why it is more effective?</li><li>Normally we think channel means number of layer in input image (RGB color). How come we can have 256 channels in neural network?</li><li>How to calculate output size of convolutional layer?</li><li>When 3x3x3 filter is applied to 224x224x3 image then how it become 224x224?</li><li>Earlier we discussed weight of each layer R, G, B is different? When and how these weights are decided?</li><li>Where do we learn features? At the level of differet layers or different channels (filter)</li></ul><h2 id=what-is-the-meaning-of-convolution-in-neural-network>What is the meaning of convolution in neural network?<a class=td-heading-self-link href=#what-is-the-meaning-of-convolution-in-neural-network aria-label="Heading self-link"></a></h2><p>In the context of neural networks, specifically Convolutional Neural Networks (CNNs), <em>convolution</em> refers to a mathematical operation that combines two functions (or datasets) to produce a third function, typically used to extract features from input data. In simple terms, it’s a way of applying a filter or kernel to an input (like an image) to create a feature map, which highlights important patterns or features such as edges, textures, or shapes.</p><p>Here’s a breakdown of the process:</p><ol><li><strong>Filter/Kernels</strong>: A small matrix (filter or kernel) is applied over the input data (e.g., an image). The filter contains a set of weights.</li><li><strong>Sliding Window</strong>: The filter slides over the input image (or data) in steps, typically starting from the top-left corner. At each position, the filter performs an element-wise multiplication with the corresponding portion of the input, and the results are summed up to produce a single value.</li><li><strong>Feature Map</strong>: This process results in a new matrix called the feature map, which represents the features detected in the input image.</li></ol><p>The main goal of convolution in CNNs is to reduce the spatial dimensions of the input (through pooling layers) while preserving important features, allowing the network to focus on relevant patterns and improve efficiency for tasks like image classification, object detection, and more.</p><p>In summary, convolution helps the neural network understand and capture local patterns in the input data, making it particularly powerful for tasks involving visual information.</p><h2 id=if-there-is-some-convolution-layer-with-64-kernel-filter-and-filter-size-is-3x3-then-does-the-filter-get-updated-during-training-process>If there is some convolution layer with 64 kernel (filter) and filter size is 3x3 then does the filter get updated during training process?<a class=td-heading-self-link href=#if-there-is-some-convolution-layer-with-64-kernel-filter-and-filter-size-is-3x3-then-does-the-filter-get-updated-during-training-process aria-label="Heading self-link"></a></h2><p>Yes, the <strong>filters (kernels) in a convolutional layer are updated during training</strong> through <strong>backpropagation</strong> and <strong>gradient descent</strong>. They learn to detect meaningful patterns (e.g., edges, textures, shapes) by adjusting their weights through backpropagation.</p><ol><li><p><strong>Convolution Layer Setup</strong></p><ul><li>If you have a convolutional layer with <strong>64 filters</strong> (or neurons), and each filter is of size <strong>3×3</strong>, then each filter has a set of weights (parameters).</li><li>These filters slide over the input feature maps, performing convolution operations and generating activation maps.</li></ul></li><li><p><strong>Weight Updates During Training</strong></p><ul><li>Each <strong>filter (3×3)</strong> has <strong>weights</strong> (along with biases).</li><li>During <strong>forward propagation</strong>, these filters extract features by performing element-wise multiplication with the input and summing up the results.</li><li>During <strong>backpropagation</strong>, the loss (error) is computed, and gradients of the loss with respect to the filter weights are calculated using the <strong>chain rule</strong>.</li><li>Using <strong>gradient descent (or any optimization algorithm like Adam, SGD, RMSProp, etc.)</strong>, the filter weights are updated in order to minimize the loss.</li></ul></li></ol><h2 id=i-heard-filters-has-only-0-and-1-value-depending-upon-what-we-want-to-extract-we-use-the-pattern-of-0-and-1-on-the-filter-like-for-edge-detection-contras-detection-etc>I heard filters has only 0 and 1 value. Depending upon what we want to extract we use the pattern of 0 and 1 on the filter, like for edge detection, contras detection etc.<a class=td-heading-self-link href=#i-heard-filters-has-only-0-and-1-value-depending-upon-what-we-want-to-extract-we-use-the-pattern-of-0-and-1-on-the-filter-like-for-edge-detection-contras-detection-etc aria-label="Heading self-link"></a></h2><p>That&rsquo;s a common misconception! You&rsquo;re referring to <strong>handcrafted filters</strong> used in traditional image processing, like <strong>Sobel, Prewitt, or Laplacian filters</strong> for edge detection, which have fixed values (e.g., 0s and 1s).</p><p>However, in <strong>deep learning (CNNs)</strong>, filters (kernels) are <strong>learned automatically during training</strong>, and their values are not just 0s and 1s. Instead, they are real-valued numbers (floating-point weights) that get updated through <strong>backpropagation</strong>.</p><hr><p><strong>Key Differences: Traditional Filters vs. CNN Filters</strong></p><table><thead><tr><th>Feature</th><th>Traditional Filters (Fixed)</th><th>CNN Filters (Learned)</th></tr></thead><tbody><tr><td>Values</td><td>Manually set (e.g., 0s, 1s, -1s)</td><td>Real-valued, learned from data</td></tr><tr><td>Purpose</td><td>Predefined for edge/contrast detection</td><td>Learn features specific to the dataset</td></tr><tr><td>Training Needed?</td><td>No training needed (fixed)</td><td>Updated using backpropagation</td></tr><tr><td>Examples</td><td>Sobel, Laplacian, Gabor filters</td><td>Learned filters in convolutional layers</td></tr></tbody></table><hr><p><strong>How CNN Filters Work?</strong></p><ol><li><p><strong>Initialization:</strong></p><ul><li>Filters start with random values (not just 0s and 1s).</li></ul></li><li><p><strong>Learning Through Training:</strong></p><ul><li>CNNs automatically adjust the filter values to detect relevant patterns (edges, textures, objects, etc.).</li><li>Filters in the first layers detect simple patterns (edges, corners), while deeper layers detect complex structures (faces, objects).</li></ul></li><li><p><strong>Backpropagation Updates Filters:</strong></p><ul><li>During training, the loss function calculates the error.</li><li>Gradients of the loss with respect to each filter are computed.</li><li>Using an optimizer (e.g., SGD, Adam), the filter values are updated to improve accuracy.</li></ul></li></ol><h2 id=if-a-layer-with-64-filter-has-3x3-filter-then-how-many-weights-are-there>If a layer with 64 filter has 3x3 filter then how many weights are there?<a class=td-heading-self-link href=#if-a-layer-with-64-filter-has-3x3-filter-then-how-many-weights-are-there aria-label="Heading self-link"></a></h2><p><strong>Understanding the Parameters in a CNN Layer</strong><br>If a convolutional layer has <strong>64 filters</strong> (or neurons), and each filter is of size <strong>3×3</strong>, the number of trainable weights depends on the <strong>number of input channels</strong> (depth of the input feature map).</p><p><strong>Case 1: Single-Channel Input (Grayscale)</strong>
If the input to this layer is <strong>grayscale</strong> (i.e., it has only 1 channel), each filter has:</p><p>3 x 3 = 9 (weights per filter)</p><p>Since there are <strong>64 filters</strong>, the total number of weights is:</p><p>64 x 9 = 576
Additionally, each filter has <strong>one bias term</strong>, so the total trainable parameters are:</p><p>64 x (9 + 1) = 640</p><p><strong>Case 2: Multi-Channel Input (e.g., RGB Image)</strong>
If the input has <strong>multiple channels</strong>, such as an <strong>RGB image with 3 channels (R, G, B)</strong>, each filter must process all channels. So, each filter has:</p><p>3 x 3 x 3 = 27 (weights per filter)}</p><p>With <strong>64 filters</strong>, the total number of trainable weights is:</p><p>64 x 27 = 1,728</p><p>Including the bias terms (one per filter):</p><p>64 x (27 + 1) = 1,792</p><p><strong>General Formula for Convolutional Layer Parameters</strong>
For a convolutional layer with:</p><ul><li><strong>F</strong> filters (neurons)</li><li><strong>K × K</strong> filter size</li><li><strong>C</strong> input channels</li><li><strong>1 bias per filter</strong></li></ul><p>The number of trainable parameters is:</p><p>F x (K x K x C + 1)</p><h2 id=there-is-very-famous-1x1-filter-how-many-weights-are-there-if-it-layer-has-64-neuron-why-it-is-more-effective>There is very famous 1x1 filter. How many weights are there if it layer has 64 neuron? Why it is more effective?<a class=td-heading-self-link href=#there-is-very-famous-1x1-filter-how-many-weights-are-there-if-it-layer-has-64-neuron-why-it-is-more-effective aria-label="Heading self-link"></a></h2><ul><li><strong>1×1 convolutions learn cross-channel interactions efficiently.</strong></li><li>They are <strong>used for dimensionality reduction</strong> (reducing channels) and <strong>feature transformation</strong>.</li><li>They <strong>do not capture spatial features</strong>, but when combined with <strong>3×3 or 5×5 convolutions</strong>, they improve efficiency dramatically</li></ul><p><strong>1×1 Convolution (Pointwise Convolution)</strong>
A <strong>1×1 filter</strong> is a convolutional kernel that operates on an entire input channel but <strong>does not consider neighboring spatial information</strong> like larger filters (e.g., 3×3 or 5×5).</p><p><strong>Number of Weights in a 1×1 Convolution Layer</strong>
If a convolutional layer has <strong>64 filters</strong> (neurons), and each filter is <strong>1×1</strong>, the number of trainable weights depends on the <strong>number of input channels (C)</strong>.</p><p><strong>Formula for Weights in a 1×1 Convolution:</strong></p><p>F x (K x K x C + 1)</p><p>where:</p><ul><li><strong>F</strong> = Number of filters (neurons) = <strong>64</strong></li><li><strong>K × K</strong> = <strong>1×1</strong> filter size</li><li><strong>C</strong> = Number of input channels</li><li><strong>+1</strong> accounts for the bias term per filter</li></ul><p><strong>Case 1: Single-Channel Input (Grayscale)</strong>
For a <strong>grayscale image (1 channel)</strong>:</p><p>64 x (1 x 1 x 1 + 1) = 64 x 2 = 128 parameters</p><p><strong>Case 2: Multi-Channel Input (RGB, C=3)</strong>
For an <strong>RGB image (3 channels)</strong>:</p><p>64 x (1 x 1 x 3 + 1) = 64 x 4 = 256 parameters</p><p><strong>Case 3: Multi-Channel - Interim Layers</strong>
For an input with <strong>C=256 channels</strong>, a <strong>1×1 convolution with 64 filters</strong> would have:</p><p>64 x (1 x 1 x 256 + 1) = 16,448 parameters</p><hr><p><strong>Why is 1×1 Convolution Effective?</strong>
Despite its simplicity, <strong>1×1 convolutions are extremely powerful</strong> and are widely used in deep learning architectures like <strong>GoogleNet (Inception), MobileNet, and ResNet</strong> for multiple reasons:</p><ol><li><p><strong>Dimensionality Reduction (Bottleneck Layers)</strong></p><ul><li>A <strong>1×1 convolution reduces the number of channels</strong> (feature maps), thus reducing computational cost.</li><li>Example: If an input has <strong>256 channels</strong>, applying a <strong>1×1 convolution with 64 filters</strong> reduces it to <strong>64 channels</strong>.</li><li>This significantly <strong>reduces model size and computation</strong>.</li></ul></li><li><p><strong>Feature Transformation</strong></p><ul><li>Even though it <strong>doesn’t change spatial dimensions</strong>, it <strong>learns new feature representations</strong> by linearly combining different channels.</li><li>It acts as a <strong>fully connected layer applied independently to each pixel</strong>.</li></ul></li><li><p><strong>Efficient Depthwise Computation (Depthwise Separable Convolutions)</strong></p><ul><li>Used in architectures like <strong>MobileNet</strong> to replace expensive 3×3 convolutions.</li><li>Instead of using a large <strong>3×3×256</strong> kernel (which has <strong>2,304</strong> weights per filter), a <strong>depthwise 3×3 convolution</strong> + <strong>1×1 convolution</strong> achieves the same effect with <strong>far fewer parameters</strong>.</li></ul></li><li><p><strong>Non-Linearity Enhancement (ReLU After 1×1)</strong></p><ul><li>Typically, after a <strong>1×1 convolution</strong>, a <strong>non-linearity (ReLU)</strong> is applied.</li><li>This helps the network learn <strong>more complex transformations</strong> while keeping computation low.</li></ul></li></ol><hr><p><strong>Comparison: 1×1 vs 3×3 Convolutions</strong></p><table><thead><tr><th>Feature</th><th>1×1 Convolution</th><th>3×3 Convolution</th></tr></thead><tbody><tr><td>Spatial Context</td><td>No spatial info</td><td>Captures local patterns</td></tr><tr><td>Parameters (per filter)</td><td>C+1</td><td>9C+1</td></tr><tr><td>Computational Cost</td><td>Low</td><td>High</td></tr><tr><td>Feature Mixing</td><td>Yes (across channels)</td><td>Yes (across spatial locations)</td></tr><tr><td>Use Case</td><td>Bottlenecks, depth reduction</td><td>Feature extraction</td></tr></tbody></table><h2 id=normally-we-think-channel-means-number-of-layer-in-input-image-rgb-color-how-come-we-can-have-256-channels-in-neural-network>Normally we think channel means number of layer in input image (RGB color). How come we can have 256 channels in neural network?<a class=td-heading-self-link href=#normally-we-think-channel-means-number-of-layer-in-input-image-rgb-color-how-come-we-can-have-256-channels-in-neural-network aria-label="Heading self-link"></a></h2><p>The concept of <strong>&ldquo;channels&rdquo;</strong> in convolutional layers extends beyond just color channels like <strong>RGB</strong>. Let’s understand this.</p><p><strong>1. Understanding Channels in CNNs</strong></p><ul><li>In the <strong>input layer</strong>, channels refer to the number of color layers (e.g., <strong>RGB = 3 channels</strong>).</li><li>However, <strong>inside convolutional layers</strong>, each filter learns a different feature map, and these become the new &ldquo;channels&rdquo; for the next layer.</li></ul><p><strong>2. How Do We Get 256 Channels?</strong>
Each convolutional layer has <strong>multiple filters</strong>, and each filter extracts <strong>different features</strong> from the input. The <strong>number of filters</strong> determines the number of <strong>output channels</strong> in that layer.</p><p><strong>Example: Expanding from RGB (3 channels)</strong></p><ol><li>Suppose we apply <strong>64 filters of size 3×3×3</strong> (since RGB has 3 input channels) in the <strong>first convolutional layer</strong>.<ul><li>Each filter extracts a feature from all 3 channels.</li><li>The output will have <strong>64 feature maps</strong>, meaning <strong>64 channels</strong>.</li></ul></li><li>If the <strong>next convolutional layer</strong> applies <strong>256 filters</strong> on those <strong>64-channel feature maps</strong>, the output will have <strong>256 channels</strong>.</li></ol><p><strong>3. What Do Extra Channels Represent?</strong>
Unlike <strong>RGB channels (red, green, blue), which are fixed</strong>, the additional <strong>channels in deeper layers are learned feature maps</strong>. These channels capture:</p><ul><li><strong>Edges, textures (early layers)</strong></li><li><strong>Shapes, patterns (middle layers)</strong></li><li><strong>High-level objects (deeper layers)</strong></li></ul><p>Each <strong>filter extracts a different aspect</strong> of the input, which is why more channels develop as the network gets deeper.</p><p><strong>4. Example in a CNN Architecture</strong></p><ul><li><strong>First Convolutional Layer (Processing RGB Image)</strong><ul><li><strong>Input</strong>: 224×224×3 (RGB image)</li><li><strong>Filters</strong>: 64 filters of 3×3×3</li><li><strong>Output</strong>: 224×224×64 (<strong>64 feature maps = 64 channels</strong>)</li></ul></li><li><strong>Second Convolutional Layer</strong><ul><li><strong>Input</strong>: 224×224×64 (previous output)</li><li><strong>Filters</strong>: 128 filters of 3×3×64</li><li><strong>Output</strong>: 224×224×128 (<strong>128 feature maps = 128 channels</strong>)</li></ul></li><li>If a later convolutional layer has <strong>256 filters</strong>, its output will have <strong>256 channels</strong>.</li></ul><p><strong>5. Why Do CNNs Use So Many Channels?</strong></p><ul><li><strong>Helps learn complex patterns</strong> → More channels mean the network can capture different types of information.</li><li><strong>Improves accuracy</strong> → More filters = better feature representation.</li><li><strong>Works well with deeper layers</strong> → As images go deeper in the network, low-level edges combine into high-level structures.</li></ul><h2 id=how-to-calculate-output-size-of-convolutional-layer>How to calculate output size of convolutional layer?<a class=td-heading-self-link href=#how-to-calculate-output-size-of-convolutional-layer aria-label="Heading self-link"></a></h2><p><strong>1. Understanding Convolution Layer Output Shape</strong>
A convolutional layer <strong>does not reduce the spatial dimensions</strong> (height & width) unless we apply <strong>stride > 1</strong> or <strong>pooling</strong>. Instead, it adds <strong>depth (channels)</strong> based on the number of filters used.</p><p><strong>Example: First Convolution Layer Processing an RGB Image</strong></p><ul><li><strong>Input Shape:</strong> 224 x 224 x 3 (Height × Width × Channels)</li><li><strong>Filters:</strong> Suppose we apply <strong>64 filters</strong> of size 3 x 3</li><li><strong>Stride:</strong> 1 (no downsampling)</li><li><strong>Padding:</strong> &ldquo;Same&rdquo; (output size remains unchanged)</li></ul><p>\text{Output Shape} = 224 x 224 x 64</p><p>✔ <strong>224 × 224</strong> → The spatial dimensions remain the same (since stride = 1).<br>✔ <strong>64 channels</strong> → Because we used 64 filters, each one creates a separate feature map.</p><p><strong>2. Why Isn&rsquo;t the Output Just 64?</strong>
You&rsquo;re probably thinking about a <strong>fully connected layer</strong>, where we get a single number per neuron.<br>However, <strong>a convolutional layer works differently</strong>:</p><ul><li>Each <strong>filter slides</strong> over the input and computes a <strong>feature map</strong>.</li><li>Since we use <strong>64 filters</strong>, we get <strong>64 feature maps</strong>, each of size <strong>224 × 224</strong>.</li><li>The output has <strong>spatial structure</strong>, not just 64 numbers.</li></ul><p>💡 <strong>Think of the output as a &ldquo;stack&rdquo; of 64 images (feature maps), each of size 224 × 224.</strong></p><p><strong>3. General Formula for Convolution Output Size</strong>
If an input has size ** H x W x C ** and we apply ** F filters** of size K x K with stride S and padding P , the output shape is:</p><p>[[[H - K + 2P]/S] + 1 x [[W - K + 2P]/S] + 1 ] x F</p><p>For our example:</p><ul><li>H = 224, W = 224, C = 3</li><li>K = 3, S = 1, P = 1 (same padding ensures output size remains 224 × 224)</li><li>F = 64 (number of filters)</li></ul><p>\text{Output Shape} = 224 x 224 x 64</p><p><strong>4. When Do We Get Just 64 Numbers?</strong>
If we want to reduce the <strong>spatial dimensions</strong>, we use:</p><ol><li><strong>Pooling (e.g., MaxPooling 2×2)</strong> → Reduces the size (e.g., from 224×224 to 112×112).</li><li><strong>Flattening</strong> → Converts the feature maps into a <strong>1D vector</strong>.</li><li><strong>Fully Connected (Dense) Layer</strong> → Outputs a single number per neuron.</li></ol><p>For example, in <strong>classification</strong>, the CNN eventually flattens feature maps and passes them through <strong>dense layers</strong>, resulting in <strong>64 or fewer numbers</strong> at the end.</p><hr><p><strong>Why Is Output 224 × 224 × 64?</strong>
✅ A <strong>convolutional layer outputs feature maps, not just a single number</strong>.<br>✅ Since we apply <strong>64 filters</strong>, we get <strong>64 feature maps</strong>, <strong>not just 64 values</strong>.<br>✅ The <strong>spatial size (224 × 224) remains the same</strong> if we use <strong>stride = 1</strong> and <strong>padding = same</strong>.</p><h2 id=when-3x3x3-filter-is-applied-to-224x224x3-image-then-how-it-become-224x224>When 3x3x3 filter is applied to 224x224x3 image then how it become 224x224?<a class=td-heading-self-link href=#when-3x3x3-filter-is-applied-to-224x224x3-image-then-how-it-become-224x224 aria-label="Heading self-link"></a></h2><p>✔ A <strong>3×3×3 filter applied to a 224×224×3 image produces a 224×224×1 feature map</strong>.<br>✔ When using <strong>64 filters, we get 224×224×64 output</strong>.<br>✔ The <strong>spatial dimensions remain 224×224</strong> because of <strong>stride=1 and &ldquo;same&rdquo; padding</strong>.<br>✔ The <strong>depth increases from 3 (RGB) to 64</strong> because each filter extracts different features.</p><p><strong>1. Understanding How a Filter Works in CNNs</strong>
In a convolutional layer:</p><ul><li>A filter (also called a kernel) slides over the <strong>spatial dimensions</strong> (height & width) of the input image.</li><li>It computes the <strong>dot product</strong> between the filter’s weights and the corresponding region of the image.</li><li>The depth of the filter <strong>must match the depth (number of channels) of the input</strong>.</li></ul><p><strong>2. How a 3×3×3 Filter Works</strong>
Consider an <strong>RGB image of size 224×224×3</strong> (Height × Width × Channels):</p><ol><li><p>The <strong>filter size is 3×3×3</strong>, meaning:</p><ul><li>It covers a <strong>3×3</strong> region of the image.</li><li>It extends across <strong>all 3 channels</strong> (Red, Green, Blue).</li><li>Each filter has <strong>3×3×3 = 27 weights</strong>.</li></ul></li><li><p>The <strong>convolution operation</strong>:</p><ul><li>The filter slides over the image <strong>one step (stride = 1) at a time</strong>.</li><li>At each position, it computes the weighted sum (dot product) of <strong>all 27 values</strong> (from 3×3 pixels across 3 channels).</li><li>This produces <strong>a single number per position</strong>.</li><li>The filter moves across the <strong>entire</strong> 224×224 spatial area.</li></ul></li><li><p>Since the filter moves <strong>one step at a time (stride = 1) and we use &ldquo;same&rdquo; padding</strong>, the output <strong>retains the same spatial size</strong> but with <strong>only one channel</strong>: Output size = 224 x 224 x 1</p><p>(i.e., one feature map).</p></li></ol><p><strong>3. What Happens When We Use 64 Filters?</strong></p><ul><li>If we apply <strong>64 different 3×3×3 filters</strong>, each filter extracts a different feature from the image.</li><li>Each filter generates <strong>one 224×224 feature map</strong>.</li><li>Since we have <strong>64 filters</strong>, we get <strong>64 feature maps</strong>.</li></ul><p>Thus, the final output shape of this convolutional layer is: 224 x 224 x 64</p><p><strong>4. General Formula for Convolution Output Size</strong>
If an input image has:</p><ul><li><strong>Size:</strong> H x W x C (Height × Width × Channels)</li><li><strong>Filter size:</strong> K x K x C (Kernel size × Kernel size × Same depth as input)</li><li><strong>Stride:</strong> S</li><li><strong>Padding:</strong> P</li></ul><p>Then the output size is:</p><p>$$\left( \frac{H - K + 2P}{S} + 1 \right) x \left( \frac{W - K + 2P}{S} + 1 \right) x F$$</p><p>where:</p><ul><li><strong>F</strong> = number of filters</li></ul><hr><p><strong>5. Example Calculation for Our Case</strong>
Given:</p><ul><li><strong>Input:</strong> 224 x 224 x 3</li><li><strong>Filter:</strong> 3 x 3 x 3</li><li><strong>Stride:</strong> S = 1</li><li><strong>Padding:</strong> &ldquo;Same&rdquo; (so that output size remains unchanged)</li></ul><p>Using the formula:</p><p>$$\left( \frac{224 - 3 + 2(1)}{1} + 1 \right) x \left( \frac{224 - 3 + 2(1)}{1} + 1 \right) x 64$$</p><p>= 224 x 224 x 64</p><p>Thus, the output retains the <strong>same height and width</strong> but increases in <strong>depth (number of filters applied)</strong>.</p><hr><p><strong>6. Why Does Depth Increase?</strong>
Each filter extracts a different feature from the image:</p><ul><li><strong>Some filters detect edges</strong>.</li><li><strong>Some detect textures</strong>.</li><li><strong>Some detect patterns like curves, shapes, or corners</strong>.</li></ul><p>Instead of just <strong>3 channels (RGB), now we have 64 feature maps</strong>, allowing the CNN to learn more <strong>complex patterns</strong>.</p><h2 id=earlier-we-discussed-weight-of-each-layer-r-g-b-is-different-when-and-how-these-weights-are-decided>Earlier we discussed weight of each layer R, G, B is different? When and how these weights are decided?<a class=td-heading-self-link href=#earlier-we-discussed-weight-of-each-layer-r-g-b-is-different-when-and-how-these-weights-are-decided aria-label="Heading self-link"></a></h2><p>✔ <strong>Each filter has different weights for each channel (R, G, B).</strong><br>✔ <strong>Weights are randomly initialized and learned during training using backpropagation.</strong><br>✔ <strong>The dot product computes the weighted sum of pixel values in all channels.</strong><br>✔ <strong>Different filters learn to detect different features, from edges to complex objects.</strong></p><p>The <strong>weights for each channel (R, G, B) are different</strong>, and they are <strong>learned during training</strong>. Let&rsquo;s understand this.</p><p><strong>1. Understanding Weights in a Convolutional Filter</strong>
Each <strong>filter (kernel)</strong> in a convolutional layer has a set of <strong>learnable weights</strong>.<br>For a <strong>3×3×3 filter</strong>, there are:</p><ul><li><strong>9 weights per channel</strong> (since the filter is 3×3).</li><li><strong>3 separate weight matrices</strong> (one for each channel: R, G, B).</li><li><strong>1 bias term</strong> (optional, but usually present).</li></ul><p>Total trainable parameters for one filter:</p><p>(3 x 3 x 3) + 1 = 27 + 1 = 28</p><p>Since <strong>each filter has different weights for R, G, and B channels</strong>, the convolution operation applies <strong>different transformations</strong> to each channel before summing them.</p><p><strong>2. How Is the Weighted Sum (Dot Product) Computed?</strong>
At each position where the filter slides over the image:</p><ol><li>The <strong>weights of the filter</strong> are multiplied with the corresponding pixel values.</li><li>The <strong>results for each channel are summed</strong> to get a single value.</li><li>A <strong>bias term is added</strong> (if used).</li><li>A <strong>non-linearity (like ReLU)</strong> is applied.</li></ol><p>Example:</p><p>If a <strong>3×3×3 filter</strong> has weights: W_R, W_G, W_B</p><p>and the image patch under the filter has pixel values: I_R, I_G, I_B</p><p>Then the output at that position is:</p><p>output = I_R \cdot + I_G \cdot W_G + I_B \cdot W_B + bias</p><p>This results in <strong>a single number per filter position</strong>.</p><p><strong>3. When and How Are These Weights Decided?</strong>
<strong>(a) Initializing Weights</strong></p><ul><li>At the start of training, weights are <strong>randomly initialized</strong> (using techniques like Xavier or He initialization).</li><li>They are <strong>not manually set</strong>—they start as small random values.</li></ul><p><strong>(b) Learning Weights During Training</strong>
Weights are learned using <strong>backpropagation</strong> and <strong>gradient descent</strong>:</p><ol><li>The <strong>forward pass</strong> computes outputs using the current weights.</li><li>The <strong>loss function</strong> (e.g., cross-entropy for classification) measures how far the predictions are from the correct labels.</li><li><strong>Backpropagation</strong> calculates gradients of the loss <strong>with respect to the weights</strong>.</li><li><strong>Gradient descent (or Adam, RMSprop, etc.) updates the weights</strong> to minimize the loss.</li><li>Steps 1–4 repeat for many iterations (epochs) until the network converges.</li></ol><p><strong>(c) Different Filters Learn Different Features</strong></p><ul><li>In <strong>early layers</strong>, filters learn <strong>edges, corners, and simple textures</strong>.</li><li>In <strong>deeper layers</strong>, filters learn <strong>complex patterns like shapes, objects, and high-level features</strong>.</li></ul><p><strong>4. Why Are There Different Weights for R, G, B?</strong>
Each filter learns <strong>a different way to combine color information</strong>.</p><ul><li>Some filters might focus more on <strong>edges in the red channel</strong>.</li><li>Others might emphasize <strong>texture in the green channel</strong>.</li><li>Others may combine <strong>all three channels differently</strong> to detect complex patterns.</li></ul><p>This flexibility allows CNNs to <strong>extract meaningful features</strong> regardless of the color composition of the image.</p><h2 id=where-do-we-learn-features-at-the-level-of-differet-layers-or-different-channels-filter>Where do we learn features? At the level of differet layers or different channels (filter)<a class=td-heading-self-link href=#where-do-we-learn-features-at-the-level-of-differet-layers-or-different-channels-filter aria-label="Heading self-link"></a></h2><p>Earlier we discussed &ldquo;In early layers, filters learn edges, corners, and simple textures.&rdquo; The we said &ldquo;If we apply 64 different 3×3×3 filters, each filter extracts a different feature from the image.&rdquo; Isn&rsquo;t contracting? Each filter extracts different feature and is represented by the channel (neuron) or each layer is learning different features?</p><p><strong>1. Each Filter Extracts a Different Feature (Per Layer)</strong>
When we apply <strong>64 different 3×3×3 filters</strong> in a <strong>single convolutional layer</strong>, each filter learns to detect <strong>a different low-level feature</strong>.</p><ul><li>One filter might detect <strong>horizontal edges</strong>.</li><li>Another might detect <strong>vertical edges</strong>.</li><li>Another might respond to <strong>small texture patterns</strong>.</li><li>Each of these filters produces a <strong>separate feature map (channel)</strong>.</li></ul><p>✔ <strong>Each filter learns a different feature in that layer.</strong></p><hr><p><strong>2. Deeper Layers Learn More Complex Features</strong>
CNNs are hierarchical:</p><ul><li><strong>Early layers</strong> learn simple features (edges, corners, textures).</li><li><strong>Middle layers</strong> learn more abstract features (patterns, shapes).</li><li><strong>Deeper layers</strong> learn high-level structures (eyes, faces, objects).</li></ul><p>✔ <strong>Each layer captures different types of features.</strong></p><p><strong>3. How Does This Work Together?</strong></p><ul><li>In <strong>Layer 1</strong>, each filter detects a different <strong>low-level feature</strong> (edges, textures).</li><li>In <strong>Layer 2</strong>, filters combine these edges/textures into <strong>shapes and patterns</strong>.</li><li>In <strong>Layer 3+,</strong> filters detect <strong>high-level structures</strong> like objects.</li></ul><p>💡 <strong>Each filter within a layer extracts a different feature, and each deeper layer extracts more abstract features.</strong></p><p><strong>4. Analogy: Detecting a Face</strong>
Imagine detecting a face in an image:</p><ul><li><strong>Layer 1:</strong> Detects edges (nose outline, eye corners).</li><li><strong>Layer 2:</strong> Combines edges into <strong>shapes</strong> (eye, mouth, nose).</li><li><strong>Layer 3:</strong> Recognizes <strong>full facial structures</strong>.</li><li><strong>Final Layers:</strong> Identify specific faces (person A vs. person B).</li></ul><p><strong>5. Final Answer: No Contradiction!</strong>
✔ <strong>Each filter in a single layer extracts a different feature (edge, texture, shape).</strong><br>✔ <strong>Each deeper layer extracts progressively more complex features.</strong><br>✔ <strong>The number of filters = number of feature maps (channels) in that layer.</strong></p><div class=category-section><h4 class=category-section__title>Categories:</h4><div class=category-badges><a href=https://localhost:1313/categories/dsblog class=category-badge>dsblog</a></div></div><div class=td-tags><h4 class=td-tags__title>Tags:</h4><div class=category-badges><a href=https://localhost:1313/tags/cnn class=category-badge>CNN</a><a href=https://localhost:1313/tags/deep-learning class=category-badge>Deep Learning</a><a href=https://localhost:1313/tags/ai class=category-badge>AI</a></div></div><div class=td-author-box><div class=td-author-box__info><h4 class=td-author-box__name>Blowfish</h4><p class=td-author-box__bio>A powerful, lightweight theme for Hugo.</p><div class=td-author-box__links><a href target=_blank rel=noopener aria-label></a><a href target=_blank rel=noopener aria-label></a></div></div></div><div class=td-social-share><h4 class=td-social-share__title>Share this article:</h4><ul class=td-social-share__list><div class=social-share><a href="https://twitter.com/intent/tweet?text=Understanding%20the%20Working%20of%20CNN&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2funderstanding-working-of-cnn%2f" target=_blank rel=noopener aria-label="Share on Twitter"><i class="fab fa-twitter"></i>
</a><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2flocalhost%3a1313%2fdsblog%2funderstanding-working-of-cnn%2f" target=_blank rel=noopener aria-label="Share on Facebook"><i class="fab fa-facebook"></i>
</a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2funderstanding-working-of-cnn%2f&title=Understanding%20the%20Working%20of%20CNN" target=_blank rel=noopener aria-label="Share on LinkedIn"><i class="fab fa-linkedin"></i>
</a><a href="https://www.reddit.com/submit?url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2funderstanding-working-of-cnn%2f&title=Understanding%20the%20Working%20of%20CNN" target=_blank rel=noopener aria-label="Share on Reddit"><i class="fab fa-reddit"></i>
</a><a href="mailto:?subject=Understanding%20the%20Working%20of%20CNN&body=https%3a%2f%2flocalhost%3a1313%2fdsblog%2funderstanding-working-of-cnn%2f" aria-label="Share via Email"><i class="fas fa-envelope"></i></a></div></ul></div><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href=https://localhost:1313/dsblog/power-of-chinese-ai-models/ aria-label="Previous page"><div class=td-pager__meta><i class="fa-solid fa-angle-left"></i>
<span class=td-pager__meta-label><b>Previous:</b></span>
<span class=td-pager__meta-title>Power of Chinese AI Models</span></div></a><a class="td-pager__link td-pager__link--next" href=https://localhost:1313/dsblog/understanding-contextual-embedding-in-transformers/ aria-label="Next page"><div class=td-pager__meta><span class=td-pager__meta-label><b>Next:</b></span>
<span class=td-pager__meta-title>Understanding Contextual Embedding in Transformers</span>
<i class="fa-solid fa-angle-right"></i></div></a></ul></div><div class=col-md-2></div></div></main><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class=col-2><a href=https://dasarpai.com target=_blank rel=noopener><img src=https://localhost:1313/assets/images/site-logo.png alt=dasarpAI width=100 style=border-radius:12px></a></div><div class=col-8><div class=row></div><div class=row></div></div><div class=col-2></div><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"></div></div></div></footer></div><script src=https://localhost:1313/js/main.min.7c31f98e62c36b0c9c834ce3f8260a0e21895dd6aa0773e81a64b104eae3b2e8.js integrity="sha256-fDH5jmLDawycg0zj+CYKDiGJXdaqB3PoGmSxBOrjsug=" crossorigin=anonymous></script><script defer src=https://localhost:1313/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=https://localhost:1313/js/tabpane-persist.js></script><script src=https://localhost:1313/js/asciinema-player.js></script><script>(function(){var e=document.querySelector("#td-section-nav");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></body></html>