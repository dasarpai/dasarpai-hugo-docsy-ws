<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.147.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href="https://localhost:1313/favicons/favicon.ico?v=1"><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-16x16.png?v=1" sizes=16x16><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-32x32.png?v=1" sizes=32x32><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><title>What is LLM | Blowfish</title><meta property="og:url" content="https://localhost:1313/dsblog/what-is-llm/">
<meta property="og:site_name" content="Blowfish"><meta property="og:title" content="What is LLM"><meta property="og:description" content="What is Large Language Model Introduction LLM stands for Large Language Model. It is a type of artificial intelligence (AI) model that is trained on a massive dataset of text and code. This allows LLMs to learn the statistical relationships between words and phrases, and to generate text that is similar to the text that they were trained on."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="dsblog"><meta property="article:published_time" content="2023-08-18T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-18T00:00:00+00:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="GPT"><meta property="article:tag" content="BERT"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="AI Models"><meta property="article:tag" content="Transformer Architecture"><meta itemprop=name content="What is LLM"><meta itemprop=description content="What is Large Language Model Introduction LLM stands for Large Language Model. It is a type of artificial intelligence (AI) model that is trained on a massive dataset of text and code. This allows LLMs to learn the statistical relationships between words and phrases, and to generate text that is similar to the text that they were trained on."><meta itemprop=datePublished content="2023-08-18T00:00:00+00:00"><meta itemprop=dateModified content="2023-08-18T00:00:00+00:00"><meta itemprop=wordCount content="4079"><meta itemprop=keywords content="Large Language Models,LLM Architecture,Transformer Models,GPT,BERT,Neural Networks,AI Language Processing,Natural Language Understanding"><meta name=twitter:card content="summary"><meta name=twitter:title content="What is LLM"><meta name=twitter:description content="What is Large Language Model Introduction LLM stands for Large Language Model. It is a type of artificial intelligence (AI) model that is trained on a massive dataset of text and code. This allows LLMs to learn the statistical relationships between words and phrases, and to generate text that is similar to the text that they were trained on."><link rel=preload href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css as=style integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css rel=stylesheet integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link rel=stylesheet type=text/css href=https://localhost:1313/css/asciinema-player.css><script src=https://code.jquery.com/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><link rel=stylesheet href=https://localhost:1313/css/custom.css><script src=https://localhost:1313/js/lunr.js></script></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-light nav-shadow flex-column flex-md-row td-navbar"><a id=agones-top class=navbar-brand href=https://localhost:1313/><svg id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg> <span class="text-uppercase fw-bold">Blowfish</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=https://localhost:1313/><span class=active>Data Science</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Docs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/community/><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/shortcodes/><span>Short Codes</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/blog/><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://github.com/nunocoracao/blowfish><span></span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href>GitHub</a></li><li class="nav-item dropdown d-none d-lg-block"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Release</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://development.agones.dev>Development</a>
<a class=dropdown-item href=https://agones.dev></a><a class=dropdown-item href=https://1-47-0.agones.dev>1.47.0</a>
<a class=dropdown-item href=https://1-46-0.agones.dev>1.46.0</a>
<a class=dropdown-item href=https://1-45-0.agones.dev>1.45.0</a>
<a class=dropdown-item href=https://1-44-0.agones.dev>1.44.0</a>
<a class=dropdown-item href=https://1-43-0.agones.dev>1.43.0</a>
<a class=dropdown-item href=https://1-42-0.agones.dev>1.42.0</a>
<a class=dropdown-item href=https://1-41-0.agones.dev>1.41.0</a>
<a class=dropdown-item href=https://1-40-0.agones.dev>1.40.0</a>
<a class=dropdown-item href=https://1-39-0.agones.dev>1.39.0</a>
<a class=dropdown-item href=https://1-38-0.agones.dev>1.38.0</a>
<a class=dropdown-item href=https://1-37-0.agones.dev>1.37.0</a>
<a class=dropdown-item href=https://1-36-0.agones.dev>1.36.0</a>
<a class=dropdown-item href=https://1-35-0.agones.dev>1.35.0</a>
<a class=dropdown-item href=https://1-34-0.agones.dev>1.34.0</a>
<a class=dropdown-item href=https://1-33-0.agones.dev>1.33.0</a>
<a class=dropdown-item href=https://1-32-0.agones.dev>1.32.0</a>
<a class=dropdown-item href=https://1-31-0.agones.dev>1.31.0</a></div></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><ul class=dropdown-menu><li><a class=dropdown-item href=https://localhost:1313/it/>Italiano</a></li></ul></div></li></ul></div><div class="navbar-nav mx-lg-2 d-none d-lg-block"></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><div class=row><div class=col-md-2></div><div class=col-md-8><p><img src=https://localhost:1313/assets/images/dspost/dsp6087-What-is-LLM.jpg alt="What is LLM"></p><h1 id=what-is-large-language-model>What is Large Language Model<a class=td-heading-self-link href=#what-is-large-language-model aria-label="Heading self-link"></a></h1><h2 id=introduction>Introduction<a class=td-heading-self-link href=#introduction aria-label="Heading self-link"></a></h2><p>LLM stands for <strong>Large Language Model</strong>. It is a type of artificial intelligence (AI) model that is trained on a massive dataset of text and code. This allows LLMs to learn the statistical relationships between words and phrases, and to generate text that is similar to the text that they were trained on.</p><p>LLMs are still under development, but they have already been shown to be capable of performing a wide variety of tasks:</p><ul><li><strong>Natural language understanding:</strong> LLMs can understand the meaning of text, summarise it, and translate it to other languages.</li><li><strong>Natural language generation:</strong> LLMs can generate text, translate languages, and write different kinds of creative content.</li><li><strong>Coding:</strong> LLMs can write and understand code.</li><li><strong>Commonsense reasoning:</strong> LLMs can reason about the world and answer questions about hypothetical situations.</li><li><strong>Conversational AI and chatbots</strong></li><li><strong>Classification and categorization</strong></li><li><strong>Sentiment analysis</strong></li></ul><p>LLMs are a powerful tool that can be used for a variety of tasks. They are still under development, but they have the potential to revolutionize the way we interact with computers.</p><h2 id=what-is-llm-model>What is LLM Model?<a class=td-heading-self-link href=#what-is-llm-model aria-label="Heading self-link"></a></h2><p>Large Language Models are a broader concept that encompasses various architectures and approaches for building models that can understand and generate human language text. These models are trained on massive amounts of text data to learn the statistical patterns, grammar, semantics, and context of language. They are capable of tasks like text completion, language translation, question answering, summarization, and more.</p><p>There is no clear definition or size limit when a model is called a large language model. It depends on the context and the comparison with other models. For example, some models that were considered large a few years ago may be considered small or medium today, as new models with more parameters and data are developed.</p><p>Important parameters that can be used to tell whether a model is LLM or not.</p><ul><li>The number of parameters: This is the number of weights or variables that the model can learn and adjust during training. The more parameters, the more complex and expressive the model can be, but also the more computationally expensive and prone to overfitting. For example, LSTM has about 100 million parameters, GRU has about 50 million parameters, ByteNet has about 200 million parameters, Switch Transformer has about 1.6 billion parameters, and GShard has about 600 billion parameters.</li><li>The number of layers: This is the number of processing units or modules that the model has. Each layer can perform a different function or operation on the input or output data. The more layers, the more hierarchical and abstract the model can be, but also the more difficult to train and optimize. For example, LSTM has 4 layers, GRU has 3 layers, ByteNet has 15 layers for each sub-network, Switch Transformer has 24 layers for each expert*, and GShard has 28 layers for each expert*.</li><li>The amount of data: This is the amount of text data that the model is trained on. The more data, the more diverse and generalizable the model can be, but also the more noisy and biased the data may be. For example, LSTM and GRU are trained on Wikipedia articles, ByteNet is trained on WMT datasets for machine translation, Switch Transformer is trained on C4 dataset for natural language understanding, and GShard is trained on WMT’14 En-De dataset for machine translation.</li></ul><p><strong>Expert:</strong> An expert is a person who has special skill or knowledge in a particular field or area of study. An expert is a neural network module that can process a subset of the input data and produce an output. Each expert has a different attention pattern or head configuration, which means that it can focus on different aspects of the input data. For example, one expert may pay more attention to the syntax of the input, while another expert may pay more attention to the semantics. <strong>Encoder expert</strong> can process a subset of the source tokens and produce an output vector for each token. <strong>Decoder expert</strong> can process a subset of the target tokens and produce an output vector for each token. The output vectors are then combined by a routing network to form the final decoder output. <strong>Attention expert</strong> can perform a different type of attention, such as self-attention, cross-attention, or global attention, on a subset of the input or output tokens. The output vectors are then combined by a routing network to form the final attention output.</p><p>According to the paper <strong>GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</strong>, the number of experts that GShard supports depends on the number of shards and the number of experts per shard. The paper reports experiments with different configurations of these parameters, such as:</p><ul><li>8 shards, each with 64 experts, for a total of 512 experts.</li><li>128 shards, each with 64 experts, for a total of 8192 experts.</li><li>256 shards, each with 256 experts, for a total of 65536 experts.</li></ul><p>Gopher (280B)
Jurassic-1 (178B)</p><h2 id=what-is-the-difference-between-llm-and-transformer-model>What is the difference between LLM and Transformer Model<a class=td-heading-self-link href=#what-is-the-difference-between-llm-and-transformer-model aria-label="Heading self-link"></a></h2><p>LLM model can be built either using Transformer Architecture or non-Tranformer architecture. When we use huge data to train a model then whatever model is output from the training process that is called LLM. When we are training a Transformer or any other neural network model only small amount of data then output model is not called LLM. LLM can work upon Text, Images, Video, Audio. Some LLMs can accept only text data for example GPT3, some can accept only with image data, some can accpet different kind of data i.e. text, image, audio for example GPT4. Some LLM takes text input and generate text output. Some LLM takes text input generate image output for example DALL-E.</p><h2 id=examples-of-transformer-based-llm>Examples of Transformer Based LLM<a class=td-heading-self-link href=#examples-of-transformer-based-llm aria-label="Heading self-link"></a></h2><p>Transformer architecture is highly parallelizable and can capture long-range dependencies in text, making it effective for tasks like machine translation, text generation, and more. The greatness of any LLM depends upon</p><ul><li>How less resources (compute, memory, and harddisk) they need for finetuning, inferencing on the downstream tasks.</li><li>How good those finetuned model performs.</li></ul><table><thead><tr><th>Model Name</th><th>Company Name</th><th>Parameters</th><th>Year of release</th><th>Model Capabilities</th></tr></thead><tbody><tr><td>LaMDA</td><td>Google</td><td>1.6 billion</td><td>2021</td><td>Conversational AI</td></tr><tr><td>Turing NLG 17B</td><td>NVIDIA and Microsoft Research</td><td>17 billion</td><td>2020</td><td>Natural Language Generation</td></tr><tr><td>Megatron NLG 530B (MT-NLG)</td><td>NVIDIA and Microsoft Research</td><td>530 billion</td><td>2021</td><td>Natural Language Generation</td></tr><tr><td>Bard (BART)</td><td>Facebook AI Research (FAIR) and University College London (UCL) researchers.</td><td>406 million</td><td>2019</td><td>Summarization, Translation, Question Answering</td></tr><tr><td>RoBERTa (Robustly Optimized BERT Pretraining Approach)</td><td>Facebook AI Research (FAIR), University of Washington and New York University.</td><td>355 million</td><td>2019</td><td>Natural Language Understanding</td></tr><tr><td>ALBERT (A Lite BERT)</td><td>Google Research Team.</td><td>12 million</td><td>2019</td><td>Natural Language Understanding</td></tr><tr><td>ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)</td><td>Google Research Team.</td><td>110 million</td><td>2020</td><td>Natural Language Understanding</td></tr><tr><td>FaluBert (Falu Language Model)</td><td>Falu AI team.</td><td>24 million</td><td>2020</td><td>Chinese language understanding</td></tr><tr><td>Gopher (Go Programming language HelpER) model</td><td>OpenAI team.</td><td>125 million</td><td>2020</td><td>Code generation for Go programming language</td></tr><tr><td>CodeBERT (Code BERT) model</td><td>Microsoft Research Asia team.</td><td>400 million</td><td>2020</td><td>Code generation for multiple programming languages</td></tr><tr><td>Chinchilla model</td><td>OpenAI team.</td><td>350 million</td><td>2020</td><td>Multilingual contextual representations</td></tr><tr><td>DialogGPT</td><td>Microsoft</td><td>345 million</td><td>2019</td><td>Open-domain conversational agent. Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017</td></tr></tbody></table><h2 id=example-of-pathway-based-llm>Example of Pathway Based LLM<a class=td-heading-self-link href=#example-of-pathway-based-llm aria-label="Heading self-link"></a></h2><ul><li><strong>PaLM:</strong> Developed by Google AI, PaLM is a large language model with 540 billion parameters. It is designed to be informative and comprehensive, and it can be used for a variety of tasks, including natural language understanding, natural language generation, and machine translation.</li></ul><h2 id=examples-of-rnn-based-llm>Examples of RNN Based LLM<a class=td-heading-self-link href=#examples-of-rnn-based-llm aria-label="Heading self-link"></a></h2><p>The transformer architecture is the most common architecture for LLMs, but it is not the only one. Other architectures can also be used to create LLMs, and they may have different strengths and weaknesses.</p><ul><li><p><strong>Meena:</strong> This is a conversational neural language model that is developed by Google AI. It is trained on a massive dataset of text and code, and it can engage in open-ended dialogue. Meena is not based on the transformer architecture, but it uses a different neural network architecture called the recurrent neural network (RNN). It has 1.37 billion parameters and can be used to generate different creative text formats, like poems, code, scripts, musical pieces, email, letters, etc.</p></li><li><p><strong>Seq2Seq:</strong> This is a sequence-to-sequence model that is used for machine translation. It is not based on the transformer architecture, but it uses a different neural network architecture called the recurrent neural network (RNN).</p></li></ul><h2 id=what-are-the-advantages-of-llms>What are the advantages of LLMs?<a class=td-heading-self-link href=#what-are-the-advantages-of-llms aria-label="Heading self-link"></a></h2><ul><li>Extensibility and adaptability. LLMs can serve as a foundation for customized use cases. Additional training on top of an LLM can create a finely tuned model for an organization&rsquo;s specific needs.</li><li>Flexibility. One LLM can be used for many different tasks and deployments across organizations, users and applications.</li><li>Performance. Modern LLMs are typically high-performing, with the ability to generate rapid, low-latency responses.</li><li>Accuracy. As the number of parameters and the volume of trained data grow in an LLM, the transformer model is able to deliver increasing levels of accuracy.</li><li>Ease of training. Many LLMs are trained on unlabeled data, which helps to accelerate the training process.</li></ul><h2 id=what-are-the-challenges-and-limitations-of-llm>What are the challenges and limitations of LLM?<a class=td-heading-self-link href=#what-are-the-challenges-and-limitations-of-llm aria-label="Heading self-link"></a></h2><ul><li>Development costs. To run, LLMs generally require large quantities of expensive graphics processing unit hardware and massive data sets.</li><li>Operational costs. After the training and development period, the cost of operating an LLM for the host organization can be very high.</li><li>Bias. A risk with any AI trained on unlabeled data is bias, as it&rsquo;s not always clear that known bias has been removed.</li><li>Explainability. The ability to explain how an LLM was able to generate a specific result is not easy or obvious for users.</li><li>Hallucination. AI hallucination occurs when an LLM provides an inaccurate response that is not based on trained data.</li><li>Complexity. With billions of parameters, modern LLMs are exceptionally complicated technologies that can be particularly complex to troubleshoot.</li><li>Glitch tokens. Maliciously designed prompts that cause an LLM to malfunction, known as glitch tokens, are part of an emerging trend since 2022.</li></ul><h2 id=what-are-different-types-of-llm>What are different types of LLM?<a class=td-heading-self-link href=#what-are-different-types-of-llm aria-label="Heading self-link"></a></h2><p>As of 2023 the common types are the following.</p><ul><li>Zero-shot model: This is a large, generalized model trained on a generic corpus of data that is able to give a fairly accurate result for general use cases, without the need for additional training. GPT-3 is often considered a zero-shot model.</li><li>One-shot model: Fine-tuned on domain-specific data with one sample.</li><li>Few-shot model: Fine-tuned on domain-specific data with less than 100 samples. Additional training on top of a zero-shot model like GPT-3 can lead to a fine-tuned models.</li><li>Language representation model. One example of a language representation model is Bidirectional Encoder Representations from Transformers (BERT), which makes use of deep learning and transformers well suited for NLP.</li><li>Multimodal model. Originally LLMs were specifically tuned just for text, but with the multimodal approach it is possible to handle both text and images. GPT4 is multimoda modell.</li></ul><h2 id=what-is-palm>What is PaLM?<a class=td-heading-self-link href=#what-is-palm aria-label="Heading self-link"></a></h2><p>PaLM stands for Pathways Language Model. It is a large language model (LLM) developed by Google AI. PaLM has 540 billion parameters, making it one of the largest LLMs in the world.</p><p>There are four different PaLM models:</p><ul><li><strong>Gecko:</strong> This is the smallest PaLM model, with 140 billion parameters. It is designed for use on mobile devices and embedded systems.
[Image of Gecko PaLM LLM model]</li><li><strong>Otter:</strong> This model has 280 billion parameters and is designed for use on laptops and desktops.
[Image of Otter PaLM LLM model]</li><li><strong>Bison:</strong> This is the largest PaLM model, with 540 billion parameters. It is designed for use on high-performance computing clusters.
[Image of Bison PaLM LLM model]</li><li><strong>Unicorn:</strong> This model is still under development, but it is expected to have 1.1 trillion parameters. It will be the largest PaLM model ever created.
[Image of Unicorn PaLM LLM model]</li></ul><p>PaLM is trained on a massive dataset of text and code, including books, articles, code repositories, and websites. This allows PaLM to perform a wide variety of tasks, including:</p><ul><li>Natural language understanding: PaLM can understand the meaning of text and translate languages.</li><li>Natural language generation: PaLM can generate text, translate languages, and write different kinds of creative content.</li><li>Coding: PaLM can write and understand code.</li><li>Commonsense reasoning: PaLM can reason about the world and answer questions about hypothetical situations.</li></ul><p>PaLM is still under development, but it has the potential to revolutionize the way we interact with computers. It could be used to create new applications for education, healthcare, and customer service. It could also be used to develop new AI-powered tools that help us to be more productive and creative.</p><h2 id=what-is-gpt3>What is GPT3?<a class=td-heading-self-link href=#what-is-gpt3 aria-label="Heading self-link"></a></h2><p>There are many different versions of the GPT-3 model, each with a different number of parameters and capabilities. Here are some of the most notable versions:</p><ul><li><strong>GPT-3-small:</strong> This is the smallest version of GPT-3, with 125 million parameters. It is designed for simple tasks and is relatively inexpensive to use.</li><li><strong>GPT-3-medium:</strong> This version has 1.3 billion parameters and is capable of more complex tasks. It is still relatively inexpensive to use.</li><li><strong>GPT-3-large:</strong> This version has 175 billion parameters and is the most powerful version of GPT-3. It is also the most expensive to use.</li><li><strong>GPT-3-XL:</strong> This version has 1.3 trillion parameters and is even more powerful than the GPT-3-large model. It is still under development, but it is expected to be released in the near future.</li></ul><p>The size of the GPT-3 models is measured in parameters. A parameter is a variable that is used to learn the relationship between inputs and outputs in a machine learning model. The more parameters a model has, the more complex the relationships it can learn.</p><p>The size of the GPT-3 models has increased over time as the technology has improved. The first version of GPT-3, released in 2020, had only 125 million parameters. The most recent version, GPT-3-XL, has 1.3 trillion parameters.</p><p>The size of the GPT-3 models has also had an impact on their capabilities. The larger models are capable of performing more complex tasks and generating more realistic text. However, they are also more expensive to use and require more computing power.</p><p>The different versions of the GPT-3 model are designed for different purposes. The smaller models are best suited for simple tasks, such as generating text or translating languages. The larger models are best suited for more complex tasks, such as writing creative content or answering questions in an informative way.</p><p>The GPT-3 models are a powerful tool that can be used for a variety of tasks. They are still under development, but they have the potential to revolutionize the way we interact with computers.</p><h2 id=what-is-gpt4>What is GPT4?<a class=td-heading-self-link href=#what-is-gpt4 aria-label="Heading self-link"></a></h2><p>There are two main versions of the GPT-4 model:</p><ul><li><strong>GPT-4 (8,192 tokens)</strong>: This is the standard version of GPT-4. It has 175 billion parameters and can process text prompts up to 8,192 tokens long.
[Image of GPT-4 (8,192 tokens) language model]</li><li><strong>GPT-4 (32,768 tokens)</strong>: This is the extended version of GPT-4. It has 1.3 trillion parameters and can process text prompts up to 32,768 tokens long.
[Image of GPT-4 (32,768 tokens) language model]</li></ul><p>The main difference between the two versions is the context length. The context length is the length of the prompt plus the maximum number of tokens in the completion. The GPT-4 (8,192 tokens) model can only process prompts that are up to 8,192 tokens long, while the GPT-4 (32,768 tokens) model can process prompts that are up to 32,768 tokens long.</p><p>The GPT-4 models are still under development, but they have already been shown to be capable of performing a wide variety of tasks, including:</p><ul><li>Natural language understanding: GPT-4 can understand the meaning of text and translate languages.</li><li>Natural language generation: GPT-4 can generate text, translate languages, and write different kinds of creative content.</li><li>Coding: GPT-4 can write and understand code.</li><li>Commonsense reasoning: GPT-4 can reason about the world and answer questions about hypothetical situations.</li></ul><p>The GPT-4 models are a significant step forward in the development of large language models. They are more powerful and versatile than previous models, and they have the potential to revolutionize the way we interact with computers.</p><h2 id=what-kind-of-hardware-cpu-gpu-ram-is-needed-to-train-llm>What kind of hardware (CPU, GPU, RAM) is needed to train LLM?<a class=td-heading-self-link href=#what-kind-of-hardware-cpu-gpu-ram-is-needed-to-train-llm aria-label="Heading self-link"></a></h2><p>The hardware requirements for inference with the GPT-3 Large model (175 billion parameters) are:</p><ul><li><strong>CPU:</strong> A minimum of 16 cores is recommended, but more cores will provide better performance.</li><li><strong>GPU:</strong> A minimum of 16 GB of VRAM is required, but more VRAM will provide better performance.</li><li><strong>RAM:</strong> A minimum of 32 GB of RAM is required, but more RAM will provide better performance.</li></ul><p>It is also important to have a fast storage system, such as an SSD, to store the model and the data that is being processed.</p><p>Here are some specific examples of hardware that you could use for inference with the GPT-3 Large model:</p><ul><li><strong>CPU:</strong> A server with 16 or more cores, such as the Intel Xeon Silver 4210 or the AMD EPYC 7571.</li><li><strong>GPU:</strong> A GPU with 16 GB of VRAM, such as the NVIDIA T4 or the AMD Radeon Pro VII.</li><li><strong>RAM:</strong> 32 GB of RAM.</li><li><strong>Storage:</strong> A fast SSD, such as the Samsung 980 Pro or the Western Digital SN850.</li></ul><p>It is important to note that these are just the minimum requirements for inference with the GPT-3 Large model. If you want to achieve the best possible performance, you will need to use more powerful hardware.</p><h2 id=hardware-for-inference-from-llm>Hardware for Inference from LLM<a class=td-heading-self-link href=#hardware-for-inference-from-llm aria-label="Heading self-link"></a></h2><p>Unfortunately, you cannot use the GPT-3 Medium model for inference without a GPU machine. The GPT-3 Medium model requires a minimum of 8 GB of VRAM, which is not available on most CPUs.</p><p>There are a few ways to get around this limitation. One way is to use a cloud computing service that provides GPU instances. For example, Google Cloud Platform offers a variety of GPU instances that you can use to run the GPT-3 Medium model.</p><p>Another way to get around this limitation is to use a smaller model. The GPT-3 Small model only requires 125 million parameters and can be run on a CPU machine. However, the GPT-3 Small model is not as powerful as the GPT-3 Medium model and cannot be used for the same tasks.</p><p>If you are looking for a way to use the GPT-3 Medium model without a GPU machine, your best option is to use a cloud computing service. This will allow you to get the performance you need without having to invest in a new machine.</p><h2 id=what-is-the-need-of-vram-for-llm>What is the Need of VRAM for LLM?<a class=td-heading-self-link href=#what-is-the-need-of-vram-for-llm aria-label="Heading self-link"></a></h2><p>VRAM stands for Video Random Access Memory. It is a type of memory that is used by graphics cards to store the data that is needed to render images on a display. VRAM is much faster than regular RAM, which is why it is used for graphics processing.</p><p>VRAM stores the image data that is being displayed on the screen, as well as the data that is being processed by the GPU. This data includes things like textures, shaders, and geometry. VRAM also stores the results of the GPU&rsquo;s calculations, which are then displayed on the screen.</p><p>The amount of VRAM that a graphics card has determines how much data it can store and process at once. This is important for games and other applications that require a lot of graphics processing. A graphics card with more VRAM will be able to handle more demanding applications without slowing down.</p><p>The amount of VRAM that you need depends on the type of games and applications that you want to use. For most games, 4GB of VRAM is sufficient. However, some more demanding games may require 6GB or more of VRAM. If you are a serious gamer or you want to use applications that require a lot of graphics processing, you should consider getting a graphics card with 8GB or more of VRAM.</p><p>Here are some of the benefits of using VRAM:</p><ul><li><strong>Faster rendering:</strong> VRAM is much faster than regular RAM, which means that graphics cards can render images much faster. This results in smoother gameplay and better graphics quality.</li><li><strong>More memory:</strong> VRAM can store more data than regular RAM, which allows graphics cards to process more complex models and textures. This is important for games and applications that require a lot of graphics processing.</li><li><strong>Less lag:</strong> VRAM can reduce lag by storing the data that is needed to render images on the GPU. This means that the CPU does not have to access the slower main memory as often, which can improve performance.</li></ul><p>Here are some of the drawbacks of using VRAM:</p><ul><li><strong>More expensive:</strong> VRAM is more expensive than regular RAM. This is because it is a specialized type of memory that is designed for graphics processing.</li><li><strong>Less accessible:</strong> VRAM is not as accessible as regular RAM. This is because it is located on the graphics card, which is a separate component from the motherboard.</li><li><strong>More power-hungry:</strong> VRAM is more power-hungry than regular RAM. This is because it is a faster type of memory.</li></ul><p>Overall, VRAM is an important part of a graphics card. It allows graphics cards to render images faster and smoother, and it can reduce lag. However, VRAM is more expensive and less accessible than regular RAM.</p><h2 id=can-i-use-virtual-memory-in-place-of-vram>Can I use Virtual Memory in place of VRAM?<a class=td-heading-self-link href=#can-i-use-virtual-memory-in-place-of-vram aria-label="Heading self-link"></a></h2><p>No. <strong>Virtual memory is a technique</strong> that allows each virtual machine to have its own dedicated memory space, even if there is not having enough physical RAM on the host machine. <strong>Virtual RAM is a portion of the hard drive</strong> that is used as temporary memory for virtual machines.</p><p>Virtual Memory or Vritual RAM is a technique that allows computers to use more memory than they actually have. This is done by using a portion of the hard drive as temporary memory. When the computer needs more memory than it has, it can swap data between the hard drive and the RAM.</p><p>Virtual RAM is also known as <strong>paging</strong> or <strong>virtual memory</strong>. It is a feature of most operating systems, including Windows, macOS, and Linux.</p><p>Virtual RAM is used to improve performance by allowing computers to run more programs at once. It can also be used to prevent programs from crashing when they run out of memory.</p><p>There are a few drawbacks to using virtual RAM. First, it can slow down the computer, especially if the hard drive is slow. Second, it can shorten the lifespan of the hard drive, because it is constantly being written to. Third, it can consume more power.</p><p>Overall, virtual RAM is a useful technique that can improve the performance of computers. However, it is important to be aware of the drawbacks before using it.</p><p>Here are some of the benefits of using virtual RAM:</p><ul><li><strong>Allows computers to run more programs at once:</strong> Virtual RAM allows computers to run more programs at once by expanding the amount of available memory. This can be helpful for users who like to multitask or who have a lot of programs open at the same time.</li><li><strong>Prevents programs from crashing:</strong> Virtual RAM can help to prevent programs from crashing by providing a temporary space for data when the computer runs out of physical RAM. This can be helpful for users who are running memory-intensive programs or who have a lot of programs open at the same time.</li><li><strong>Improves performance:</strong> Virtual RAM can improve the performance of computers by reducing the amount of time that the CPU has to wait for data to be loaded from the hard drive. This can be helpful for users who are running multiple programs or who are playing games.</li></ul><p>Here are some of the drawbacks of using virtual RAM:</p><ul><li><strong>Slows down the computer:</strong> Virtual RAM can slow down the computer, especially if the hard drive is slow. This is because the data that is stored in virtual RAM has to be swapped back and forth between the hard drive and the RAM.</li><li><strong>Shortens the lifespan of the hard drive:</strong> Virtual RAM can shorten the lifespan of the hard drive, because it is constantly being written to. This is because the data that is stored in virtual RAM has to be written to the hard drive every time it is changed.</li><li><strong>Consumes more power:</strong> Virtual RAM can consume more power than physical RAM. This is because the hard drive has to be accessed more often when virtual RAM is used.</li></ul><p>Overall, virtual RAM is a useful technique that can improve the performance of computers. However, it cannot be used in place of VRAM.</p><div class=category-section><h4 class=category-section__title>Categories:</h4><div class=category-badges><a href=https://localhost:1313/categories/dsblog class=category-badge>dsblog</a></div></div><div class=td-tags><h4 class=td-tags__title>Tags:</h4><div class=category-badges><a href=https://localhost:1313/tags/llm class=category-badge>LLM</a><a href=https://localhost:1313/tags/gpt class=category-badge>GPT</a><a href=https://localhost:1313/tags/bert class=category-badge>BERT</a><a href=https://localhost:1313/tags/natural-language-processing class=category-badge>Natural Language Processing</a><a href=https://localhost:1313/tags/ai-models class=category-badge>AI Models</a><a href=https://localhost:1313/tags/transformer-architecture class=category-badge>Transformer Architecture</a><a href=https://localhost:1313/tags/deep-learning class=category-badge>Deep Learning</a><a href=https://localhost:1313/tags/language-models class=category-badge>Language Models</a><a href=https://localhost:1313/tags/machine-learning class=category-badge>Machine Learning</a></div></div><div class=td-author-box><div class=td-author-box__info><h4 class=td-author-box__name>Blowfish</h4><p class=td-author-box__bio>A powerful, lightweight theme for Hugo.</p><div class=td-author-box__links><a href target=_blank rel=noopener aria-label></a><a href target=_blank rel=noopener aria-label></a></div></div></div><div class=td-social-share><h4 class=td-social-share__title>Share this article:</h4><ul class=td-social-share__list><div class=social-share><a href="https://twitter.com/intent/tweet?text=What%20is%20LLM&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fwhat-is-llm%2f" target=_blank rel=noopener aria-label="Share on Twitter"><i class="fab fa-twitter"></i>
</a><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fwhat-is-llm%2f" target=_blank rel=noopener aria-label="Share on Facebook"><i class="fab fa-facebook"></i>
</a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fwhat-is-llm%2f&title=What%20is%20LLM" target=_blank rel=noopener aria-label="Share on LinkedIn"><i class="fab fa-linkedin"></i>
</a><a href="https://www.reddit.com/submit?url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fwhat-is-llm%2f&title=What%20is%20LLM" target=_blank rel=noopener aria-label="Share on Reddit"><i class="fab fa-reddit"></i>
</a><a href="mailto:?subject=What%20is%20LLM&body=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fwhat-is-llm%2f" aria-label="Share via Email"><i class="fas fa-envelope"></i></a></div></ul></div><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href=https://localhost:1313/dsblog/How-To-Do-Literature-Review/ aria-label="Previous page"><div class=td-pager__meta><i class="fa-solid fa-angle-left"></i>
<span class=td-pager__meta-label><b>Previous:</b></span>
<span class=td-pager__meta-title>How to do Literature Review</span></div></a><a class="td-pager__link td-pager__link--next" href=https://localhost:1313/dsblog/rps-Pretrained-Language-Models-for-Text-Generation/ aria-label="Next page"><div class=td-pager__meta><span class=td-pager__meta-label><b>Next:</b></span>
<span class=td-pager__meta-title>Paper-Summary- A Survey Paper# Pretrained Language Models for Text Generation</span>
<i class="fa-solid fa-angle-right"></i></div></a></ul></div><div class=col-md-2></div></div></main><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class=col-2><a href=https://dasarpai.com target=_blank rel=noopener><img src=https://localhost:1313/assets/images/site-logo.png alt=dasarpAI width=100 style=border-radius:12px></a></div><div class=col-8><div class=row></div><div class=row></div></div><div class=col-2></div><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"></div></div></div></footer></div><script src=https://localhost:1313/js/main.min.7c31f98e62c36b0c9c834ce3f8260a0e21895dd6aa0773e81a64b104eae3b2e8.js integrity="sha256-fDH5jmLDawycg0zj+CYKDiGJXdaqB3PoGmSxBOrjsug=" crossorigin=anonymous></script><script defer src=https://localhost:1313/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=https://localhost:1313/js/tabpane-persist.js></script><script src=https://localhost:1313/js/asciinema-player.js></script><script>(function(){var e=document.querySelector("#td-section-nav");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></body></html>