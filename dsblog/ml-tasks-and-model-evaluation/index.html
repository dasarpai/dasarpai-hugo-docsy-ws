<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.147.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href="../../favicons/favicon.ico?v=1"><link rel=apple-touch-icon href="../../favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><link rel=icon type=image/png href="../../favicons/favicon-16x16.png?v=1" sizes=16x16><link rel=icon type=image/png href="../../favicons/favicon-32x32.png?v=1" sizes=32x32><link rel=apple-touch-icon href="../../favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><title>Machine Learning Tasks and Model Evaluation | Blowfish</title><meta property="og:url" content="https://dasarpai.github.io/dsblog/ml-tasks-and-model-evaluation/">
<meta property="og:site_name" content="Blowfish"><meta property="og:title" content="Machine Learning Tasks and Model Evaluation"><meta property="og:description" content="Machine Learning Tasks and Model Evaluation Introduction Machine learning is a subject where we study how to create & evaluate machine learning models. To create these models, we need different types of data. We build models which can help us do various kinds of tasks. There are hundreds of model building techniques and researchers keep adding new techniques, and architectures as when need arises. But, the question is how do you evaluate these models which are output of the model trainings? To evaluate the performance of a model on structured data, or classification/regression/clustering models, we require one kind of metrics. But this becomes complicated when we are dealing with voice, text and audio data. How do you evaluate ten models which are responsible for translation, or locating an object in the image, transcribing voice into text, captioning an image? To solve this problem, standard databases are created and everyone needs to demonstrate the performance of their model, architecture, or approach against that dataset. But, even if you have a baseline dataset, how will you evaluate various NLP or deep learning tasks? For that GLUE, SuperGLUE benchmarks are created."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="dsblog"><meta property="article:published_time" content="2021-07-14T00:00:00+00:00"><meta property="article:modified_time" content="2021-07-14T00:00:00+00:00"><meta property="article:tag" content="DS Resources"><meta property="article:tag" content="Model Evaluation"><meta property="article:tag" content="Machine Learning Tasks"><meta property="article:tag" content="NLP Tasks"><meta property="article:tag" content="NLP Model Evaluation"><meta itemprop=name content="Machine Learning Tasks and Model Evaluation"><meta itemprop=description content="Machine Learning Tasks and Model Evaluation Introduction Machine learning is a subject where we study how to create & evaluate machine learning models. To create these models, we need different types of data. We build models which can help us do various kinds of tasks. There are hundreds of model building techniques and researchers keep adding new techniques, and architectures as when need arises. But, the question is how do you evaluate these models which are output of the model trainings? To evaluate the performance of a model on structured data, or classification/regression/clustering models, we require one kind of metrics. But this becomes complicated when we are dealing with voice, text and audio data. How do you evaluate ten models which are responsible for translation, or locating an object in the image, transcribing voice into text, captioning an image? To solve this problem, standard databases are created and everyone needs to demonstrate the performance of their model, architecture, or approach against that dataset. But, even if you have a baseline dataset, how will you evaluate various NLP or deep learning tasks? For that GLUE, SuperGLUE benchmarks are created."><meta itemprop=datePublished content="2021-07-14T00:00:00+00:00"><meta itemprop=dateModified content="2021-07-14T00:00:00+00:00"><meta itemprop=wordCount content="3061"><meta itemprop=keywords content="model,evaluation,metrics,,machine,learning,evaluation,,BLEU,score,,GLUE,benchmark,,SuperGLUE,,NLP,tasks,,model,performance,,evaluation,metrics,,machine,translation,metrics,,model,benchmarking"><meta name=twitter:card content="summary"><meta name=twitter:title content="Machine Learning Tasks and Model Evaluation"><meta name=twitter:description content="Machine Learning Tasks and Model Evaluation Introduction Machine learning is a subject where we study how to create & evaluate machine learning models. To create these models, we need different types of data. We build models which can help us do various kinds of tasks. There are hundreds of model building techniques and researchers keep adding new techniques, and architectures as when need arises. But, the question is how do you evaluate these models which are output of the model trainings? To evaluate the performance of a model on structured data, or classification/regression/clustering models, we require one kind of metrics. But this becomes complicated when we are dealing with voice, text and audio data. How do you evaluate ten models which are responsible for translation, or locating an object in the image, transcribing voice into text, captioning an image? To solve this problem, standard databases are created and everyone needs to demonstrate the performance of their model, architecture, or approach against that dataset. But, even if you have a baseline dataset, how will you evaluate various NLP or deep learning tasks? For that GLUE, SuperGLUE benchmarks are created."><link rel=preload href=../../scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css as=style integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link href=../../scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css rel=stylesheet integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link rel=stylesheet type=text/css href=https://dasarpai.github.io/css/asciinema-player.css><script src=https://code.jquery.com/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><link rel=stylesheet href=../../css/custom.css><script src=../../js/lunr.js></script></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-light nav-shadow flex-column flex-md-row td-navbar"><a id=agones-top class=navbar-brand href=../../><svg id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg> <span class="text-uppercase fw-bold">Blowfish</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=../../><span class=active>Data Science</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/docs/><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/docs/><span>Docs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../community/><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/docs/shortcodes/><span>Short Codes</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/blog/><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://github.com/nunocoracao/blowfish><span></span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href>GitHub</a></li><li class="nav-item dropdown d-none d-lg-block"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Release</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://development.agones.dev>Development</a>
<a class=dropdown-item href=https://agones.dev></a><a class=dropdown-item href=https://1-47-0.agones.dev>1.47.0</a>
<a class=dropdown-item href=https://1-46-0.agones.dev>1.46.0</a>
<a class=dropdown-item href=https://1-45-0.agones.dev>1.45.0</a>
<a class=dropdown-item href=https://1-44-0.agones.dev>1.44.0</a>
<a class=dropdown-item href=https://1-43-0.agones.dev>1.43.0</a>
<a class=dropdown-item href=https://1-42-0.agones.dev>1.42.0</a>
<a class=dropdown-item href=https://1-41-0.agones.dev>1.41.0</a>
<a class=dropdown-item href=https://1-40-0.agones.dev>1.40.0</a>
<a class=dropdown-item href=https://1-39-0.agones.dev>1.39.0</a>
<a class=dropdown-item href=https://1-38-0.agones.dev>1.38.0</a>
<a class=dropdown-item href=https://1-37-0.agones.dev>1.37.0</a>
<a class=dropdown-item href=https://1-36-0.agones.dev>1.36.0</a>
<a class=dropdown-item href=https://1-35-0.agones.dev>1.35.0</a>
<a class=dropdown-item href=https://1-34-0.agones.dev>1.34.0</a>
<a class=dropdown-item href=https://1-33-0.agones.dev>1.33.0</a>
<a class=dropdown-item href=https://1-32-0.agones.dev>1.32.0</a>
<a class=dropdown-item href=https://1-31-0.agones.dev>1.31.0</a></div></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><ul class=dropdown-menu><li><a class=dropdown-item href=../../it/>Italiano</a></li></ul></div></li></ul></div><div class="navbar-nav mx-lg-2 d-none d-lg-block"></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><div class=row><div class=col-md-2></div><div class=col-md-8><p><img src=../../assets/images/dsresources/dsr114-ml-tasks-and-model-evaluation.jpg alt="Deep Learning Tasks and Models"></p><h1 id=machine-learning-tasks-and-model-evaluation>Machine Learning Tasks and Model Evaluation<a class=td-heading-self-link href=#machine-learning-tasks-and-model-evaluation aria-label="Heading self-link"></a></h1><h2 id=introduction>Introduction<a class=td-heading-self-link href=#introduction aria-label="Heading self-link"></a></h2><p>Machine learning is a subject where we study how to create & evaluate machine learning models. To create these models, we need different types of data. We build models which can help us do various kinds of tasks. There are hundreds of model building techniques and researchers keep adding new techniques, and architectures as when need arises. But, the question is how do you evaluate these models which are output of the model trainings? To evaluate the performance of a model on structured data, or classification/regression/clustering models, we require one kind of metrics. But this becomes complicated when we are dealing with voice, text and audio data. How do you evaluate ten models which are responsible for translation, or locating an object in the image, transcribing voice into text, captioning an image? To solve this problem, standard databases are created and everyone needs to demonstrate the performance of their model, architecture, or approach against that dataset. But, even if you have a baseline dataset, how will you evaluate various NLP or deep learning tasks? For that GLUE, SuperGLUE benchmarks are created.</p><h2 id=what-is-bleu-benchmark>What is BLEU Benchmark?<a class=td-heading-self-link href=#what-is-bleu-benchmark aria-label="Heading self-link"></a></h2><p>BLEU (Bilingual Evaluation Understudy) is a metric used to evaluate the quality of machine translation. It is based on the n-gram precision between the machine translation and the reference translation.</p><p>BLEU-1 measures the unigram precision, which is the fraction of unigrams in the machine translation that also appear in the reference translation. A BLEU-1 score of 1 means that the machine translation is a perfect match for the reference translation.</p><p>BLEU-1 is a simple and easy-to-understand metric, but it can be biased towards shorter translations. For example, a machine translation that is twice as long as the reference translation will have a lower BLEU-1 score, even if it is more accurate.</p><p>To address this, BLEU-n metrics were developed, which measure the n-gram precision for n>1. BLEU-4 is the most commonly used BLEU-n metric. A BLEU-4 score of 1 means that the machine translation is a perfect match for the reference translation. However, BLEU-4 scores are typically much lower than 1, and a score of 0.5 or higher is considered to be good.</p><p>BLEU-4 is a simple and easy-to-understand metric, but it can be biased towards shorter translations. For example, a machine translation that is twice as long as the reference translation will have a lower BLEU-4 score, even if it is more accurate.</p><p>Steps to calculate BlLEU score are as following.</p><ol><li>Step1: Calculate N-gram Precision: BLEU calculates precision scores based on matching n-grams (sequences of n words) between the candidate translation and the reference translations. It computes precision scores for 1-gram (unigram), 2-gram (bigram), 3-gram, and 4-gram matches.</li><li>Step2: Brevity Penalty: BLEU also takes into account the length of the candidate translation compared to the reference translations. This is because shorter translations tend to have an advantage in n-gram matching, but they might not convey the full meaning of the source text. BLEU applies a brevity penalty to avoid favoring overly short translations.</li><li>Step3: Compute Geometric Mean: BLEU combines the precision scores of different n-gram matches using a geometric mean. This is done to give a balanced consideration to different n-gram orders. This metric helps in capturing both local and global translation quality.</li><li>Step4: Score Calculation: The BLEU score is calculated by multiplying the geometric mean of the n-gram precisions by the brevity penalty. The result is a value between 0 and 1, where a higher score indicates a better match between the candidate translation and the reference translations.</li></ol><p>It&rsquo;s important to note that while BLEU is widely used for evaluating machine translation systems, it has some limitations. For instance, BLEU relies solely on n-gram matching and does not capture higher-level semantic or syntactic aspects of translation quality. As a result, it may not always align well with human judgments of translation quality, especially when dealing with creative or complex translations.</p><p>Despite its limitations, BLEU remains a widely used and easily computable metric for quick and automated evaluation of machine translation outputs. Researchers often combine BLEU with other metrics or use it as a starting point for evaluation, but more advanced metrics have been developed over time to address its limitations.</p><p>Advantages of using BLEU score:</p><ul><li>It is a simple and easy-to-understand metric.</li><li>It is relatively insensitive to changes in word order.</li><li>It has been shown to be effective in evaluating the performance of machine translation.</li></ul><p>Disadvantages of using BLEU score:</p><ul><li>It can be biased towards shorter translations.</li><li>It does not take into account the semantic similarity between the machine translation and the reference translation.</li><li>It can be difficult to interpret the results of BLEU score for different tasks.</li></ul><h2 id=what-is-glue-benchmark>What is GLUE Benchmark?<a class=td-heading-self-link href=#what-is-glue-benchmark aria-label="Heading self-link"></a></h2><p>The <strong>GLUE (General Language Understanding Evaluation)</strong> benchmark is a collection of diverse natural language processing (NLP) tasks designed to evaluate and compare the performance of various machine learning models and techniques in understanding and processing human language. It serves as a standard evaluation framework for assessing the general language understanding capabilities of different models.</p><p>The GLUE benchmark was <strong>introduced in 2018</strong> and consists of a set of <strong>nine different NLP tasks</strong>, covering a wide range of language understanding tasks including sentence classification, sentence similarity, natural language inference, and question answering. Some of the tasks included in GLUE are the Stanford Sentiment Treebank, Multi-Genre Natural Language Inference, and the Recognizing Textual Entailment dataset.</p><p>The primary goal of the GLUE benchmark is to encourage the development of models that can perform well across multiple NLP tasks, thus demonstrating a more comprehensive understanding of human language. The performance of models is measured using a single metric called the GLUE score, which is computed by aggregating the performance of models on individual tasks.</p><p>The GLUE benchmark has been instrumental in advancing the field of NLP and has served as a benchmark for many state-of-the-art models, including various transformer-based architectures like BERT (Bidirectional Encoder Representations from Transformers) and RoBERTa.</p><p>It&rsquo;s worth noting that since the introduction of the GLUE benchmark, other benchmarks like SuperGLUE and XTREME have been developed to address some limitations and provide more challenging evaluation tasks for further advancing the state of NLP research.</p><ul><li><a href=https://gluebenchmark.com/>https://gluebenchmark.com/</a></li><li><a href=https://gluebenchmark.com/leaderboard>https://gluebenchmark.com/leaderboard</a></li><li><a href=https://paperswithcode.com/dataset/glue>https://paperswithcode.com/dataset/glue</a></li></ul><p>The GLUE benchmark is used by companies to evaluate the performance of their NLU models. For example, Google uses the GLUE benchmark to evaluate the performance of its BERT model. The higher the GLUE score, the better the overall performance of the model on various NLP tasks.</p><h2 id=what-are-the-tasks-of-glue-benchmark>What are the Tasks of GLUE Benchmark?<a class=td-heading-self-link href=#what-are-the-tasks-of-glue-benchmark aria-label="Heading self-link"></a></h2><p>The current version of the GLUE benchmark includes the following 9 tasks</p><ul><li>CoLA: A sentence-level grammaticality task.</li><li>SST-2: A binary sentiment classification task.</li><li>MRPC: A sentence-pair similarity task.</li><li>STS-B: A sentence-pair similarity task that measures the semantic relatedness of two sentences.</li><li>QQP: A question-answering task.</li><li>MNLI: A natural language inference task that measures whether a sentence entails another sentence.</li><li>QNLI: A natural language inference task that measures whether a sentence entails a question.</li><li>RTE: A natural language inference task that measures whether a sentence contradicts another sentence.</li><li>WNLI: A word similarity task that measures the similarity between two words.</li></ul><h2 id=what-is-superglue-benchmark>What is SuperGLUE Benchmark?<a class=td-heading-self-link href=#what-is-superglue-benchmark aria-label="Heading self-link"></a></h2><p>The SuperGLUE (Super General Language Understanding Evaluation) benchmark is an enhanced version of the GLUE benchmark introduced to address its limitations and provide more challenging language understanding tasks for evaluating and comparing the performance of natural language processing (NLP) models. SuperGLUE builds upon the success of GLUE and aims to push the boundaries of NLP research further.</p><p>SuperGLUE was <strong>introduced in 2019</strong> as an extension of GLUE, consisting of a more diverse and difficult set of language understanding tasks. It includes a <strong>total of eight challenging tasks</strong>, including tasks like BoolQ (Boolean Questions), COPA (Choice of Plausible Alternatives), and RTE (Recognizing Textual Entailment), among others. These tasks are carefully designed to require more advanced reasoning and understanding abilities from models.</p><p>The primary objective of SuperGLUE is to evaluate models on a more comprehensive set of tasks that demand higher levels of language comprehension and reasoning capabilities. It provides a broader and more challenging evaluation platform to assess the progress and performance of NLP models beyond what was covered by the original GLUE benchmark.</p><p>Similar to GLUE, SuperGLUE also utilizes a single evaluation metric called the SuperGLUE score to assess model performance across the different tasks. The SuperGLUE benchmark has spurred further research and development in the field of NLP, pushing for advancements in model architectures, training techniques, and performance improvements.</p><p>SuperGLUE has become a prominent benchmark for evaluating the state-of-the-art NLP models, building on the success of GLUE and encouraging the development of more sophisticated models that can tackle complex language understanding tasks.</p><p>It&rsquo;s important to note that the SuperGLUE benchmark, while providing more challenging tasks, is still evolving, and researchers continue to work on expanding and refining the benchmark to further push the boundaries of NLP research.</p><ul><li><a href=https://super.gluebenchmark.com/>https://super.gluebenchmark.com/</a></li><li><a href=https://super.gluebenchmark.com/leaderboard>https://super.gluebenchmark.com/leaderboard</a></li><li><a href=https://paperswithcode.com/dataset/superglue>https://paperswithcode.com/dataset/superglue</a></li></ul><h2 id=glue--superglue-tasks>GLUE & SuperGLUE tasks<a class=td-heading-self-link href=#glue--superglue-tasks aria-label="Heading self-link"></a></h2><p>Below is list of different NLP and Deep Learning tasks for which different benchmark datasets are created and model&rsquo;s perormance is measured against those tasks.</p><table><thead><tr><th>Sno</th><th>Task</th><th>Corpus</th><th>Paper</th><th>GLUE</th><th>SuperGLUE</th></tr></thead><tbody><tr><td>1</td><td>Sentence acceptability judgment</td><td><a href=https://nyu-mll.github.io/CoLA/>CoLA (Corpus of Linguistic Acceptability)</a></td><td><a href=https://arxiv.org/abs/1805.12471>CoLA Warstadt et al., 2018</a></td><td>Yes</td><td>No</td></tr><tr><td>2</td><td>Sentiment analysis</td><td><a href=https://nlp.stanford.edu/sentiment/index.html>SST-2 (Stanford Sentiment Treebank)</a></td><td><a href=https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf>SST-2 Socher et al., 2013</a></td><td>Yes</td><td>No</td></tr><tr><td>3</td><td>Paraphrasing/sentence similarity</td><td><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52398">MRPC (Microsoft Research Paraphrase Corpus)</a></td><td><a href=https://aclanthology.org/I05-5002>MRPC Dolan and Brockett, 2005</a></td><td>Yes</td><td>No</td></tr><tr><td>4</td><td></td><td><a href=http://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark>STS-B (Semantic Textual Similarity Benchmark)</a></td><td><a href=https://arxiv.org/abs/1708.00055>STS-B Ceret al., 2017</a></td><td>Yes</td><td>No</td></tr><tr><td>5</td><td></td><td><a href=https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs>QQP (Quora Question Pairs)</a></td><td><a href=https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs>QQP Iyer et al., 2017</a></td><td>Yes</td><td>No</td></tr><tr><td>6</td><td>Natural language inference</td><td><a href=https://arxiv.org/abs/1704.05426>MNLI (Multi-Genre Natural Language Inference)</a></td><td><a href=https://arxiv.org/abs/1704.05426>MNLI Williams et al., 2017</a></td><td>Yes</td><td>No</td></tr><tr><td>7</td><td></td><td><a href=https://rajpurkar.github.io/SQuAD-explorer/>QNLI (Question-answering Natural Language Inference)</a></td><td><a href=https://arxiv.org/abs/1606.05250>QNLI Rajpurkar et al.,2016</a></td><td>Yes</td><td>No</td></tr><tr><td>8</td><td></td><td><a href=https://aclweb.org/aclwiki/Recognizing_Textual_Entailment>RTE (Recognizing Textual Entailment)</a></td><td><a href=https://link.springer.com/chapter/10.1007/11736790_9>RTE Dagan et al., 2005</a></td><td>Yes</td><td>Yes</td></tr><tr><td>9</td><td></td><td><a href=https://github.com/mcdm/CommitmentBank>CB (CommitmentBank)</a></td><td><a href=https://semanticsarchive.net/Archive/Tg3ZGI2M/Marneffe.pdf>CB De Marneff et al., 2019</a></td><td>No</td><td>Yes</td></tr><tr><td>10</td><td>Sentence completion</td><td><a href=https://people.ict.usc.edu/~gordon/copa.html>COPA (Choice of Plausible Alternatives)</a></td><td><a href=https://www.researchgate.net/publication/221251392_Choice_of_Plausible_Alternatives_An_Evaluation_of_Commonsense_Causal_Reasoning>COPA Roemmele et al., 2011</a></td><td>No</td><td>Yes</td></tr><tr><td>11</td><td>Word sense disambiguation</td><td><a href=https://pilehvar.github.io/wic/>WiC (Word-in-Context)</a></td><td><a href=https://arxiv.org/abs/1808.09121>WIC Pilehvar and Camacho-Collados, 2018</a></td><td>No</td><td>Yes</td></tr><tr><td>12</td><td></td><td><a href=https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html>WNLI (Winograd Natural Language Inference)</a></td><td><a href></a></td><td>Yes</td><td>No</td></tr><tr><td>13</td><td>Question answering</td><td><a href=https://cogcomp.org/multirc/>MultiRC (Multi-sentence Reading Comprehension)</a></td><td><a href=https://aclanthology.org/N18-1023>MultiRC Khashabi et al., 2018</a></td><td>No</td><td>Yes</td></tr><tr><td>14</td><td></td><td><a href=https://sheng-z.github.io/ReCoRD-explorer/>ReCoRD (Reading Comprehension with Commonsense Reasoning Dataset)</a></td><td><a href=https://arxiv.org/abs/1810.12885>ReCoRD Zhang et al., 2018</a></td><td>No</td><td>Yes</td></tr><tr><td>15</td><td></td><td><a href=https://github.com/google-research-datasets/boolean-questions>BoolQ (Boolean Questions)</a></td><td><a href=https://arxiv.org/abs/1905.10044>BoolQ Clark et al., 2019</a></td><td>No</td><td>Yes</td></tr><tr><td>16</td><td>Common Sense Reasoning</td><td><a href=https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html>WSC (Winograd Schema Challenge)</a></td><td><a href=https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html></a></td><td>No</td><td>Yes</td></tr><tr><td>17</td><td></td><td><a href>AX-b : Broadcoverage Diagnostic - Mathew&rsquo;s Corr</a></td><td><a href></a></td><td>No</td><td>Yes</td></tr><tr><td>18</td><td></td><td><a href>AX-g : Winogender Shema Diagnostic Gender Parity - Accuracy</a></td><td><a href></a></td><td>No</td><td>Yes</td></tr></tbody></table><h3 id=what-is-the-difference-between-bleu-and-glue>What is the difference between BLEU and GLUE?<a class=td-heading-self-link href=#what-is-the-difference-between-bleu-and-glue aria-label="Heading self-link"></a></h3><p>The <strong>BLEU (Bilingual Evaluation Understudy)</strong> score and the <strong>GLUE (General Language Understanding Evaluation)</strong> score are two different evaluation metrics used in the field of natural language processing (NLP), and they serve different purposes and evaluate different aspects of NLP models.</p><p>BLEU is a metric commonly used to evaluate the <strong>quality of machine translation</strong> systems. It measures the similarity between the machine-generated translations and reference translations provided by human translators. BLEU score is <strong>based on n-gram precision</strong>, where it compares the n-gram sequences (typically up to 4-grams) between the machine-generated output and the reference translations. It assigns a score between 0 and 1, with a higher score indicating better translation quality.</p><p>GLUE, on the other hand, is a benchmark for evaluating the performance of NLP models on a range of language understanding tasks. It provides a single-number evaluation metric that aggregates the performance of a model across multiple tasks, including textual entailment, sentiment analysis, question-answering, and more. The GLUE score is a weighted average of the model&rsquo;s performance on each of these tasks, with higher scores indicating better overall language understanding capabilities.</p><h2 id=what-is-meteor-score>What is METEOR Score?<a class=td-heading-self-link href=#what-is-meteor-score aria-label="Heading self-link"></a></h2><p><strong>METEOR (Metric for Evaluation of Translation with Explicit ORdering)</strong> is a popular automatic evaluation metric used in the field of Natural Language Processing (NLP) to assess the quality of machine translation outputs. METEOR is designed to measure the overall similarity and alignment between a generated translation and a reference (human-generated) translation, taking into account multiple levels of linguistic analysis.</p><p>Unlike simpler metrics such as BLEU (Bilingual Evaluation Understudy), which primarily focus on measuring n-gram overlaps between the generated and reference translations, METEOR incorporates more sophisticated linguistic features and alignments to provide a more comprehensive evaluation.</p><p>Key features of the METEOR metric include:</p><ul><li>Tokenization and Stemming: METEOR tokenizes and stems both the generated and reference translations to handle variations in word forms and improve alignment.</li><li>Exact Matching and Stem Matching: METEOR calculates the precision and recall of exact word matches as well as matches based on stemmed forms of words, considering synonyms and related words.</li><li>Alignment and Synonymy: METEOR uses a paraphrase database to identify synonyms and related words, which helps in capturing semantically equivalent terms.</li><li>Word Order: METEOR explicitly considers word order and alignment between words in the generated and reference translations.</li><li>Chunk-based Matching: METEOR evaluates matches at the chunk level, which allows for partial credit for translations that have word-level differences but still capture the same meaning.</li><li>Punctuation and Function Words: METEOR takes into account punctuation and function words and their alignment, as they play a role in overall sentence coherence.</li><li>Precision and Recall: METEOR calculates precision and recall scores for each of the above components and combines them to compute the final metric.</li><li>Normalization and Penalty: METEOR applies normalization to the precision and recall scores and incorporates penalty terms to balance precision and recall contributions.</li></ul><p>The final METEOR score is a combination of these individual components, reflecting the quality of the generated translation in terms of semantic content, syntax, word choice, and alignment with the reference translation. METEOR is designed to address some of the limitations of BLEU and provide a more holistic evaluation of translation quality, considering linguistic variations and nuances.</p><p>METEOR has been widely used in machine translation research and competitions, serving as an important tool for comparing and benchmarking different translation systems and approaches. It provides a more comprehensive and linguistically informed perspective on translation quality, making it a valuable addition to the arsenal of NLP evaluation metrics.</p><h2 id=what-is-xtreme-benchmark>What is XTREME Benchmark?<a class=td-heading-self-link href=#what-is-xtreme-benchmark aria-label="Heading self-link"></a></h2><p>The XTREME (Cross-lingual TRansfer Evaluation of Multilingual Encoders) benchmark is a comprehensive evaluation framework <strong>introduced in 2020</strong> for assessing the performance of multilingual models in natural language understanding (NLU) tasks across multiple languages. It aims to evaluate the generalization and transfer capabilities of models in cross-lingual settings.</p><p>XTREME was developed as an extension of previous benchmarks like GLUE and SuperGLUE, with a specific focus on evaluating models&rsquo; <strong>abilities to understand and process languages beyond English</strong>. It includes a diverse range of tasks spanning multiple languages, such as named entity recognition, part-of-speech tagging, machine translation, sentence classification, and question answering, among others.
Tasks on <a href=https://sites.research.google/xtreme>xtreme bechmark</a></p><ul><li>Sentence-pair Classification</li><li>Structured Prediction</li><li>Question Answering</li><li>Sentence Retrieval</li></ul><p>The main objective of the XTREME benchmark is to encourage the <strong>development of models that can effectively transfer knowledge across different languages</strong>, leveraging pretraining on large-scale multilingual data. By evaluating models on a wide range of languages and tasks, XTREME provides insights into the cross-lingual transfer capabilities and identifies areas for improvement in multilingual NLU.</p><p>Similar to GLUE and SuperGLUE, XTREME utilizes a single metric called the XTREME score to assess the performance of models across the various tasks and languages. The XTREME benchmark serves as an important evaluation platform for advancing research and development in multilingual NLU, fostering the development of models that can effectively handle language diversity and facilitate cross-lingual understanding.</p><p>XTREME has gained significant attention and has been instrumental in driving progress in multilingual NLU, pushing researchers to develop models that exhibit strong cross-lingual transfer capabilities and perform well across a wide range of languages and tasks. The benchmark continues to evolve and expand to include additional languages, tasks, and evaluation metrics to further enhance the evaluation of multilingual models.</p><ul><li><a href=https://sites.research.google/xtreme>https://sites.research.google/xtreme</a></li><li><a href=https://arxiv.org/abs/2003.11080>https://arxiv.org/abs/2003.11080</a></li><li><a href=https://paperswithcode.com/dataset/xtreme>https://paperswithcode.com/dataset/xtreme</a></li></ul><h2 id=what-is-rouge-score>What is ROUGE Score?<a class=td-heading-self-link href=#what-is-rouge-score aria-label="Heading self-link"></a></h2><p>Recall-Oriented Understudy for Gisting Evaluation is a set of metrics for evaluating the quality of automatic summaries and machine translation. It measures the similarity between a machine-generated summary and a reference summary using overlapping n-grams, word sequences that appear in both the machine-generated summary and the reference summary.</p><p>The most common n-grams used are unigrams, bigrams, and trigrams. ROUGE score calculates the recall of n-grams in the machine-generated summary by comparing them to the reference summaries.</p><p>Formula for calculating ROUGE-N:</p><p>$ROUGE-N = \frac{\sum_{i=1}^{m} \text{card}(S_i \cap R_i)}{\sum_{i=1}^{m} \text{card}(R_i)}$</p><p>where:</p><p>$S_i$ is the set of n-grams in the machine-generated summary<br>$R_i$ is the set of n-grams in the reference summary</p><p>m is the maximum n-gram length</p><p>For example, ROUGE-1 measures the overlap of unigrams, ROUGE-2 measures the overlap of bigrams, and ROUGE-L measures the longest common subsequence of the machine-generated summary and the reference summary.</p><p>Advantages of using ROUGE score:</p><ul><li>It is a simple and easy-to-understand metric.</li><li>It is relatively insensitive to changes in word order.</li><li>It has been shown to be effective in evaluating the performance of automatic summaries and machine translation.</li></ul><p>Disadvantages of using ROUGE score:</p><ul><li>It does not take into account the semantic similarity between the machine-generated summary and the reference summary.</li><li>It can be biased towards longer summaries.</li><li>It can be difficult to interpret the results of ROUGE score for different tasks.</li></ul><h2 id=what-is-big-bench>What is BIG-Bench?<a class=td-heading-self-link href=#what-is-big-bench aria-label="Heading self-link"></a></h2><p>BIG-Bench is a collaborative benchmark intended to probe large language models and extrapolate their future capabilities. It is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. <a href=https://dasarpai.com/dsblog/nlp-tasks#214-nlp-tasks-from-big-benchmark>214 tasks</a> included in BIG-bench are summarized by keyword.</p><h2 id=deep-learning-tasks--models-on-huggingface-100k-models>Deep Learning Tasks & Models on Huggingface (100K Models)<a class=td-heading-self-link href=#deep-learning-tasks--models-on-huggingface-100k-models aria-label="Heading self-link"></a></h2><p>There are many tasks like below for different modalities. And there are different metrics to measure the performance of a model against those tasks. In future I will expend this article, which will contain the metrics for the tasks mentioned below.</p><h3 id=computer-vision-models-6000-models><a href="https://huggingface.co/models?pipeline_tag=translation">Computer Vision Models</a>, 6000+ Models<a class=td-heading-self-link href=#computer-vision-models-6000-models aria-label="Heading self-link"></a></h3><p>1 <a href=https://huggingface.co/tasks/depth-estimation>Depth Estimation</a><br>2 <a href=https://huggingface.co/tasks/image-classification>Image Classification</a><br>3 <a href=https://huggingface.co/tasks/image-segmentation>Image Segmentation</a><br>4 <a href=https://huggingface.co/tasks/image-to-image>Image-to-Image</a><br>5 <a href=https://huggingface.co/tasks/object-detection>Object Detection</a><br>6 <a href=https://huggingface.co/tasks/video-classification>Video Classification</a><br>7 <a href=https://huggingface.co/tasks/unconditional-image-generation>Unconditional Image Generation</a><br>8 <a href=https://huggingface.co/tasks/zero-shot-image-classification>Zero-Shot Image Classification</a></p><h3 id=natural-language-processing-models-65000-models><a href="https://huggingface.co/models?pipeline_tag=text-generation">Natural Language Processing Models</a>, 65000+ Models<a class=td-heading-self-link href=#natural-language-processing-models-65000-models aria-label="Heading self-link"></a></h3><p>1 <a href=https://huggingface.co/tasks/conversational>Conversational</a><br>2 <a href=https://huggingface.co/tasks/fill-mask>Fill-Mask</a><br>3 <a href=https://huggingface.co/tasks/question-answering>Question Answering</a><br>4 <a href=https://huggingface.co/tasks/sentence-similarity>Sentence Similarity</a><br>5 <a href=https://huggingface.co/tasks/summarization>Summarization</a><br>6 <a href=https://huggingface.co/tasks/table-question-answering>Table Question Answering</a><br>7 <a href=https://huggingface.co/tasks/text-classification>Text Classification</a><br>8 <a href=https://huggingface.co/tasks/text-generation>Text Generation</a><br>9 <a href=https://huggingface.co/tasks/token-classification>Token Classification</a><br>10 <a href=https://huggingface.co/tasks/translation>Translation</a><br>11 <a href=https://huggingface.co/tasks/zero-shot-classification>Zero-Shot Classification</a></p><h3 id=audio-models-10000-models><a href="https://huggingface.co/models?pipeline_tag=voice-activity-detection">Audio Models</a>, 10000+ Models<a class=td-heading-self-link href=#audio-models-10000-models aria-label="Heading self-link"></a></h3><p>1 <a href=https://huggingface.co/tasks/audio-classification>Audio Classification</a><br>2 <a href=https://huggingface.co/tasks/audio-to-audio>Audio-to-Audio</a><br>3 <a href=https://huggingface.co/tasks/automatic-speech-recognition>Automatic Speech Recognition</a><br>4 <a href=https://huggingface.co/tasks/text-to-speech>Text-to-Speech</a><br>5 <a href="https://huggingface.co/models?pipeline_tag=tabular-classification">Tabular</a><br>6 <a href=https://huggingface.co/tasks/tabular-classification>Tabular Classification</a><br>7 <a href=https://huggingface.co/tasks/tabular-regression>Tabular Regression</a></p><h3 id=multimodal-models-9000-models><a href="https://huggingface.co/models?pipeline_tag=reinforcement-learning">Multimodal Models</a>, 9000+ Models<a class=td-heading-self-link href=#multimodal-models-9000-models aria-label="Heading self-link"></a></h3><p>1 <a href=https://huggingface.co/tasks/document-question-answering>Document Question Answering</a><br>2 <a href=https://huggingface.co/tasks/feature-extraction>Feature Extraction</a><br>3 <a href=https://huggingface.co/tasks/image-to-text>​Image-to-Text</a><br>4 <a href=https://huggingface.co/tasks/text-to-image>Text-to-Image</a><br>5 <a href=https://huggingface.co/tasks/text-to-video>Text-to-Video Contribute</a><br>6 <a href=https://huggingface.co/tasks/visual-question-answering>Visual Question Answering</a></p><h3 id=reinforcement-learning-22000-models><a href=https://huggingface.co/tasks/reinforcement-learning>Reinforcement Learning</a>, 22000+ Models<a class=td-heading-self-link href=#reinforcement-learning-22000-models aria-label="Heading self-link"></a></h3><div class=category-section><h4 class=category-section__title>Categories:</h4><div class=category-badges><a href=../../categories/dsblog class=category-badge>dsblog</a><a href=../../categories/dsresources class=category-badge>dsresources</a></div></div><div class=td-tags><h4 class=td-tags__title>Tags:</h4><div class=category-badges><a href=../../tags/ds-resources class=category-badge>DS Resources</a><a href=../../tags/model-evaluation class=category-badge>Model Evaluation</a><a href=../../tags/machine-learning-tasks class=category-badge>Machine Learning Tasks</a><a href=../../tags/nlp-tasks class=category-badge>NLP Tasks</a><a href=../../tags/nlp-model-evaluation class=category-badge>NLP Model Evaluation</a></div></div><div class=td-author-box><div class=td-author-box__info><h4 class=td-author-box__name>Blowfish</h4><p class=td-author-box__bio>A powerful, lightweight theme for Hugo.</p><div class=td-author-box__links><a href target=_blank rel=noopener aria-label></a><a href target=_blank rel=noopener aria-label></a></div></div></div><div class=td-social-share><h4 class=td-social-share__title>Share this article:</h4><ul class=td-social-share__list><div class=social-share><a href="https://twitter.com/intent/tweet?text=Machine%20Learning%20Tasks%20and%20Model%20Evaluation&url=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fml-tasks-and-model-evaluation%2f" target=_blank rel=noopener aria-label="Share on Twitter"><i class="fab fa-twitter"></i>
</a><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fml-tasks-and-model-evaluation%2f" target=_blank rel=noopener aria-label="Share on Facebook"><i class="fab fa-facebook"></i>
</a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fml-tasks-and-model-evaluation%2f&title=Machine%20Learning%20Tasks%20and%20Model%20Evaluation" target=_blank rel=noopener aria-label="Share on LinkedIn"><i class="fab fa-linkedin"></i>
</a><a href="https://www.reddit.com/submit?url=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fml-tasks-and-model-evaluation%2f&title=Machine%20Learning%20Tasks%20and%20Model%20Evaluation" target=_blank rel=noopener aria-label="Share on Reddit"><i class="fab fa-reddit"></i>
</a><a href="mailto:?subject=Machine%20Learning%20Tasks%20and%20Model%20Evaluation&body=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fml-tasks-and-model-evaluation%2f" aria-label="Share via Email"><i class="fas fa-envelope"></i></a></div></ul></div><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href=../../dsblog/ml-frameworks-libraries-tools/ aria-label="Previous page"><div class=td-pager__meta><i class="fa-solid fa-angle-left"></i>
<span class=td-pager__meta-label><b>Previous:</b></span>
<span class=td-pager__meta-title>Machine Learning Framework, Library, Tools</span></div></a><a class="td-pager__link td-pager__link--next" href=../../dsblog/datasets/ aria-label="Next page"><div class=td-pager__meta><span class=td-pager__meta-label><b>Next:</b></span>
<span class=td-pager__meta-title>Thousands of Machine Learning Datasets </span><i class="fa-solid fa-angle-right"></i></div></a></ul></div><div class=col-md-2></div></div></main><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class=col-2><a href=https://dasarpai.com target=_blank rel=noopener><img src=https://dasarpai.github.io/assets/images/site-logo.png alt=dasarpAI width=100 style=border-radius:12px></a></div><div class=col-8><div class=row></div><div class=row></div></div><div class=col-2></div><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"></div></div></div></footer></div><script src=../../js/main.min.4e728f6d1777a74bef81c238fd7b573f42f1848a910503e07f877f49f1f442cb.js integrity="sha256-TnKPbRd3p0vvgcI4/XtXP0LxhIqRBQPgf4d/SfH0Qss=" crossorigin=anonymous></script><script defer src=../../js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=../../js/tabpane-persist.js></script><script src=https://dasarpai.github.io/js/asciinema-player.js></script><script>(function(){var e=document.querySelector("#td-section-nav");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></body></html>