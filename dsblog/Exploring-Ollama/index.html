<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.147.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href="https://localhost:1313/favicons/favicon.ico?v=1"><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-16x16.png?v=1" sizes=16x16><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-32x32.png?v=1" sizes=32x32><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><title>Exploring Ollama & LM Studio | Blowfish</title><meta property="og:url" content="https://localhost:1313/dsblog/Exploring-Ollama/">
<meta property="og:site_name" content="Blowfish"><meta property="og:title" content="Exploring Ollama & LM Studio"><meta property="og:description" content="Exploring Ollama & LM Studio Is this article for me? If you are looking answers to the following questions, then this article is for you:"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="dsblog"><meta property="article:published_time" content="2024-09-18T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-18T00:00:00+00:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Ollama"><meta property="article:tag" content="Meta"><meta property="article:tag" content="Large Language Model"><meta itemprop=name content="Exploring Ollama & LM Studio"><meta itemprop=description content="Exploring Ollama & LM Studio Is this article for me? If you are looking answers to the following questions, then this article is for you:"><meta itemprop=datePublished content="2024-09-18T00:00:00+00:00"><meta itemprop=dateModified content="2024-09-18T00:00:00+00:00"><meta itemprop=wordCount content="4881"><meta itemprop=keywords content="Ollama AI,On-device AI,Local LLM frameworks,AI model management,LangChain integration"><meta name=twitter:card content="summary"><meta name=twitter:title content="Exploring Ollama & LM Studio"><meta name=twitter:description content="Exploring Ollama & LM Studio Is this article for me? If you are looking answers to the following questions, then this article is for you:"><link rel=preload href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css as=style integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css rel=stylesheet integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link rel=stylesheet type=text/css href=https://localhost:1313/css/asciinema-player.css><script src=https://code.jquery.com/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><link rel=stylesheet href=https://localhost:1313/css/custom.css><script src=https://localhost:1313/js/lunr.js></script></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-light nav-shadow flex-column flex-md-row td-navbar"><a id=agones-top class=navbar-brand href=https://localhost:1313/><svg id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg> <span class="text-uppercase fw-bold">Blowfish</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=https://localhost:1313/><span class=active>Data Science</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Docs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/community/><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/shortcodes/><span>Short Codes</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/blog/><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://github.com/nunocoracao/blowfish><span></span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href>GitHub</a></li><li class="nav-item dropdown d-none d-lg-block"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Release</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://development.agones.dev>Development</a>
<a class=dropdown-item href=https://agones.dev></a><a class=dropdown-item href=https://1-47-0.agones.dev>1.47.0</a>
<a class=dropdown-item href=https://1-46-0.agones.dev>1.46.0</a>
<a class=dropdown-item href=https://1-45-0.agones.dev>1.45.0</a>
<a class=dropdown-item href=https://1-44-0.agones.dev>1.44.0</a>
<a class=dropdown-item href=https://1-43-0.agones.dev>1.43.0</a>
<a class=dropdown-item href=https://1-42-0.agones.dev>1.42.0</a>
<a class=dropdown-item href=https://1-41-0.agones.dev>1.41.0</a>
<a class=dropdown-item href=https://1-40-0.agones.dev>1.40.0</a>
<a class=dropdown-item href=https://1-39-0.agones.dev>1.39.0</a>
<a class=dropdown-item href=https://1-38-0.agones.dev>1.38.0</a>
<a class=dropdown-item href=https://1-37-0.agones.dev>1.37.0</a>
<a class=dropdown-item href=https://1-36-0.agones.dev>1.36.0</a>
<a class=dropdown-item href=https://1-35-0.agones.dev>1.35.0</a>
<a class=dropdown-item href=https://1-34-0.agones.dev>1.34.0</a>
<a class=dropdown-item href=https://1-33-0.agones.dev>1.33.0</a>
<a class=dropdown-item href=https://1-32-0.agones.dev>1.32.0</a>
<a class=dropdown-item href=https://1-31-0.agones.dev>1.31.0</a></div></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><ul class=dropdown-menu><li><a class=dropdown-item href=https://localhost:1313/it/>Italiano</a></li></ul></div></li></ul></div><div class="navbar-nav mx-lg-2 d-none d-lg-block"></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><div class=row><div class=col-md-2></div><div class=col-md-8><p><img src=https://localhost:1313/assets/images/dspost/dsp6143-Exploring-Ollama.jpg alt="Exploring Ollama & LM Studio"></p><h1 id=exploring-ollama--lm-studio>Exploring Ollama & LM Studio<a class=td-heading-self-link href=#exploring-ollama--lm-studio aria-label="Heading self-link"></a></h1><h2 id=is-this-article-for-me>Is this article for me?<a class=td-heading-self-link href=#is-this-article-for-me aria-label="Heading self-link"></a></h2><p>If you are looking answers to the following questions, then this article is for you:</p><ul><li>Question: What is Ollama? Is it like Docker?</li><li>Question: How is Ollama different from Docker?</li><li>Question: How to install ollama on my machine?</li><li>Question: How to create customized LLM Model (docker like image)?</li><li>Question: What are the LLM available on ollama?</li><li>Question: Can we integrate these hundreds with different UI like ChatGPT?</li><li>Question: If I want to use all these Ollama models via Jupyter Notebook then what to do?</li><li>Question: Does Ollama have plugins like github copilot? Can I use those from my visual code?</li><li>Question: What kind of software are LM Studio or Ollama?</li><li>Question: What is LM Studio and how different it is from Ollama?</li><li>Question: What are different formats to save model, specifically LLMs?</li><li>Question: What is gguf model extention?</li><li>Question: If I have finetuned my models using clouds like aws sagemaker, vertexai, azure and kept there then can I use them inside my ollama and LM Studio?</li></ul><h2 id=question-what-is-ollama-is-it-like-docker>Question: What is Ollama? Is it like Docker?<a class=td-heading-self-link href=#question-what-is-ollama-is-it-like-docker aria-label="Heading self-link"></a></h2><p>Ollama is a platform designed to make running and interacting with large language models (LLMs) easier. It abstracts away the complexities of managing LLM models, GPU resources, and related configurations by offering a simple CLI interface. With Ollama, you can run, manage, and deploy LLMs locally or in various cloud environments without having to worry about the intricate details of setting up environments, downloading models, or configuring them.</p><h3 id=key-features-of-ollama>Key Features of Ollama:<a class=td-heading-self-link href=#key-features-of-ollama aria-label="Heading self-link"></a></h3><ul><li><strong>Model Management</strong>: Ollama can download and store LLMs in a local cache for you to run, typically in a format optimized for the hardware available (like your local GPU).</li><li><strong>GPU/CPU Utilization</strong>: It detects hardware resources, such as your NVIDIA GPU, and automatically uses them for model acceleration without additional setup.</li><li><strong>Service Setup</strong>: When you install Ollama, it sets up a service running in the background that serves models on an API, so you can interact with them programmatically.</li></ul><h2 id=question-how-is-ollama-different-from-docker>Question: How is Ollama different from Docker?<a class=td-heading-self-link href=#question-how-is-ollama-different-from-docker aria-label="Heading self-link"></a></h2><p>While Ollama and Docker both deal with isolated environments, they serve different purposes:</p><ul><li><strong>Ollama</strong> focuses specifically on running machine learning models, especially large language models, and optimizes resources to make them easily accessible and deployable.</li><li><strong>Docker</strong> is a general-purpose containerization tool that allows you to package applications with their dependencies in isolated environments. It’s used for deploying a wide variety of applications, not just models.</li></ul><p>So, while Docker might also be used to set up machine learning environments or serve models, Ollama is specialized and optimized for the LLM use case.</p><p>In Summary: <strong>Ollama</strong> = Model management platform for LLMs, with easy CLI and automatic resource optimization. <strong>Docker</strong> = General containerization tool for deploying all types of applications in isolated environments.</p><h2 id=question-how-to-install-ollama-on-my-machine>Question: How to install ollama on my machine?<a class=td-heading-self-link href=#question-how-to-install-ollama-on-my-machine aria-label="Heading self-link"></a></h2><p>Refer: <a href=https://ollama.com/download/linux>https://ollama.com/download/linux</a>, and <a href=https://github.com/ollama/ollama>https://github.com/ollama/ollama</a>, and <a href=https://github.com/ollama/ollama-python>https://github.com/ollama/ollama-python</a></p><ul><li>To download Ollama on linux/wsl:<br>curl -fsSL <a href=https://ollama.com/install.sh>https://ollama.com/install.sh</a> | sh</li><li>To run<br>ollama run phi3<br><strong><font color=green>http://127.0.0.1:11434/ - ollama is running</font></strong></li></ul><h2 id=question-how-to-create-customized-llm-model-docker-like-image>Question: How to create customized LLM Model (docker like image)?<a class=td-heading-self-link href=#question-how-to-create-customized-llm-model-docker-like-image aria-label="Heading self-link"></a></h2><p>If you know the working of Docker image, container, docker hub, docker command then you will feel at home with ollama commands.</p><p>Step 1: Create a ModelFile</p><pre tabindex=0><code>FROM llama3.1

# set the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1

# set the system message
SYSTEM &#34;&#34;&#34;
You are Travel Advisor from Air India Airlines. Answer as AI Advisor, the assistant, only.
&#34;&#34;&#34;
</code></pre><p>Step 2: Create and run the model</p><pre tabindex=0><code>ollama create aiadvisor -f ./Modelfile
ollama run aiadvisor
&gt;&gt;&gt; hi
Hello! It&#39;s your friend AI Advisor.
</code></pre><h2 id=question-what-are-the-llm-available-on-ollama>Question: What are the LLM available on ollama?<a class=td-heading-self-link href=#question-what-are-the-llm-available-on-ollama aria-label="Heading self-link"></a></h2><p>There are 100+ LLM available via ollama. They have different capabilities in terms of domain task like
coding, embedding, reasoning, chatting, philosophy, medical, maths, function calling. And in terms of context window 8k, 16k, 24k, 128k etc. And in terms of hardware/gpu required or not to run these.</p><h3 id=chattingassistant>Chatting/Assistant/<a class=td-heading-self-link href=#chattingassistant aria-label="Heading self-link"></a></h3><ol><li>alfred: A robust conversational model designed to be used for both chat and instruct use cases.</li><li>all-minilm: Embedding models on very large sentence level datasets.</li><li>An experimental 1.1B parameter model trained on the new Dolphin 2.8 dataset by Eric Hartford and based on TinyLlama.</li><li>Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages.</li><li>bge-large: Embedding model from BAAI mapping texts to vectors.</li><li>BGE-M3 is a new Embedding model from BAAI distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity.</li><li>Command R is a Large Language Model optimized for conversational interaction and long context tasks.</li><li>Command R+ is a powerful, scalable large language model purpose-built to excel at real-world enterprise use cases.</li><li>DBRX is an open, general-purpose LLM created by Databricks.</li><li>deepseek-llm: An advanced language model crafted with 2 trillion bilingual tokens.</li><li>deepseek-v2: A strong, economical, and efficient Mixture-of-Experts language model.</li><li>deepseek-v2.5: An upgraded version of DeekSeek-V2 that integrates the general and coding abilities of both DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct.</li><li>Dolphin 2.9 is a new model with 8B and 70B sizes by Eric Hartford based on Llama 3 that has a variety of instruction, conversational, and coding skills.</li><li>dolphin-mixtral: Uncensored, 8x7b and 8x22b fine-tuned models based on the Mixtral mixture of experts models that excels at coding tasks. Created by Eric Hartford.</li><li>everythinglm: Uncensored Llama2 based model with support for a 16K context window.</li><li>falcon: A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and chat bots.</li><li>Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind.</li><li>glm4: A strong multi-lingual general language model with competitive performance to Llama 3.</li><li>goliath: A language model created by combining two fine-tuned Llama 2 70B models into one.</li><li>Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.</li><li>Hermes 3 is the latest version of the flagship Hermes series of LLMs by Nous Research</li><li>Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.</li><li>Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.</li><li>llama2-chinese: Llama 2 based model fine tuned to improve Chinese dialogue ability.</li><li>llama3-chatqa: A model from NVIDIA based on Llama 3 that excels at conversational question answering (QA) and retrieval-augmented generation (RAG).</li><li>llama3-gradient: This model extends LLama-3 8B&rsquo;s context length from 8k to over 1m tokens.</li><li>MegaDolphin-2.2-120b is a transformation of Dolphin-2.2-70b created by interleaving the model with itself.</li><li>Meta Llama 3: The most capable openly available LLM to date</li><li>Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset.</li><li>mistral-nemo: A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.</li><li>mistral-small: Mistral Small is a lightweight model designed for cost-effective use in tasks like translation and summarization.</li><li>MistralLite is a fine-tuned model based on Mistral with enhanced capabilities of processing long contexts.</li><li>mixtral: A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes.</li><li>neural-chat: A fine-tuned model based on Mistral with good coverage of domain and language.</li><li>notus: A 7B chat model fine-tuned with high-quality data and based on Zephyr.</li><li>notux: A top-performing mixture of experts model, fine-tuned with high-quality data.</li><li>nous-hermes: General use models based on Llama and Llama 2 from Nous Research.</li><li>nous-hermes2: The powerful family of models by Nous Research that excels at scientific discussion and coding tasks.</li><li>nuextract: A 3.8B model fine-tuned on a private high-quality synthetic dataset for information extraction, based on Phi-3.</li><li>OpenHermes 2.5 is a 7B model fine-tuned by Teknium on Mistral with fully open datasets.</li><li>orca-mini: A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.</li><li>Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.</li><li>phi3.5: A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger sized models.</li><li>Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters</li><li>Qwen2 is a new series of large language models from Alibaba group</li><li>reader-lm: A series of models that convert HTML content to Markdown content, which is useful for content conversion tasks.</li><li>samantha-mistral: A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistral.</li><li>smollm: A family of small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset.</li><li>solar: A compact, yet powerful 10.7B large language model designed for single-turn conversation.</li><li>Stable LM 2 is a state-of-the-art 1.6B and 12B parameter language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch.</li><li>stable-beluga: Llama 2 based model fine tuned on an Orca-style dataset. Originally called Free Willy.</li><li>stablelm-zephyr: A lightweight chat model allowing accurate, and responsive output without requiring high-end hardware.</li><li>Starling is a large language model trained by reinforcement learning from AI feedback focused on improving chatbot helpfulness.</li><li>The Nous Hermes 2 model from Nous Research, now trained over Mixtral.</li><li>The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.</li><li>vicuna: General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.</li><li>Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj.</li><li>Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford.</li><li>wizardlm-uncensored: Uncensored version of Wizard LM model</li><li>xwinlm: Conversational model based on Llama 2 that performs competitively on various benchmarks.</li><li>yarn-llama2: An extension of Llama 2 that supports a context of up to 128k tokens.</li><li>yarn-mistral: An extension of Mistral to support context windows of 64K or 128K.</li><li>Yi 1.5 is a high-performing, bilingual language model.</li><li>Zephyr is a series of fine-tuned versions of the Mistral and Mixtral models that are trained to act as helpful assistants.</li></ol><h3 id=multimodal--vision>Multimodal & Vision<a class=td-heading-self-link href=#multimodal--vision aria-label="Heading self-link"></a></h3><ol><li>BakLLaVA is a multimodal (vision) model consisting of the Mistral 7B base model augmented with the LLaVA architecture.</li><li>minicpm-v: A series of multimodal LLMs (MLLMs) designed for vision-language understanding.</li><li>LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding.</li><li>llava-llama3: A LLaVA (vision) model fine-tuned from Llama 3 Instruct with better scores in several benchmarks.</li><li>llava-phi3: A new small LLaVA (vision) model fine-tuned from Phi 3 Mini.</li><li>moondream2 is a small vision language model designed to run efficiently on edge devices.</li></ol><h3 id=math>Math<a class=td-heading-self-link href=#math aria-label="Heading self-link"></a></h3><ol><li>llama-pro: An expansion of Llama 2 that specializes in integrating both general language understanding and domain-specific knowledge, particularly in programming and mathematics.</li><li>Qwen2 Math is a series of specialized math language models built upon the Qwen2 LLMs, which significantly outperforms the mathematical capabilities of open-source models and even closed-source models (e.g., GPT4o).</li><li>wizard-math: Model focused on math and logic problems</li></ol><h3 id=coding>Coding<a class=td-heading-self-link href=#coding aria-label="Heading self-link"></a></h3><ol><li>codellama: A large language model that can use text prompts to generate and discuss code.</li><li>codegeex4: A versatile model for AI software development scenarios, including code completion.</li><li>codeup: Great code generation model based on Llama2.</li><li>codebooga: A high-performing code instruct model created by merging two existing code models.</li><li>Magicoder is a family of 7B parameter models trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets.</li><li>wizardcoder: State-of-the-art code generation model</li><li>phind-codellama: Code generation model based on Code Llama.</li><li>dolphincoder: A 7B and 15B uncensored variant of the Dolphin model family that excels at coding, based on StarCoder2.</li><li>granite-code: A family of open foundation models by IBM for Code Intelligence</li><li>deepseek-coder-v2: An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.</li><li>SQLCoder is a code completion model fined-tuned on StarCoder for SQL generation tasks</li><li>StarCoder is a code generation model trained on 80+ programming languages.</li><li>Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding performance with fewer than 10 billion parameters.</li><li>Codestral is Mistral AI’s first-ever code model designed for code generation tasks.</li><li>Falcon2 is an 11B parameters causal decoder-only model built by TII and trained over 5T tokens.</li><li>Stable Code 3B is a coding model with instruct and code completion variants on par with models such as Code Llama 7B that are 2.5x larger.</li><li>StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes: 3B, 7B and 15B parameters.</li><li>DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.</li><li>CodeQwen1.5 is a large language model pretrained on a large amount of code data.</li><li>Mistral Large 2 is Mistral&rsquo;s new flagship model that is significantly more capable in code generation, mathematics, and reasoning with 128k context window and support for dozens of languages.</li><li>open-orca-platypus2: Merge of the Open Orca OpenChat model and the Garage-bAInd Platypus 2 model. Designed for chat and code generation.</li><li>CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks like fill-in-the-middle code completion, code generation, natural language understanding, mathematical reasoning, and instruction following.</li></ol><h3 id=embedding>Embedding<a class=td-heading-self-link href=#embedding aria-label="Heading self-link"></a></h3><ol><li>nomic-embed-text: A high-performing open embedding model with a large token context window.</li><li>mxbai-embed-large</li><li>snowflake-arctic-embed: A suite of text embedding models by Snowflake, optimized for performance.</li><li>State-of-the-art large embedding model from mixedbread.ai</li><li>paraphrase-multilingual: Sentence-transformers (embedding) model that can be used for tasks like clustering or semantic search.</li></ol><h3 id=medical>Medical<a class=td-heading-self-link href=#medical aria-label="Heading self-link"></a></h3><ol><li>medllama2: Fine-tuned Llama 2 model to answer medical questions based on an open source medical dataset.</li><li>meditron: Open-source medical large language model adapted from Llama 2 to the medical domain.</li></ol><h3 id=function-calling>Function Calling<a class=td-heading-self-link href=#function-calling aria-label="Heading self-link"></a></h3><ol><li>Nexus Raven is a 13B instruction tuned model for function calling tasks.</li><li>llama3-groq-tool-use: A series of models from Groq that represent a significant advancement in open-source AI capabilities for tool use/function calling.</li><li>firefunction-v2: An open weights function calling model based on Llama 3, competitive with GPT-4o function calling capabilities.</li></ol><h3 id=reasoning>Reasoning<a class=td-heading-self-link href=#reasoning aria-label="Heading self-link"></a></h3><ol><li>mathstral: MathΣtral: a 7B model designed for math reasoning and scientific discovery by Mistral AI.</li><li>Phi-2: a 2.7B language model by Microsoft Research that demonstrates outstanding reasoning and language understanding capabilities.</li><li>InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding reasoning capability.</li><li>wizardlm2: State of the art large language model from Microsoft AI with improved performance on complex chat, multilingual, reasoning and agent use cases.</li><li>reflection: A high-performing model trained with a new technique called Reflection-tuning that teaches a LLM to detect mistakes in its reasoning and correct course.</li><li>Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta&rsquo;s Llama 2 models. The model is designed to excel particularly in reasoning.</li></ol><h2 id=question-can-we-integrate-these-hundreds-with-different-ui-like-chatgpt>Question: Can we integrate these hundreds with different UI like ChatGPT?<a class=td-heading-self-link href=#question-can-we-integrate-these-hundreds-with-different-ui-like-chatgpt aria-label="Heading self-link"></a></h2><p>Yes, in fact you need NOT to create any new UI. Hundreds of good UI are available which are integrated with these hundreds of LLMs available on Ollama. You can see below some of the popular UI via which Ollama models can be accessed.</p><ul><li><a href=https://github.com/open-webui/open-webui>Open WebUI</a></li><li><a href=https://github.com/AugustDev/enchanted>Enchanted (macOS native)</a></li><li><a href=https://github.com/fmaclen/hollama>Hollama</a></li><li><a href=https://github.com/ParisNeo/lollms-webui>Lollms-Webui</a></li><li><a href=https://github.com/danny-avila/LibreChat>LibreChat</a></li><li><a href=https://github.com/bionic-gpt/bionic-gpt>Bionic GPT</a></li><li><a href=https://github.com/rtcfirefly/ollama-ui>HTML UI</a></li><li><a href=https://github.com/jikkuatwork/saddle>Saddle</a></li><li><a href=https://github.com/ivanfioravanti/chatbot-ollama>Chatbot UI</a></li><li><a href=https://github.com/mckaywrigley/chatbot-ui>Chatbot UI v2</a></li><li><a href="https://github.com/ollama-interface/Ollama-Gui?tab=readme-ov-file">Typescript UI</a></li><li><a href=https://github.com/richawo/minimal-llm-ui>Minimalistic React UI for Ollama Models</a></li><li><a href=https://github.com/kevinhermawan/Ollamac>Ollamac</a></li><li><a href=https://github.com/enricoros/big-AGI/blob/main/docs/config-local-ollama.md>big-AGI</a></li><li><a href=https://github.com/cheshire-cat-ai/core>Cheshire Cat assistant framework</a></li><li><a href=https://github.com/semperai/amica>Amica</a></li><li><a href=https://github.com/BruceMacD/chatd>chatd</a></li><li><a href=https://github.com/kghandour/Ollama-SwiftUI>Ollama-SwiftUI</a></li><li><a href=https://github.com/langgenius/dify>Dify.AI</a></li><li><a href=https://mindmac.app>MindMac</a></li><li><a href=https://github.com/jakobhoeg/nextjs-ollama-llm-ui>NextJS Web Interface for Ollama</a></li><li><a href=https://msty.app>Msty</a></li><li><a href=https://github.com/Bin-Huang/Chatbox>Chatbox</a></li><li><a href=https://github.com/tgraupmann/WinForm_Ollama_Copilot>WinForm Ollama Copilot</a></li><li><a href=https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web>NextChat</a> with <a href=https://docs.nextchat.dev/models/ollama>Get Started Doc</a></li><li><a href=https://github.com/mmo80/alpaca-webui>Alpaca WebUI</a></li><li><a href=https://github.com/enoch1118/ollamaGUI>OllamaGUI</a></li><li><a href=https://github.com/InternLM/OpenAOE>OpenAOE</a></li><li><a href=https://github.com/leonid20000/OdinRunes>Odin Runes</a></li><li><a href=https://github.com/mrdjohnson/llm-x>LLM-X</a> (Progressive Web App)</li><li><a href=https://github.com/Mintplex-Labs/anything-llm>AnythingLLM (Docker + MacOs/Windows/Linux native app)</a></li><li><a href=https://github.com/rapidarchitect/ollama_basic_chat>Ollama Basic Chat: Uses HyperDiv Reactive UI</a></li><li><a href=https://github.com/drazdra/ollama-chats>Ollama-chats RPG</a></li><li><a href=https://github.com/reid41/QA-Pilot>QA-Pilot</a> (Chat with Code Repository)</li><li><a href=https://github.com/sugarforever/chat-ollama>ChatOllama</a> (Open Source Chatbot based on Ollama with Knowledge Bases)</li><li><a href=https://github.com/Nagi-ovo/CRAG-Ollama-Chat>CRAG Ollama Chat</a> (Simple Web Search with Corrective RAG)</li><li><a href=https://github.com/infiniflow/ragflow>RAGFlow</a> (Open-source Retrieval-Augmented Generation engine based on deep document understanding)</li><li><a href=https://github.com/StreamDeploy-DevRel/streamdeploy-llm-app-scaffold>StreamDeploy</a> (LLM Application Scaffold)</li><li><a href=https://github.com/swuecho/chat>chat</a> (chat web app for teams)</li><li><a href=https://github.com/lobehub/lobe-chat>Lobe Chat</a> with <a href=https://lobehub.com/docs/self-hosting/examples/ollama>Integrating Doc</a></li><li><a href=https://github.com/datvodinh/rag-chatbot.git>Ollama RAG Chatbot</a> (Local Chat with multiple PDFs using Ollama and RAG)</li><li><a href=https://www.nurgo-software.com/products/brainsoup>BrainSoup</a> (Flexible native client with RAG & multi-agent automation)</li><li><a href=https://github.com/Renset/macai>macai</a> (macOS client for Ollama, ChatGPT, and other compatible API back-ends)</li><li><a href=https://github.com/Otacon/olpaka>Olpaka</a> (User-friendly Flutter Web App for Ollama)</li><li><a href=https://github.com/CrazyNeil/OllamaSpring>OllamaSpring</a> (Ollama Client for macOS)</li><li><a href=https://github.com/kartikm7/llocal>LLocal.in</a> (Easy to use Electron Desktop Client for Ollama)</li><li><a href=https://github.com/zeyoyt/ailama>AiLama</a> (A Discord User App that allows you to interact with Ollama anywhere in discord )</li><li><a href=https://github.com/rapidarchitect/ollama_mesop/>Ollama with Google Mesop</a> (Mesop Chat Client implementation with Ollama)</li><li><a href=https://github.com/mateuszmigas/painting-droid>Painting Droid</a> (Painting app with AI integrations)</li><li><a href=https://www.kerlig.com/>Kerlig AI</a> (AI writing assistant for macOS)</li><li><a href=https://github.com/MindWorkAI/AI-Studio>AI Studio</a></li><li><a href=https://github.com/gyopak/sidellama>Sidellama</a> (browser-based LLM client)</li><li><a href=https://github.com/trypromptly/LLMStack>LLMStack</a> (No-code multi-agent framework to build LLM agents and workflows)</li><li><a href=https://boltai.com>BoltAI for Mac</a> (AI Chat Client for Mac)</li><li><a href=https://github.com/av/harbor>Harbor</a> (Containerized LLM Toolkit with Ollama as default backend)</li><li><a href=https://www.jonathanhecl.com/go-crew/>Go-CREW</a> (Powerful Offline RAG in Golang)</li><li><a href=https://github.com/openvmp/partcad/>PartCAD</a> (CAD model generation with OpenSCAD and CadQuery)</li><li><a href=https://github.com/ollama4j/ollama4j-web-ui>Ollama4j Web UI</a> - Java-based Web UI for Ollama built with Vaadin, Spring Boot and Ollama4j</li><li><a href=https://github.com/kspviswa/pyOllaMx>PyOllaMx</a> - macOS application capable of chatting with both Ollama and Apple MLX models.</li><li><a href=https://github.com/saoudrizwan/claude-dev>Claude Dev</a> - VSCode extension for multi-file/whole-repo coding</li><li><a href=https://github.com/kangfenmao/cherry-studio>Cherry Studio</a> (Desktop client with Ollama support)</li><li><a href=https://github.com/1runeberg/confichat>ConfiChat</a> (Lightweight, standalone, multi-platform, and privacy focused LLM chat interface with optional encryption)</li><li><a href=https://github.com/nickthecook/archyve>Archyve</a> (RAG-enabling document library)</li><li><a href=https://github.com/rapidarchitect/ollama-crew-mesop>crewAI with Mesop</a> (Mesop Web Interface to run crewAI with Ollama)</li></ul><p>For mobile UI, you can explore these resources.</p><ul><li><a href=https://github.com/AugustDev/enchanted>Enchanted</a></li><li><a href=https://github.com/Mobile-Artificial-Intelligence/maid>Maid</a></li><li><a href=https://github.com/1runeberg/confichat>ConfiChat</a> (Lightweight, standalone, multi-platform, and privacy focused LLM chat interface with optional encryption)</li></ul><h2 id=question-if-i-want-to-use-all-these-ollama-models-via-jupyter-notebook-then-what-to-do>Question: If I want to use all these Ollama models via Jupyter Notebook then what to do?<a class=td-heading-self-link href=#question-if-i-want-to-use-all-these-ollama-models-via-jupyter-notebook-then-what-to-do aria-label="Heading self-link"></a></h2><p>There are dozens of libraries which integrate ollama models. You can pip install and use these libraries in your python code. Some of these popular libraries are:</p><ul><li><a href=https://python.langchain.com/docs/integrations/llms/ollama>LangChain</a> and <a href=https://js.langchain.com/docs/modules/model_io/models/llms/integrations/ollama>LangChain.js</a> with <a href=https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa>example</a></li><li><a href=https://firebase.google.com/docs/genkit/plugins/ollama>Firebase Genkit</a></li><li><a href=https://github.com/crewAIInc/crewAI>crewAI</a></li><li><a href=https://github.com/tmc/langchaingo/>LangChainGo</a> with <a href=https://github.com/tmc/langchaingo/tree/main/examples/ollama-completion-example>example</a></li><li><a href=https://github.com/langchain4j/langchain4j>LangChain4j</a> with <a href=https://github.com/langchain4j/langchain4j-examples/tree/main/ollama-examples/src/main/java>example</a></li><li><a href=https://github.com/Abraxas-365/langchain-rust>LangChainRust</a> with <a href=https://github.com/Abraxas-365/langchain-rust/blob/main/examples/llm_ollama.rs>example</a></li><li><a href=https://gpt-index.readthedocs.io/en/stable/examples/llm/ollama.html>LlamaIndex</a></li><li><a href=https://github.com/BerriAI/litellm>LiteLLM</a></li><li><a href=https://github.com/presbrey/ollamafarm>OllamaFarm for Go</a></li><li><a href=https://github.com/awaescher/OllamaSharp>OllamaSharp for .NET</a></li><li><a href=https://github.com/gbaptista/ollama-ai>Ollama for Ruby</a></li><li><a href=https://github.com/pepperoni21/ollama-rs>Ollama-rs for Rust</a></li><li><a href=https://github.com/jmont-dev/ollama-hpp>Ollama-hpp for C++</a></li><li><a href=https://github.com/ollama4j/ollama4j>Ollama4j for Java</a></li><li><a href=https://modelfusion.dev/integration/model-provider/ollama>ModelFusion Typescript Library</a></li><li><a href=https://github.com/kevinhermawan/OllamaKit>OllamaKit for Swift</a></li><li><a href=https://github.com/breitburg/dart-ollama>Ollama for Dart</a></li><li><a href=https://github.com/cloudstudio/ollama-laravel>Ollama for Laravel</a></li><li><a href=https://github.com/davidmigloz/langchain_dart>LangChainDart</a></li><li><a href=https://github.com/microsoft/semantic-kernel/tree/main/python/semantic_kernel/connectors/ai/ollama>Semantic Kernel - Python</a></li><li><a href=https://github.com/deepset-ai/haystack-integrations/blob/main/integrations/ollama.md>Haystack</a></li><li><a href=https://github.com/brainlid/langchain>Elixir LangChain</a></li><li><a href=https://github.com/JBGruber/rollama>Ollama for R - rollama</a></li><li><a href=https://github.com/hauselin/ollama-r>Ollama for R - ollama-r</a></li><li><a href=https://github.com/lebrunel/ollama-ex>Ollama-ex for Elixir</a></li><li><a href=https://github.com/b-tocs/abap_btocs_ollama>Ollama Connector for SAP ABAP</a></li><li><a href=https://testcontainers.com/modules/ollama/>Testcontainers</a></li><li><a href=https://portkey.ai/docs/welcome/integration-guides/ollama>Portkey</a></li><li><a href=https://github.com/svilupp/PromptingTools.jl>PromptingTools.jl</a> with an <a href=https://svilupp.github.io/PromptingTools.jl/dev/examples/working_with_ollama>example</a></li><li><a href=https://github.com/Project-Llama/llamascript>LlamaScript</a></li><li><a href=https://docs.gollm.co/examples/ollama-example>Gollm</a></li><li><a href=https://github.com/xyproto/ollamaclient>Ollamaclient for Golang</a></li><li><a href=https://gitlab.com/tozd/go/fun>High-level function abstraction in Go</a></li><li><a href=https://github.com/ArdaGnsrn/ollama-php>Ollama PHP</a></li><li><a href=https://github.com/agents-flex/agents-flex>Agents-Flex for Java</a> with <a href=https://github.com/agents-flex/agents-flex/tree/main/agents-flex-llm/agents-flex-llm-ollama/src/test/java/com/agentsflex/llm/ollama>example</a></li></ul><h2 id=question-does-ollama-have-plugins-like-github-copilot-can-i-use-those-from-my-visual-code>Question: Does Ollama have plugins like github copilot? Can I use those from my visual code?<a class=td-heading-self-link href=#question-does-ollama-have-plugins-like-github-copilot-can-i-use-those-from-my-visual-code aria-label="Heading self-link"></a></h2><p>Yes, there are many plugins like that for different purpose apart from coding. Even for the coding there are dozens of plugin available with different capabilities. And you need not to pay for these plugins like you have to pay monthly to Micorosoft! Some of those plugins are :</p><ul><li><a href=https://github.com/ex3ndr/llama-coder>Llama Coder</a> (Copilot alternative using Ollama)</li><li><a href=https://github.com/bernardo-bruning/ollama-copilot>Ollama Copilot</a> (Proxy that allows you to use ollama as a copilot like Github copilot)</li><li><a href=https://github.com/logancyang/obsidian-copilot>Copilot for Obsidian plugin</a></li><li><a href=https://github.com/MassimilianoPasquini97/raycast_ollama>Raycast extension</a></li><li><a href=https://github.com/mxyng/discollama>Discollama</a> (Discord bot inside the Ollama discord channel)</li><li><a href=https://github.com/continuedev/continue>Continue</a></li><li><a href=https://github.com/hinterdupfinger/obsidian-ollama>Obsidian Ollama plugin</a></li><li><a href=https://github.com/omagdy7/ollama-logseq>Logseq Ollama plugin</a></li><li><a href=https://github.com/andersrex/notesollama>NotesOllama</a> (Apple Notes Ollama plugin)</li><li><a href=https://github.com/samalba/dagger-chatbot>Dagger Chatbot</a></li><li><a href=https://github.com/mekb-turtle/discord-ai-bot>Discord AI Bot</a></li><li><a href=https://github.com/ruecat/ollama-telegram>Ollama Telegram Bot</a></li><li><a href=https://github.com/ej52/hass-ollama-conversation>Hass Ollama Conversation</a></li><li><a href=https://github.com/abrenneke/rivet-plugin-ollama>Rivet plugin</a></li><li><a href=https://github.com/longy2k/obsidian-bmo-chatbot>Obsidian BMO Chatbot plugin</a></li><li><a href=https://github.com/herval/cliobot>Cliobot</a> (Telegram bot with Ollama support)</li><li><a href=https://github.com/pfrankov/obsidian-local-gpt>Obsidian Local GPT plugin</a></li><li><a href=https://docs.openinterpreter.com/language-model-setup/local-models/ollama>Open Interpreter</a></li><li><a href=https://github.com/rjmacarthy/twinny>twinny</a> (Copilot and Copilot chat alternative using Ollama)</li><li><a href=https://github.com/RussellCanfield/wingman-ai>Wingman-AI</a> (Copilot code and chat alternative using Ollama and Hugging Face)</li><li><a href=https://github.com/n4ze3m/page-assist>Page Assist</a> (Chrome Extension)</li><li><a href=https://github.com/imoize/plasmoid-ollamacontrol>Plasmoid Ollama Control</a> (KDE Plasma extension that allows you to quickly manage/control Ollama model)</li><li><a href=https://github.com/tusharhero/aitelegrambot>AI Telegram Bot</a> (Telegram bot using Ollama in backend)</li><li><a href=https://github.com/yaroslavyaroslav/OpenAI-sublime-text>AI ST Completion</a> (Sublime Text 4 AI assistant plugin with Ollama support)</li><li><a href=https://github.com/kevinthedang/discord-ollama>Discord-Ollama Chat Bot</a> (Generalized TypeScript Discord Bot w/ Tuning Documentation)</li><li><a href=https://github.com/rapmd73/Companion>Discord AI chat/moderation bot</a> Chat/moderation bot written in python. Uses Ollama to create personalities.</li><li><a href=https://github.com/nischalj10/headless-ollama>Headless Ollama</a> (Scripts to automatically install ollama client & models on any OS for apps that depends on ollama server)</li><li><a href=https://github.com/jk011ru/vnc-lm>vnc-lm</a> (A containerized Discord bot with support for attachments and web links)</li><li><a href=https://github.com/SilasMarvin/lsp-ai>LSP-AI</a> (Open-source language server for AI-powered functionality)</li><li><a href=https://github.com/Palm1r/QodeAssist>QodeAssist</a> (AI-powered coding assistant plugin for Qt Creator)</li><li><a href=https://github.com/ECuiDev/obsidian-quiz-generator>Obsidian Quiz Generator plugin</a></li></ul><h2 id=question-what-kind-of-software-are-lm-studio-or-ollama>Question: What kind of software are LM Studio or Ollama?<a class=td-heading-self-link href=#question-what-kind-of-software-are-lm-studio-or-ollama aria-label="Heading self-link"></a></h2><p>Their role is to facilitate easy use of these models by providing a platform that supports multiple models, offering features like local deployment, training, and experimentation without needing to deal with the complex setup each model requires.</p><p>They are platform or interface for LLM: Both Ollama and LM Studio are software tools that allow users to interact with, run, fine-tune, and experiment with multiple large language models. They are more like model management tools or LLM execution environments rather than models themselves.</p><p>They are Model Hub: These tools serve as hubs where you can load, execute, and work with a variety of pre-trained LLMs. Instead of being limited to one specific model like GPT-4, they allow users to work with models like LLaMA, GPT-3, GPT-4, and others.</p><p>They are Model Runners: Since they enable the running and execution of multiple models.</p><p>They are LLM Execution/Management Tools: They manage various models and allow you to deploy them.</p><p>You can think of them as infrastructure that abstracts away the complexities of working with different LLMs.</p><h2 id=question-what-is-lm-studio-and-how-different-it-is-from-ollama>Question: What is LM Studio and how different it is from Ollama?<a class=td-heading-self-link href=#question-what-is-lm-studio-and-how-different-it-is-from-ollama aria-label="Heading self-link"></a></h2><p><strong>Ollama</strong> is designed for ease of use and running pre-trained models locally, perfect for developers and non-technical users who prioritize simplicity, privacy, and API-based integration.
<strong>LM Studio</strong> is a research-oriented tool for AI engineers and researchers who need in-depth control over model fine-tuning, training, and experimentation, with a steeper learning curve but greater flexibility.</p><p>Here&rsquo;s a detailed comparison of <strong>Ollama</strong> and <strong>LM Studio</strong> in terms of their capabilities:</p><table><thead><tr><th><strong>Feature/Capability</strong></th><th><strong>Ollama</strong></th><th><strong>LM Studio</strong></th></tr></thead><tbody><tr><td><strong>Primary Purpose</strong></td><td>Run and manage multiple LLMs locally</td><td>Research, experimentation, fine-tuning, and training of LLMs</td></tr><tr><td><strong>Supported Models</strong></td><td>LLaMA, GPT-3, GPT-4, and other popular LLMs</td><td>LLaMA, GPT, custom LLMs, and more, with a focus on fine-tuning</td></tr><tr><td><strong>Local Model Execution</strong></td><td>Supports running models locally without cloud dependencies</td><td>Allows for local execution, including training and experimentation</td></tr><tr><td><strong>Model Fine-tuning/Training</strong></td><td>No, typically runs pre-trained models</td><td>Yes, built for fine-tuning LLMs on custom datasets</td></tr><tr><td><strong>Experimentation Tools</strong></td><td>Minimal experimentation features, more focused on simple deployment</td><td>Extensive tools for experimenting with models, datasets, and hyperparameters</td></tr><tr><td><strong>Ease of Use</strong></td><td>Simple, user-friendly interface for non-technical users</td><td>More advanced, with a steeper learning curve but richer in functionality for researchers</td></tr><tr><td><strong>Hardware Requirements</strong></td><td>Optimized for running on GPUs or CPUs locally</td><td>Requires higher-end hardware (GPUs) for fine-tuning and training</td></tr><tr><td><strong>Privacy</strong></td><td>Strong privacy due to local model execution</td><td>Supports local execution for privacy, but also scales to cloud-based setups</td></tr><tr><td><strong>API Integration</strong></td><td>Yes, offers APIs to integrate LLMs into custom applications</td><td>Primarily a standalone platform, with some ability for integration into workflows</td></tr><tr><td><strong>Cloud Integration</strong></td><td>Primarily local execution; not designed for cloud-based workflows</td><td>Supports both local and cloud-based training environments, useful for large-scale training</td></tr><tr><td><strong>Model Deployment</strong></td><td>Can be deployed locally or integrated via API into applications</td><td>Typically used for experimentation, research, and training, with some deployment capabilities</td></tr><tr><td><strong>Pre-Trained Models</strong></td><td>Easy access to pre-trained models (LLaMA, GPT, etc.)</td><td>Access to a variety of pre-trained models (hugging face and others), with emphasis on customization and fine-tuning</td></tr><tr><td><strong>Target Audience</strong></td><td>Developers, non-technical users who want easy local LLM access</td><td>AI researchers, developers, engineers who require deeper control and experimentation</td></tr><tr><td><strong>Community & Support</strong></td><td>Developer-focused community</td><td>Strong research community with contributions from AI developers</td></tr></tbody></table><h3 id=key-capabilities-of-ollama>Key Capabilities of <strong>Ollama</strong>:<a class=td-heading-self-link href=#key-capabilities-of-ollama aria-label="Heading self-link"></a></h3><ol><li><strong>Run LLMs Locally</strong>: Focuses on running pre-trained models such as GPT-3, GPT-4, and LLaMA on your local machine without requiring cloud dependencies.</li><li><strong>Simple Setup</strong>: Aimed at developers and non-technical users who want easy access to LLMs.</li><li><strong>Privacy & Security</strong>: Since models run locally, no data is sent to external servers, enhancing privacy.</li><li><strong>API Integration</strong>: Provides APIs to integrate models into applications, making it useful for local deployment.</li><li><strong>Resource Optimization</strong>: Automatically manages local system resources, including CPU and GPU, to run models efficiently.</li></ol><h3 id=key-capabilities-of-lm-studio>Key Capabilities of <strong>LM Studio</strong>:<a class=td-heading-self-link href=#key-capabilities-of-lm-studio aria-label="Heading self-link"></a></h3><ol><li><strong>Fine-Tuning LLMs</strong>: Supports fine-tuning pre-trained LLMs on custom datasets, ideal for research and development.</li><li><strong>Model Training</strong>: Enables the training of LLMs from scratch or with specific hyperparameter configurations.</li><li><strong>Advanced Experimentation</strong>: Provides tools to run experiments, tweak models, and monitor results for research purposes.</li><li><strong>Customizable Infrastructure</strong>: Gives more control over hardware resources and configuration, allowing for scaling on cloud or local machines.</li><li><strong>Open-Source Platform</strong>: Built for researchers, it has a rich ecosystem of community-driven features and tools.</li></ol><h2 id=question-what-are-different-formats-to-save-model-specifically-llms>Question: What are different formats to save model, specifically LLMs?<a class=td-heading-self-link href=#question-what-are-different-formats-to-save-model-specifically-llms aria-label="Heading self-link"></a></h2><p>Large language models (LLMs) can be stored in various formats, each suited for different purposes and platforms.
These formats cater to different needs, from interoperability between frameworks (ONNX) to specific hardware optimizations (OpenVINO, TensorFlow Lite). The choice of format depends on the specific requirements of the deployment environment and the tools being used.</p><p>Here are some common model formats used for LLMs:</p><h3 id=1-pytorch-><strong>1. PyTorch (<code>.pt</code>, <code>.pth</code>)</strong><a class=td-heading-self-link href=#1-pytorch- aria-label="Heading self-link"></a></h3><ul><li><strong>Description</strong>: Files with <code>.pt</code> or <code>.pth</code> extensions are commonly used to store PyTorch models. These files contain the model&rsquo;s weights and architecture.</li><li><strong>Usage</strong>: Typically used with PyTorch frameworks for loading and running models.</li><li><strong>Example</strong>: Models saved using <code>torch.save(model.state_dict(), 'model.pth')</code>.</li></ul><h3 id=2-tensorflow-><strong>2. TensorFlow (<code>.pb</code>, <code>.h5</code>, <code>.tf</code>)</strong><a class=td-heading-self-link href=#2-tensorflow- aria-label="Heading self-link"></a></h3><ul><li><strong>Description</strong>: TensorFlow models can be saved in multiple formats:<ul><li><strong><code>.pb</code> (Protocol Buffers)</strong>: Used for saving the complete model, including weights and architecture.</li><li><strong><code>.h5</code> (HDF5)</strong>: Used for saving models in Keras (which is a high-level API for TensorFlow).</li><li><strong><code>.tf</code></strong>: Used for saving TensorFlow models in the SavedModel format.</li></ul></li><li><strong>Usage</strong>: Used with TensorFlow for model deployment and inference.</li><li><strong>Example</strong>: Models saved using <code>model.save('model.h5')</code> or <code>tf.saved_model.save(model, 'saved_model')</code>.</li></ul><h3 id=3-onnx-><strong>3. ONNX (<code>.onnx</code>)</strong><a class=td-heading-self-link href=#3-onnx- aria-label="Heading self-link"></a></h3><ul><li><strong>Description</strong>: Open Neural Network Exchange (ONNX) is a format for representing deep learning models. It allows interoperability between different deep learning frameworks.</li><li><strong>Usage</strong>: Enables models trained in one framework (like PyTorch) to be used in another (like TensorFlow).</li><li><strong>Example</strong>: Models converted to ONNX using <code>torch.onnx.export(model, inputs, 'model.onnx')</code>.</li></ul><h3 id=4-openvino-><strong>4. OpenVINO (<code>.bin</code>, <code>.xml</code>)</strong><a class=td-heading-self-link href=#4-openvino- aria-label="Heading self-link"></a></h3><ul><li><strong>Description</strong>: OpenVINO uses <code>.bin</code> and <code>.xml</code> files to represent optimized models for Intel hardware.</li><li><strong>Usage</strong>: Provides acceleration for inference on Intel devices.</li><li><strong>Example</strong>: Models optimized with OpenVINO are stored in <code>.xml</code> (model structure) and <code>.bin</code> (weights) files.</li></ul><h3 id=5-gguf-><strong>5. GGUF (<code>.gguf</code>)</strong><a class=td-heading-self-link href=#5-gguf- aria-label="Heading self-link"></a></h3><ul><li><strong>Description</strong>: Generalized Graph Universal Format (GGUF) is a format used by Meta for storing LLaMA models. It provides a standardized way to store and share large language models.</li><li><strong>Usage</strong>: <font color=green>Specifically designed for LLaMA models but can be used more broadly for LLMs.</font></li><li><strong>Example</strong>: Models saved in GGUF format will have the <code>.gguf</code> file extension.</li></ul><h3 id=6-savedmodel-><strong>6. SavedModel (<code>SavedModel</code>)</strong><a class=td-heading-self-link href=#6-savedmodel- aria-label="Heading self-link"></a></h3><ul><li><strong>Description</strong>: TensorFlow&rsquo;s SavedModel format includes a directory with serialized model weights, graph definitions, and metadata.</li><li><strong>Usage</strong>: TensorFlow&rsquo;s recommended format for serving models in production.</li><li><strong>Example</strong>: SavedModel format directory includes files like <code>saved_model.pb</code> and a variables directory.</li></ul><h3 id=7-core-ml-><strong>7. Core ML (<code>.mlmodel</code>)</strong><a class=td-heading-self-link href=#7-core-ml- aria-label="Heading self-link"></a></h3><ul><li><strong>Description</strong>: Apple&rsquo;s Core ML format is used for deploying models on iOS, macOS, watchOS, and tvOS.</li><li><strong>Usage</strong>: Used for integrating machine learning models into Apple applications.</li><li><strong>Example</strong>: Models converted to Core ML using tools like <code>coremltools</code>.</li></ul><h3 id=8-tensorflow-lite-><strong>8. TensorFlow Lite (<code>.tflite</code>)</strong><a class=td-heading-self-link href=#8-tensorflow-lite- aria-label="Heading self-link"></a></h3><ul><li><strong>Description</strong>: A format for deploying TensorFlow models on mobile and edge devices. It provides a smaller, more efficient representation of the model.</li><li><strong>Usage</strong>: Optimized for mobile and embedded devices.</li><li><strong>Example</strong>: Models converted to TensorFlow Lite format using <code>tf.lite.TFLiteConverter</code>.</li></ul><h3 id=9-hugging-face-><strong>9. Hugging Face (<code>.bin</code>, <code>config.json</code>, <code>tokenizer.json</code>)</strong><a class=td-heading-self-link href=#9-hugging-face- aria-label="Heading self-link"></a></h3><ul><li><strong>Description</strong>: Hugging Face models typically use <code>.bin</code> files for weights and JSON files for configuration and tokenizers. This format is often associated with the Transformers library.</li><li><strong>Usage</strong>: Used with Hugging Face’s Transformers library for loading and fine-tuning models.</li><li><strong>Example</strong>: Models from Hugging Face&rsquo;s model hub include <code>.bin</code> files for weights and configuration files.</li></ul><h3 id=10-hugging-face-><strong>10. Hugging Face (<code>.safetensors</code>)</strong><a class=td-heading-self-link href=#10-hugging-face- aria-label="Heading self-link"></a></h3><ul><li><strong>Description</strong>: SafeTensors (developed recently by hugging face) is a format developed for safely and efficiently storing tensor data, particularly for large language models. It aims to provide secure and consistent handling of model weights.</li><li><strong>Usage</strong>: Designed to improve safety and integrity in model storage by addressing issues related to file corruption and ensuring the integrity of the model data. It&rsquo;s increasingly used in machine learning and AI communities for its security benefits.</li><li><strong>Key Features</strong>:<ul><li><strong>Safety</strong>: Ensures data integrity and helps prevent corruption.</li><li><strong>Efficiency</strong>: Optimized for storage and retrieval of large model weights.</li><li><strong>Compatibility</strong>: Designed to be used with various frameworks and tools that support tensor-based models.</li></ul></li></ul><h3 id=summary-of-model-formats-including-safetensors>Summary of Model Formats Including SafeTensors:<a class=td-heading-self-link href=#summary-of-model-formats-including-safetensors aria-label="Heading self-link"></a></h3><ol><li><strong>PyTorch (<code>.pt</code>, <code>.pth</code>)</strong></li><li><strong>TensorFlow (<code>.pb</code>, <code>.h5</code>, <code>.tf</code>)</strong></li><li><strong>ONNX (<code>.onnx</code>)</strong></li><li><strong>Hugging Face (<code>.bin</code>, <code>config.json</code>, <code>tokenizer.json</code>)</strong></li><li><strong>GGUF (<code>.gguf</code>)</strong></li><li><strong>SavedModel (<code>SavedModel</code>)</strong></li><li><strong>Core ML (<code>.mlmodel</code>)</strong></li><li><strong>TensorFlow Lite (<code>.tflite</code>)</strong></li><li><strong>OpenVINO (<code>.bin</code>, <code>.xml</code>)</strong></li><li><strong>SafeTensors (<code>.safetensors</code>)</strong></li></ol><h2 id=question-what-is-gguf-model-extention>Question: What is gguf model extention?<a class=td-heading-self-link href=#question-what-is-gguf-model-extention aria-label="Heading self-link"></a></h2><p>The <strong>GGUF</strong> (Generalized Graph Universal Format) is designed to provide a standardized format for storing and sharing large language models. It aims to facilitate the interoperability of models across different platforms and tools. GGUF is particularly associated with Meta’s LLaMA (Large Language Model Meta AI) series of models. It is used for representing the weights and configurations of these models in a way that can be easily loaded and utilized across different environments.</p><p>GGUF format aims:</p><ul><li><strong>Standardization</strong>: GGUF aims to standardize how model data is stored and exchanged, making it easier to work with LLaMA models and potentially other models that adopt this format.</li><li><strong>Efficiency</strong>: The format is designed to efficiently handle the large size of modern language models, ensuring that models can be loaded and processed quickly.</li></ul><h2 id=question-if-i-have-finetuned-my-models-using-clouds-like-aws-sagemaker-vertexai-azure-and-kept-there-then-can-i-use-them-inside-my-ollama-and-lm-studio>Question: If I have finetuned my models using clouds like aws sagemaker, vertexai, azure and kept there then can I use them inside my ollama and LM Studio?<a class=td-heading-self-link href=#question-if-i-have-finetuned-my-models-using-clouds-like-aws-sagemaker-vertexai-azure-and-kept-there-then-can-i-use-them-inside-my-ollama-and-lm-studio aria-label="Heading self-link"></a></h2><p>Yes, we can use them</p><p><strong>Method 1: API integration</strong></p><ul><li>Obtain the endpoint URL and API key from cloud platform (Vertex/AWS/Azure) ML.</li><li>Prepare your environment for making HTTP requests.</li><li>Send requests to the API endpoint using tools like Python’s requests library.</li><li>Integrate the API calls into LM Studio or other tools.</li><li>Test and validate the integration to ensure it functions correctly.</li></ul><p><strong>Method 2: Model conversion and export</strong><br>Export Models: Export models from the cloud services in formats compatible with Ollama (e.g., ONNX, TensorFlow SavedModel). This might involve transferring the model files. Import into Ollama: If Ollama supports these formats, you can then import the models into Ollama’s environment.</p><h2 id=references>References<a class=td-heading-self-link href=#references aria-label="Heading self-link"></a></h2><ul><li><a href=https://ollama.com>https://ollama.com</a></li><li><a href=https://github.com/ollama/ollama>https://github.com/ollama/ollama</a></li></ul><div class=category-section><h4 class=category-section__title>Categories:</h4><div class=category-badges><a href=https://localhost:1313/categories/dsblog class=category-badge>dsblog</a></div></div><div class=td-tags><h4 class=td-tags__title>Tags:</h4><div class=category-badges><a href=https://localhost:1313/tags/llm class=category-badge>LLM</a><a href=https://localhost:1313/tags/ollama class=category-badge>Ollama</a><a href=https://localhost:1313/tags/meta class=category-badge>Meta</a><a href=https://localhost:1313/tags/large-language-model class=category-badge>Large Language Model</a></div></div><div class=td-author-box><div class=td-author-box__info><h4 class=td-author-box__name>Blowfish</h4><p class=td-author-box__bio>A powerful, lightweight theme for Hugo.</p><div class=td-author-box__links><a href target=_blank rel=noopener aria-label></a><a href target=_blank rel=noopener aria-label></a></div></div></div><div class=td-social-share><h4 class=td-social-share__title>Share this article:</h4><ul class=td-social-share__list><div class=social-share><a href="https://twitter.com/intent/tweet?text=Exploring%20Ollama%20%26amp%3b%20LM%20Studio&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fExploring-Ollama%2f" target=_blank rel=noopener aria-label="Share on Twitter"><i class="fab fa-twitter"></i>
</a><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fExploring-Ollama%2f" target=_blank rel=noopener aria-label="Share on Facebook"><i class="fab fa-facebook"></i>
</a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fExploring-Ollama%2f&title=Exploring%20Ollama%20%26amp%3b%20LM%20Studio" target=_blank rel=noopener aria-label="Share on LinkedIn"><i class="fab fa-linkedin"></i>
</a><a href="https://www.reddit.com/submit?url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fExploring-Ollama%2f&title=Exploring%20Ollama%20%26amp%3b%20LM%20Studio" target=_blank rel=noopener aria-label="Share on Reddit"><i class="fab fa-reddit"></i>
</a><a href="mailto:?subject=Exploring%20Ollama%20%26amp%3b%20LM%20Studio&body=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fExploring-Ollama%2f" aria-label="Share via Email"><i class="fas fa-envelope"></i></a></div></ul></div><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href=https://localhost:1313/dsblog/Exploring-Github/ aria-label="Previous page"><div class=td-pager__meta><i class="fa-solid fa-angle-left"></i>
<span class=td-pager__meta-label><b>Previous:</b></span>
<span class=td-pager__meta-title>Exploring Github</span></div></a><a class="td-pager__link td-pager__link--next" href=https://localhost:1313/dsblog/GenAI-Capabilities-from-AWS+GCP+Azure/ aria-label="Next page"><div class=td-pager__meta><span class=td-pager__meta-label><b>Next:</b></span>
<span class=td-pager__meta-title>GenAI Capabilities from AWS, Azure and GCP</span>
<i class="fa-solid fa-angle-right"></i></div></a></ul></div><div class=col-md-2></div></div></main><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class=col-2><a href=https://dasarpai.com target=_blank rel=noopener><img src=https://localhost:1313/assets/images/site-logo.png alt=dasarpAI width=100 style=border-radius:12px></a></div><div class=col-8><div class=row></div><div class=row></div></div><div class=col-2></div><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"></div></div></div></footer></div><script src=https://localhost:1313/js/main.min.7c31f98e62c36b0c9c834ce3f8260a0e21895dd6aa0773e81a64b104eae3b2e8.js integrity="sha256-fDH5jmLDawycg0zj+CYKDiGJXdaqB3PoGmSxBOrjsug=" crossorigin=anonymous></script><script defer src=https://localhost:1313/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=https://localhost:1313/js/tabpane-persist.js></script><script src=https://localhost:1313/js/asciinema-player.js></script><script>(function(){var e=document.querySelector("#td-section-nav");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></body></html>