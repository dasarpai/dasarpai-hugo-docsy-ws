<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.147.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href="../../favicons/favicon.ico?v=1"><link rel=apple-touch-icon href="../../favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><link rel=icon type=image/png href="../../favicons/favicon-16x16.png?v=1" sizes=16x16><link rel=icon type=image/png href="../../favicons/favicon-32x32.png?v=1" sizes=32x32><link rel=apple-touch-icon href="../../favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><title>Types of Machine Learning | Blowfish</title><meta property="og:url" content="https://dasarpai.github.io/dsblog/Types-of-Machine-Learning/">
<meta property="og:site_name" content="Blowfish"><meta property="og:title" content="Types of Machine Learning"><meta property="og:description" content="Types of Machine Learning Introduction Machine learning is a field of artificial intelligence that focuses on developing algorithms that can learn from data and make predictions or decisions. There are several types of machine learning techniques, each with its strengths and weaknesses. In this post, we will explore some of the most commonly used machine learning techniques, including supervised learning, unsupervised learning, reinforcement learning, and more. This post is not about deep diving into these topics but to give you a oneliner understanding and the difference between these different techniques."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="dsblog"><meta property="article:published_time" content="2023-04-27T00:00:00+00:00"><meta property="article:modified_time" content="2023-04-27T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Artificial Intelligence"><meta property="article:tag" content="Neural Networks"><meta property="article:tag" content="Data Science"><meta property="article:tag" content="Supervised Learning"><meta itemprop=name content="Types of Machine Learning"><meta itemprop=description content="Types of Machine Learning Introduction Machine learning is a field of artificial intelligence that focuses on developing algorithms that can learn from data and make predictions or decisions. There are several types of machine learning techniques, each with its strengths and weaknesses. In this post, we will explore some of the most commonly used machine learning techniques, including supervised learning, unsupervised learning, reinforcement learning, and more. This post is not about deep diving into these topics but to give you a oneliner understanding and the difference between these different techniques."><meta itemprop=datePublished content="2023-04-27T00:00:00+00:00"><meta itemprop=dateModified content="2023-04-27T00:00:00+00:00"><meta itemprop=wordCount content="8382"><meta itemprop=keywords content="machine learning types,AI algorithms,deep learning methods,neural networks,supervised learning,unsupervised learning,reinforcement learning,ML techniques,artificial intelligence models"><meta name=twitter:card content="summary"><meta name=twitter:title content="Types of Machine Learning"><meta name=twitter:description content="Types of Machine Learning Introduction Machine learning is a field of artificial intelligence that focuses on developing algorithms that can learn from data and make predictions or decisions. There are several types of machine learning techniques, each with its strengths and weaknesses. In this post, we will explore some of the most commonly used machine learning techniques, including supervised learning, unsupervised learning, reinforcement learning, and more. This post is not about deep diving into these topics but to give you a oneliner understanding and the difference between these different techniques."><link rel=preload href=../../scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css as=style integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link href=../../scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css rel=stylesheet integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link rel=stylesheet type=text/css href=https://dasarpai.github.io/css/asciinema-player.css><script src=https://code.jquery.com/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><link rel=stylesheet href=../../css/custom.css><script src=../../js/lunr.js></script></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-light nav-shadow flex-column flex-md-row td-navbar"><a id=agones-top class=navbar-brand href=../../><svg id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg> <span class="text-uppercase fw-bold">Blowfish</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=../../><span class=active>Data Science</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/docs/><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/docs/><span>Docs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../community/><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/docs/shortcodes/><span>Short Codes</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=../../template/blog/><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://github.com/nunocoracao/blowfish><span></span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href>GitHub</a></li><li class="nav-item dropdown d-none d-lg-block"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Release</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://development.agones.dev>Development</a>
<a class=dropdown-item href=https://agones.dev></a><a class=dropdown-item href=https://1-47-0.agones.dev>1.47.0</a>
<a class=dropdown-item href=https://1-46-0.agones.dev>1.46.0</a>
<a class=dropdown-item href=https://1-45-0.agones.dev>1.45.0</a>
<a class=dropdown-item href=https://1-44-0.agones.dev>1.44.0</a>
<a class=dropdown-item href=https://1-43-0.agones.dev>1.43.0</a>
<a class=dropdown-item href=https://1-42-0.agones.dev>1.42.0</a>
<a class=dropdown-item href=https://1-41-0.agones.dev>1.41.0</a>
<a class=dropdown-item href=https://1-40-0.agones.dev>1.40.0</a>
<a class=dropdown-item href=https://1-39-0.agones.dev>1.39.0</a>
<a class=dropdown-item href=https://1-38-0.agones.dev>1.38.0</a>
<a class=dropdown-item href=https://1-37-0.agones.dev>1.37.0</a>
<a class=dropdown-item href=https://1-36-0.agones.dev>1.36.0</a>
<a class=dropdown-item href=https://1-35-0.agones.dev>1.35.0</a>
<a class=dropdown-item href=https://1-34-0.agones.dev>1.34.0</a>
<a class=dropdown-item href=https://1-33-0.agones.dev>1.33.0</a>
<a class=dropdown-item href=https://1-32-0.agones.dev>1.32.0</a>
<a class=dropdown-item href=https://1-31-0.agones.dev>1.31.0</a></div></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><ul class=dropdown-menu><li><a class=dropdown-item href=../../it/>Italiano</a></li></ul></div></li></ul></div><div class="navbar-nav mx-lg-2 d-none d-lg-block"></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><div class=row><div class=col-md-2></div><div class=col-md-8><p><img src=../../assets/images/dspost/dsp6056-Types-of-Machine-Learning.jpg alt="Types of Machine Learning"></p><h1 id=types-of-machine-learning>Types of Machine Learning<a class=td-heading-self-link href=#types-of-machine-learning aria-label="Heading self-link"></a></h1><h2 id=introduction>Introduction<a class=td-heading-self-link href=#introduction aria-label="Heading self-link"></a></h2><p>Machine learning is a field of artificial intelligence that focuses on developing algorithms that can learn from data and make predictions or decisions. There are several types of machine learning techniques, each with its strengths and weaknesses. In this post, we will explore some of the most commonly used machine learning techniques, including supervised learning, unsupervised learning, reinforcement learning, and more. This post is not about deep diving into these topics but to give you a oneliner understanding and the difference between these different techniques.</p><p>Before we dive into different kind of learning let&rsquo;s understand few related important concepts.</p><h2 id=what-is-pre-training-and-what-is-pre-trained-model>What is pre-training and what is pre-trained model?<a class=td-heading-self-link href=#what-is-pre-training-and-what-is-pre-trained-model aria-label="Heading self-link"></a></h2><p><strong>Pre-Training</strong>: The process of training a machine learning model on a large, general dataset to learn broad patterns and features.</p><p><strong>Pre-Trained Model</strong>: A model that has already undergone pre-training and learned general features, which can be further adapted or fine-tuned for specific tasks.</p><p>Pre-trained models are developed using extensive and diverse datasets, sometime diverse modalities, allowing them to learn a broad range of features, patterns, and representations. For example, a pre-trained image classification model might be trained on millions of images across various categories.</p><p>Pre-trained models are often used in transfer learning, where they are adapted to a new, often more specific task. Instead of training a model from scratch, which can be time-consuming and require large amounts of data, you start with a pre-trained model and fine-tune it on your specific dataset.</p><p>Using pre-trained models can significantly reduce training time and computational resources, as the model already has learned useful features and representations that can be leveraged for new tasks.</p><p>In NLP Models like GPT-3, BERT, and T5 are pre-trained on large text corpora and can be fine-tuned for specific tasks like sentiment analysis, translation, or summarization. In Computer Vision Models like ResNet, VGG, and EfficientNet are pre-trained on large image datasets like ImageNet and can be fine-tuned for tasks like object detection or medical image analysis.</p><h3 id=benefits-of-pretrained-models>Benefits of Pretrained Models<a class=td-heading-self-link href=#benefits-of-pretrained-models aria-label="Heading self-link"></a></h3><ul><li><strong>Reduced Training Time</strong>: Leveraging existing pre-trained models saves time compared to training a model from scratch.</li><li><strong>Improved Performance</strong>: Pre-trained models often achieve better performance on specific tasks due to their exposure to extensive and diverse data during pre-training.</li><li><strong>Resource Efficiency</strong>: Less computational power is required since the model is already trained on large datasets.</li></ul><h2 id=fine-tuning-vs-shot-learning>Fine-tuning vs shot-learning<a class=td-heading-self-link href=#fine-tuning-vs-shot-learning aria-label="Heading self-link"></a></h2><p>Fine-tuning and shot learning looks same but here is a shuttle difference between them.</p><table><thead><tr><th>Aspect</th><th>Fine-Tuning</th><th>Shot Learning</th></tr></thead><tbody><tr><td><strong>Definition</strong></td><td>Further training of a pre-trained model on a smaller, task-specific dataset.</td><td>Training a model to perform well with very few examples or even just one.</td></tr><tr><td><strong>Training Data</strong></td><td>Typically involves a moderate-sized dataset related to the specific task.</td><td>Involves minimal data (few-shot or one-shot examples).</td></tr><tr><td><strong>Weight Updates</strong></td><td>Weights of the model are updated during the fine-tuning process.</td><td>Often involves minimal weight updates (for deep learing), no weight update in LLM; focuses on adapting to new tasks quickly.</td></tr><tr><td><strong>Purpose</strong></td><td>Adapt a general-purpose model to a specific application or domain.</td><td>Enable quick adaptation to new tasks or classes with limited examples.</td></tr><tr><td><strong>Examples</strong></td><td>Fine-tuning a language model on specific industry jargon/concept/process/idea.</td><td>Using a few examples to teach a model to classify new types of images.</td></tr><tr><td><strong>Techniques</strong></td><td>Transfer learning, domain adaptation.</td><td>Meta-learning, prototypical networks, relation networks.</td></tr><tr><td><strong>Computational Cost</strong></td><td>Moderate, depending on the size of the fine-tuning dataset.</td><td>Low, as only a small amount of data is used for adaptation.</td></tr></tbody></table><h2 id=shot-learning-in-the-context-of-deep-learning-vs-llm>Shot learning in the context of Deep Learning vs LLM<a class=td-heading-self-link href=#shot-learning-in-the-context-of-deep-learning-vs-llm aria-label="Heading self-link"></a></h2><p>Due to LLM flood in the market, nowadays when people hear about shot learning they think it is prompt engineering. But shot-learning is different in the context of deep learning vs its usage in LLM.</p><p>Certainly! Here’s a comparison of shot learning in the context of Deep Learning versus Large Language Models (LLMs):</p><table><thead><tr><th>Aspect</th><th>Deep Learning</th><th>Large Language Models (LLMs)</th></tr></thead><tbody><tr><td><strong>Definition</strong></td><td>Training models to recognize or perform tasks with very few examples.</td><td>Adapting models to handle new tasks with minimal examples in the prompt.</td></tr><tr><td><strong>Training Process</strong></td><td>Involves techniques like meta-learning, where the model&rsquo;s weights are updated with a few examples.</td><td>Uses in-context learning, where the model adapts to tasks based on the examples provided in the prompt without updating weights.</td></tr><tr><td><strong>Weight Updates</strong></td><td>Model weights are updated during training with few examples (e.g., few-shot learning).</td><td>No weight updates; the model uses examples to infer how to perform the task at inference time.</td></tr><tr><td><strong>Examples</strong></td><td>Few-shot learning for image classification where the model is fine-tuned with a few images per class.</td><td>Few-shot prompting for text generation where a few example inputs and outputs are provided in the prompt.</td></tr><tr><td><strong>Contextual Adaptation</strong></td><td>Adapts through learning mechanisms specific to the task (e.g., metric learning or prototype-based methods).</td><td>Adapts using the context provided in the prompt to understand and perform new tasks.</td></tr><tr><td><strong>Common Techniques</strong></td><td>Prototypical Networks, Matching Networks, Meta-Learning (e.g., MAML).</td><td>In-context learning, prompt engineering, zero-shot and few-shot prompting.</td></tr><tr><td><strong>Computational Cost</strong></td><td>Can be higher due to fine-tuning or training with few examples.</td><td>Generally lower, as it involves using the pre-trained model directly with example prompts.</td></tr><tr><td><strong>Data Efficiency</strong></td><td>Requires careful selection of few examples for effective learning.</td><td>Leverages pre-trained knowledge to perform well with minimal data in prompts.</td></tr></tbody></table><h2 id=what-is-zero-shot-learning>What is Zero-shot learning?<a class=td-heading-self-link href=#what-is-zero-shot-learning aria-label="Heading self-link"></a></h2><p><strong>Zero-Shot Learning</strong> is a technique where a model performs a task or recognizes a class it was not explicitly trained on, using its pre-existing knowledge. Keep in mind this is not learning in traditional sense, because there is not weight and bias updatation happening. A pretrained model is being used for inference purpose in the development environment. If you are satisfied then you can move it to production.</p><h3 id=key-points>Key Points<a class=td-heading-self-link href=#key-points aria-label="Heading self-link"></a></h3><ul><li><strong>Pre-Trained Knowledge</strong>: The model has been trained on a large and diverse dataset, learning general patterns and features.</li><li><strong>No Additional Training</strong>: The model performs the new task without further training or weight updates specific to that task.</li><li><strong>Task Generalization</strong>: The model uses its broad understanding from pre-training to generalize and handle new tasks or categories.</li><li><strong>Inference</strong>: The process involves applying learned knowledge to new tasks based on descriptions, prompts, or context provided.</li></ul><h3 id=example>Example<a class=td-heading-self-link href=#example aria-label="Heading self-link"></a></h3><p>A model trained on general text data can classify sentiment, answer questions, or perform named entity recognition on new, unseen tasks by leveraging its pre-trained knowledge.</p><p>In essence, zero-shot learning enables models to tackle new tasks effectively without task-specific training.</p><p><strong>Now let&rsquo;s understand different type of machine learning, which is the main topic of this artcile.</strong></p><h2 id=machine-learning-techniques>Machine Learning Techniques<a class=td-heading-self-link href=#machine-learning-techniques aria-label="Heading self-link"></a></h2><h3 id=1-supervised-learning>1. Supervised learning<a class=td-heading-self-link href=#1-supervised-learning aria-label="Heading self-link"></a></h3><p>It is a type of machine learning where the model is trained on labeled data. In supervised learning, the model learns from a set of examples where the input data and the corresponding output labels are provided. Supervised learning is used for most common regression and classification tasks.</p><p>Some examples of Supervised learning algorithms:</p><ul><li>Linear regression</li><li>Logistic regression</li><li>Decision trees</li><li>Random forests</li><li>Support vector machines (SVMs)</li><li>Naive Bayes</li><li>K-nearest neighbors (KNN)</li><li>Neural networks</li><li>Gradient Boosting Machines (GBMs)</li><li>Convolutional Neural Networks (CNNs)</li><li>Recurrent Neural Networks (RNNs)</li><li>Long Short-Term Memory (LSTM)</li><li>Gated Recurrent Unit (GRU)</li><li>Extreme Gradient Boosting (XGBoost)</li><li>LightGBM.</li></ul><h3 id=2-unsupervised-learning>2. Unsupervised learning<a class=td-heading-self-link href=#2-unsupervised-learning aria-label="Heading self-link"></a></h3><p>It is a type of machine learning where the model learns to recognize patterns in data without the need for labeled examples. Unsupervised learning is used for clustering, dimensionality reduction, and anomaly detection.</p><p>Some examples of unsupervised learning algorithms:</p><ul><li>Clustering Algorithms (K-Means, Hierarchical Clustering, DBSCAN, etc.)</li><li>Dimensionality Reduction Algorithms (PCA, t-SNE, LLE, etc.)</li><li>Anomaly Detection Algorithms (Isolation Forest, One-Class SVM, etc.)</li><li>Association Rule Learning Algorithms (Apriori, ECLAT, etc.)</li><li>Generative Adversarial Networks (GANs)</li><li>Autoencoders</li><li>Self-Organizing Maps (SOMs)</li><li>Independent Component Analysis (ICA)</li><li>Principal Component Analysis (PCA)</li><li>Non-negative Matrix Factorization (NMF)</li></ul><h3 id=3-reinforcement-learning>3. Reinforcement learning<a class=td-heading-self-link href=#3-reinforcement-learning aria-label="Heading self-link"></a></h3><p>It is a type of machine learning where the model learns through trial and error by interacting with an environment. The model receives feedback in the form of rewards or penalties for the actions it takes and learns to maximize the rewards over time. RL is used when we don&rsquo;t have enough data to train the model, or when the data does not represent the real world, which has changed or changing fast. RL model building is time-consuming because learning is the process of acting. A robot that can perform a task where the law of physics needs to be obeyed needs RL.</p><p>Some examples of reinforcement learning algorithms:</p><ul><li>Q-Learning: Q-learning is a model-free reinforcement learning algorithm that learns to determine the value of an action in a particular state. It does not require a model of the environment and can handle problems with stochastic transitions and rewards.</li><li>SARSA (State-Action-Reward-State-Action): SARSA is another model-free reinforcement learning algorithm that is similar to Q-learning. However, instead of learning the value of the optimal action in a given state, it learns the value of the action actually taken.</li><li>Deep Q-Networks (DQN): DQN is a deep reinforcement learning algorithm that uses a neural network to approximate the action-value function. It has been successfully used to play Atari games and other complex tasks.</li><li>Actor-Critic: Actor-Critic is a family of reinforcement learning algorithms that combine elements of policy-based and value-based methods. It uses two networks, one to estimate the value of a state and another to estimate the policy.</li><li>Proximal Policy Optimization (PPO): PPO is a policy optimization method that updates the policy in small steps while ensuring that the policy does not move too far away from the previous policy. It has been shown to be effective in a wide range of tasks, including robotics and game playing.</li><li>Asynchronous Advantage Actor-Critic (A3C): A3C is a distributed version of the Actor-Critic algorithm that allows multiple agents to learn concurrently. It has been shown to be effective in both continuous and discrete action spaces.</li></ul><h3 id=4-semi-supervised-learning>4. Semi-supervised learning<a class=td-heading-self-link href=#4-semi-supervised-learning aria-label="Heading self-link"></a></h3><p>It is a type of machine learning that combines both supervised and unsupervised learning. Semi-supervised learning uses a small amount of labeled data along with a large amount of unlabeled data to train the model. Here, first, we create a model with small labeled data and then use the model to predict the label of unlabelled data. After that, we use these predicted labels and create a bigger dataset. Now from this bigger dataset, we can further train the model. We keep doing this and making note of how the model is performing on the originally labeled data. Semi-supervised clustering is used in applications such as gene expression analysis and social network analysis.</p><p>Some popular approaches for semi-supervised learning include:</p><ul><li>Self-Training: Self-training is a simple approach for semi-supervised learning. It involves training a model on the labeled data and then using that model to predict labels for the unlabeled data. The confident predictions are then added to the labeled dataset, and the process is repeated.</li><li>Co-Training: Co-Training is a semi-supervised learning approach where the algorithm uses multiple views of the data to train models. It involves training multiple models, each on a different subset of the features, and then exchanging the predicted labels for the unlabeled data between the models.</li><li>Generative Models: Generative models, such as Variational Autoencoders (VAE) and Generative Adversarial Networks (GAN), can also be used for semi-supervised learning. These models learn to generate new data points that are similar to the training data and can be used to label the unlabeled data.</li><li>Graph-Based Methods: Graph-based semi-supervised learning methods use the structure of the data to propagate the labels from the labeled to the unlabeled data. Some examples of graph-based methods include Label Propagation and Graph Convolutional Networks (GCN).</li><li>Entropy Regularization: Entropy regularization is a technique that encourages the model to make confident predictions by penalizing the entropy of the model&rsquo;s output distribution. This technique can be used to make the most of the limited labeled data by leveraging the information in the unlabeled data.</li></ul><h3 id=5-transfer-learning>5. Transfer learning<a class=td-heading-self-link href=#5-transfer-learning aria-label="Heading self-link"></a></h3><p>It is a technique that enables a model to transfer knowledge learned from one task to another. Transfer learning can be used to improve the performance of a model on a new task where labeled data is scarce or where a task is very common and data is similar. TL is very common in Computer Vision, NLP, and reinforcement Learning.</p><p>Although every neural network based model can be used for transfer learning but for example sake various models for transfer learning are:</p><ul><li>VGG: VGG is a popular deep learning model known for its accuracy on image classification tasks. It is often used as a pre-trained model for transfer learning.</li><li>ResNet: ResNet is another popular deep learning model known for its performance on image classification tasks. It has several variants, including ResNet-50 and ResNet-101.</li><li>BERT: BERT is a popular pre-trained model for natural language processing (NLP) tasks, including text classification and question answering.</li><li>GPT: GPT (Generative Pre-trained Transformer) is a pre-trained model for NLP tasks, including language modeling, question answering, and text completion.</li><li>Inception: Inception is a deep learning model known for its performance on image classification tasks. It has several variants, including InceptionV3 and Inception-ResNet.</li><li>MobileNet: MobileNet is a deep learning model optimized for mobile devices. It is often used for transfer learning on image classification tasks.</li><li>AlexNet: AlexNet is a deep learning model known for its accuracy on image classification tasks. It is often used as a pre-trained model for transfer learning.</li></ul><h3 id=6-online-learning>6. Online learning<a class=td-heading-self-link href=#6-online-learning aria-label="Heading self-link"></a></h3><p>It is a type of machine learning where the model learns incrementally from incoming data streams. Online learning is used for real-time applications where the model needs to continuously adapt to new data. So parameters of the model keep updating after some time or after a certain number of observations. When data is too large to fit in the memory or changing fast, like millions of people buying thousands of products online, fraud detection, online advertisement, a live cast of an event, city traffic, mass political rally, city law, and order situation, especially in tense time, etc.</p><p>Examples of online learning algorithms</p><ul><li>Online Gradient Descent</li><li>Stochastic Gradient Descent</li><li>Adaptive Gradient Descent</li><li>Passive-Aggressive Algorithm</li><li>Online Random Forests</li><li>Online Naive Bayes</li><li>Online Support Vector Machines</li><li>Recursive Least Squares</li><li>Incremental Decision Trees</li><li>Perceptron Algorithm</li></ul><h3 id=7-deep-learning>7. Deep learning<a class=td-heading-self-link href=#7-deep-learning aria-label="Heading self-link"></a></h3><p>It is a subset of machine learning that uses artificial neural networks with multiple layers to learn from data. Deep learning has achieved state-of-the-art results in tasks such as image recognition, natural language processing, and speech recognition.</p><p>Some examples of deep learning algorithms:</p><ul><li>Convolutional Neural Networks (CNNs) - used for image and video analysis</li><li>Recurrent Neural Networks (RNNs) - used for natural language processing, speech recognition, and time series analysis</li><li>Long Short-Term Memory (LSTM) networks - a type of RNN used for modeling sequences with long-term dependencies</li><li>Generative Adversarial Networks (GANs) - used for generating new data that resembles the training data</li><li>Deep Belief Networks (DBNs) - used for unsupervised learning, feature learning, and classification tasks</li><li>Autoencoders - used for dimensionality reduction, feature extraction, and image denoising</li><li>Deep Reinforcement Learning (DRL) - is used for decision-making tasks in environments with complex rules and states.</li></ul><h3 id=8-evolutionary-algorithms>8. Evolutionary algorithms<a class=td-heading-self-link href=#8-evolutionary-algorithms aria-label="Heading self-link"></a></h3><p>Evolutionary algorithms are a family of optimization algorithms that are inspired by the process of natural selection. In this approach, a population of candidate solutions is evolved over multiple generations to find the best solution to a given problem. The basic idea is to apply the principles of survival of the fittest, reproduction, and mutation to search for optimal solutions.</p><p>An example of an evolutionary algorithm is the genetic algorithm. In this algorithm, the candidate solutions are represented as chromosomes, which are encoded as binary strings or vectors of real numbers. The algorithm starts with a population of randomly generated chromosomes and then applies selection, crossover, and mutation operators to create new offspring chromosomes for the next generation.</p><p>During the selection phase, the best chromosomes in the population are chosen based on their fitness, which is a measure of how well they solve the problem. The fitness function is problem-specific and evaluates the quality of the solution represented by each chromosome. The selected chromosomes are then combined through crossover, which involves swapping parts of their genetic material, to create new offspring chromosomes. Finally, the offspring chromosomes are subject to mutation, which randomly changes their genetic material to introduce new variations into the population.</p><p>This process is repeated over multiple generations, with the hope that the best solution will emerge over time. Evolutionary algorithms have been used to solve a wide range of problems, including optimization, control, and machine learning.</p><p>Evolutionary algorithms are often used when other methods such as deep learning or Bayesian learning are not feasible or appropriate for the problem at hand.</p><p>Some examples of Evolutionary algorithms are</p><ul><li>Genetic algorithm (GA)</li><li>Differential evolution (DE)</li><li>Particle swarm optimization (PSO)</li><li>Ant colony optimization (ACO)</li><li>Artificial bee colony (ABC)</li><li>Cultural algorithm (CA)</li><li>Harmony search (HS)</li><li>Firefly algorithm (FA)</li><li>Bat algorithm (BA)</li><li>Grey wolf optimizer (GWO)</li><li>Cuckoo search (CS)</li><li>Imperialist competitive algorithm (ICA)</li><li>Whale optimization algorithm (WOA)</li><li>Gravitational search algorithm (GSA)</li><li>Multi-objective optimization algorithms (MOO) such as NSGA-II and SPEA2.</li></ul><h3 id=9-bayesian-learning>9. Bayesian learning<a class=td-heading-self-link href=#9-bayesian-learning aria-label="Heading self-link"></a></h3><p>Bayesian learning is a probabilistic approach to machine learning that involves using Bayes&rsquo; theorem to update the probability of a hypothesis as new evidence is acquired. In Bayesian learning, we start with a prior belief about the probability of a hypothesis and then update this belief as new evidence becomes available. The result is a posterior probability distribution over the hypotheses that reflects our updated belief.</p><p>An example of Bayesian learning is in the spam email classification. Suppose we have a dataset of emails that are labeled as either spam or not spam. We want to build a model that can classify new emails as spam or not spam. We can use Bayesian learning to build a spam filter by treating each email as a hypothesis and updating our belief about the probability of the email being spam as new evidence becomes available.</p><p>To do this, we start by assigning a prior probability distribution to the hypothesis (i.e., the email being spam or not spam) based on our prior knowledge about the dataset. As we receive new emails and classify them as spam or not spam, we update the prior probability distribution using Bayes&rsquo; theorem. Specifically, we use the likelihood of the data (i.e., the features of the email) given the hypothesis and the prior probability distribution to calculate the posterior probability distribution over the hypotheses.</p><p>Bayesian learning has the advantage of being able to incorporate prior knowledge into the learning process and update this knowledge as new evidence becomes available. This makes it particularly useful in situations where data is limited and prior knowledge is available.</p><p>Examples of Bayesian learning algorithms:</p><ul><li>Naive Bayes Classifier: It is a simple probabilistic classifier based on Bayes&rsquo; theorem. It assumes that the features are independent of each other.</li><li>Bayesian Linear Regression: It is a regression technique that uses Bayesian inference to estimate the parameters of a linear regression model.</li><li>Bayesian Neural Networks: It is a type of neural network that uses Bayesian inference to estimate the parameters of the network. It can provide uncertainty estimates for the model predictions.</li><li>Bayesian Decision Theory: It is a framework that uses Bayesian inference to make decisions based on uncertain or incomplete information.</li><li>Bayesian Belief Networks: It is a graphical model that represents a set of random variables and their conditional dependencies using a directed acyclic graph. It can be used for probabilistic inference and decision-making.</li></ul><h3 id=10-instance-based-learning>10. Instance-based learning<a class=td-heading-self-link href=#10-instance-based-learning aria-label="Heading self-link"></a></h3><p>Instance-based learning, also known as lazy learning, is a type of machine learning in which the algorithm learns by memorizing the training dataset instead of constructing a general model. It is a type of machine learning that relies on similarity measures between instances in a dataset to make predictions. In this approach, the algorithm makes predictions based on the similarity between new instances and the instances in the training dataset.</p><p>For example, consider a dataset containing information about different cars such as their make, model, year of manufacture, mileage, and price. We want to predict the price of a new car based on its make, model, year of manufacture, and mileage.</p><p>In instance-based learning, the algorithm would simply store the entire training dataset in memory, and when a new car is presented, it would find the k most similar cars in the training dataset and use their prices to predict the price of the new car. The similarity between instances can be calculated using distance measures such as Euclidean distance, Manhattan distance, or cosine similarity.</p><p>One popular instance-based learning algorithm is k-nearest neighbors (k-NN), where k is the number of nearest neighbors to consider. In the car price prediction example, if k is set to 3, the algorithm would find the 3 most similar cars in the training dataset and use their prices to predict the price of the new car.</p><p>Instance-based learning has the advantage of being able to handle complex and nonlinear relationships in the data, but it can be computationally expensive and requires a large amount of memory to store the training dataset. Instance-based learning is used in applications such as recommendation systems and anomaly detection.</p><p>Examples of instance-based learning algorithms are:</p><ul><li>k-Nearest Neighbors (k-NN)</li><li>Learning Vector Quantization (LVQ)</li><li>Case-based reasoning (CBR)</li><li>Locally weighted regression (LWR)</li><li>Self-organizing map (SOM)</li></ul><h3 id=11-kernel-methods>11. Kernel methods<a class=td-heading-self-link href=#11-kernel-methods aria-label="Heading self-link"></a></h3><p>These are techniques that use kernel functions to transform data into a higher-dimensional space, where linear models can be used to make predictions. Kernel methods are used in tasks such as classification, regression, and clustering. One of the most popular kernel methods is the support vector machine (SVM), which uses a kernel function to map input data into a higher-dimensional space and finds a hyperplane that maximizes the margin between two classes of data. Other examples of kernel methods include kernel PCA (principal component analysis) for dimensionality reduction, kernel k-means for clustering, and Gaussian processes for regression. Kernel methods are widely used in various domains, including image and speech recognition, natural language processing, and bioinformatics. They are particularly useful when the data has a complex structure and cannot be easily modeled using linear methods.</p><p>Examples of kernel methods algorithms:</p><ul><li>Support Vector Machines (SVM)</li><li>Kernel Principal Component Analysis (KPCA)</li><li>Kernel Canonical Correlation Analysis (KCCA)</li><li>Gaussian Processes (GP)</li><li>Kernel Ridge Regression (KRR)</li><li>Locally Linear Embedding (LLE) with the kernel trick</li><li>Multiple Kernel Learning (MKL)</li><li>Kernel Discriminant Analysis (KDA)</li><li>Spectral Clustering with the kernel trick</li><li>Regularized Least Squares Classification (RLSC)</li></ul><h3 id=12-decision-tree-learning>12. Decision tree learning<a class=td-heading-self-link href=#12-decision-tree-learning aria-label="Heading self-link"></a></h3><p>It is a method that builds a tree-like model of decisions and their possible consequences. Decision tree learning is used in many domains like medical diagnosis, fraud detection and credit risk analysis. Its simplicity and interpretability make it a popular choice for many businesses and organizations.</p><p>Some examples of decision tree learning algorithms:</p><ul><li>ID3 (Iterative Dichotomiser 3): This is one of the earliest decision tree algorithms developed by Ross Quinlan in 1986. It uses the entropy measure to find the best split at each node.</li><li>C4.5: This is an extension of the ID3 algorithm, developed by Quinlan in 1993. It uses the gain ratio measure to overcome the bias of ID3 towards attributes with a large number of values.</li><li>CART (Classification and Regression Trees): This algorithm was developed by Breiman, Friedman, Olshen, and Stone in 1984. CART can be used for both classification and regression tasks and uses the Gini index as a measure of impurity.</li></ul><h3 id=13-ensemble-learning>13. Ensemble learning<a class=td-heading-self-link href=#13-ensemble-learning aria-label="Heading self-link"></a></h3><p>It is a technique that combines multiple models to improve their accuracy and robustness. Ensemble learning is used in applications such as classification and regression.</p><p>Some examples of ensemble learning algorithms:</p><ul><li>Random Forest: It is an ensemble of decision trees that are trained on different subsets of the dataset and use a random subset of features for each split. The final prediction is the mode of the predictions of all trees.</li><li>Bagging: It is an ensemble of models trained on bootstrapped samples of the dataset. The final prediction is the average of predictions of all models.</li><li>Stacking: It is an ensemble of models where the predictions of base models are used as input to a meta-model that learns to combine them. The final prediction is the output of the meta-model.</li><li>AdaBoost: AdaBoost is short for &ldquo;Adaptive Boosting&rdquo;. In this algorithm, each base learner is trained on the samples that are misclassified by the previous base learners. It is called &ldquo;adaptive&rdquo; because the weights of the misclassified samples are increased, making them more important in subsequent iterations. This allows AdaBoost to focus on the difficult samples and achieve high accuracy with a small number of base learners. The final prediction is the weighted sum of the predictions of all learners.</li><li>Gradient Boosting: Gradient Boosting is similar to AdaBoost, but it uses the gradient of the loss function with respect to the model&rsquo;s prediction as a measure of the sample&rsquo;s importance. In each iteration, a new base learner is trained to fit the negative gradient of the loss function. This allows Gradient Boosting to handle differentiable loss functions and achieve high accuracy on a wide range of tasks. The final prediction is the sum of the predictions of all learners.</li><li>XGBoost: XGBoost stands for &ldquo;Extreme Gradient Boosting&rdquo;. It is a more advanced version of Gradient Boosting that includes several additional features, such as regularization, tree pruning, and parallel processing. These features allow XGBoost to handle larger datasets and achieve even higher accuracy than Gradient Boosting.</li><li>LightGBM: LightGBM is a more recent boosting algorithm that is designed to be faster and more memory-efficient than XGBoost. It achieves this by using a novel approach called &ldquo;Gradient-based One-Side Sampling&rdquo; (GOSS) to select the most informative samples for each base learner. This allows LightGBM to achieve high accuracy with a smaller number of base learners and lower memory usage.</li><li>CatBoost: CatBoost is another recent boosting algorithm that is designed to handle categorical features more effectively than other boosting algorithms. It achieves this by using a novel approach called &ldquo;Ordered Boosting&rdquo;, which creates a permutation of the categorical features that are ordered based on their impact on the loss function. This allows CatBoost to achieve higher accuracy on datasets with a large number of categorical features.</li></ul><h3 id=14-self-supervised-learning>14. Self-supervised learning<a class=td-heading-self-link href=#14-self-supervised-learning aria-label="Heading self-link"></a></h3><p>Self-supervised learning is a type of machine learning in which the algorithm learns from unlabeled data by creating labels from the data itself, rather than relying on human-labeled data.</p><p>Suppose we have a large dataset of unlabeled images, and we want to train a model to recognize objects in those images. In a self-supervised approach, we can create labels for the images by asking the model to perform a related but easier task, such as predicting the rotation angle of the image.</p><p>We start by randomly rotating each image by a certain angle (e.g., 90 degrees) to create a new, rotated version of the image. We then train the model to predict the rotation angle of the rotated image, given the original, unrotated image as input. The model learns to predict the rotation angle by extracting and encoding features from the image, such as the location of edges, corners, and other patterns.</p><p>After training on this task, the model has learned a set of features that are useful for predicting the rotation angle but can also be used for other related tasks, such as object recognition. We can then fine-tune the model on a smaller set of labeled data, such as images with labeled object categories, to improve its performance on the specific task of object recognition.</p><p>Overall, self-supervised learning is a powerful technique for training machine learning models on large amounts of unlabeled data and can lead to significant improvements in performance when combined with smaller amounts of labeled data. It has been applied to a wide range of tasks in computer vision, natural language processing, video processing, and other areas of machine learning.</p><p>Some examples of self-supervised learning:</p><ul><li><p>Contrastive Predictive Coding (CPC): In this method, a neural network is trained to predict the relationship between different parts of the same input. For example, in an image, the network learns to predict the relationship between two different patches of the same image.</p></li><li><p>Autoencoder: An autoencoder is a neural network that learns to reconstruct the input data from a lower-dimensional representation. It is trained on unlabeled data, where the input and output are the same.</p></li><li><p>Generative Adversarial Networks (GANs): GANs are a type of self-supervised learning method where two neural networks are trained simultaneously. One network generates fake data, while the other network tries to distinguish the fake data from real data.</p></li><li><p>Pretext tasks: Pretext tasks are a general category of self-supervised learning methods that involve training a neural network to perform a specific task that is not the ultimate task of interest, but is related to it. For example, a network may be trained to predict the rotation or colorization of an image and then fine-tuned on a downstream task such as image classification.</p></li></ul><h3 id=shot-learning>Shot Learning<a class=td-heading-self-link href=#shot-learning aria-label="Heading self-link"></a></h3><h4 id=15-zero-shot-learning>15. Zero-shot learning<a class=td-heading-self-link href=#15-zero-shot-learning aria-label="Heading self-link"></a></h4><p>Zero-shot learning is a machine learning technique that allows a model to recognize and classify objects that it has never seen before. In zero-shot learning, the model is trained on a set of classes but is expected to recognize and classify instances of new classes that were not part of the training set. This is achieved by learning a semantic embedding space that maps the objects to a semantic space, where classes are represented as points or regions, and objects are classified based on their proximity to these points or regions. Zero-shot learning is particularly useful in situations where it is difficult or impossible to obtain labeled training data for all possible classes. An example of zero-shot learning is recognizing a new species of bird based on its visual characteristics, without having any labeled examples of that bird species in the training set.</p><p>Various Models which help Zero-shot learning:</p><ul><li>Attribute-based models: These models learn a semantic representation of the attributes that describe the visual properties of objects, and use this to classify new objects.</li><li>Semantic embeddings: These models learn a mapping from the visual features of an object to a semantic embedding space, where objects can be compared and classified based on their similarity in the embedding space.</li><li>Generative models: These models learn a generative model of the data distribution, which can be used to generate new samples and classify new objects.</li><li>Hybrid models: These models combine different approaches, such as attribute-based and semantic embedding models, to improve performance and generalize across different types of objects.</li></ul><h4 id=16-one-shot-learning>16. One-shot learning<a class=td-heading-self-link href=#16-one-shot-learning aria-label="Heading self-link"></a></h4><p>It is a type of machine learning where the model can learn from a single example. One-shot learning is used in applications such as facial recognition and handwriting recognition.</p><p>Models for one-shot learning, including:</p><ul><li>Siamese Networks: Siamese Networks use a twin neural network architecture that learns to compare two images and output a similarity score. It is trained on a set of pairs of images, one from the same class and another from a different class. Once the network is trained, it can compare a new image to the training set and output a similarity score, allowing it to classify the new image.</li><li>Matching Networks: Matching Networks use a learnable attention mechanism to select a subset of the support set that is most relevant to the query image. This allows the network to effectively leverage the limited labeled data available in the few-shot scenario.</li><li>Prototypical Networks: Prototypical Networks learn a metric space where images of the same class are close together and images of different classes are far apart. The network learns to encode images as a set of prototypes, one for each class, and classify new images based on their similarity to the prototypes.</li><li>Meta-learning (or Learning-to-learn) Models: Meta-learning models learn to learn from a few examples by training on many similar tasks. The idea is to learn a set of shared parameters that can be quickly adapted to a new task with few examples. One example of such models is MAML (Model-Agnostic Meta-Learning).</li></ul><h4 id=17-few-shot-learning>17. Few-shot learning<a class=td-heading-self-link href=#17-few-shot-learning aria-label="Heading self-link"></a></h4><p>Few-shot learning is a type of machine learning where a model is trained to learn from a few examples of a new task or class. This is useful in scenarios where the dataset for a particular task is limited, but the model needs to generalize well and perform well on unseen data.</p><p>For example, suppose we have a dataset of images of animals and want to train a model to recognize new species of animals that have only a few images available. In traditional machine learning, we would need a large labeled dataset of the new species to train a model that performs well. In few-shot learning, we can train a model using a small number of examples of the new species, for example, only 5 or 10 images, and then use this model to recognize new instances of the species.</p><p>In few-shot learning, the model is typically trained on a small number of examples of the new task or class, along with examples from other tasks or classes. The model then uses this knowledge to recognize new instances of the new task or class.</p><p>Various Models which help few-shot learning:</p><ul><li>Siamese Networks: Siamese networks consist of two or more identical neural networks that share weights. They are often used for tasks such as image matching and one-shot learning, where the network learns to recognize similar or dissimilar pairs of inputs.</li><li>Prototypical Networks: Prototypical networks learn to classify new examples based on the distance to a set of learned prototypes or centroids. The prototypes are computed as the mean feature vectors of each class in the support set.</li><li>Matching Networks: Matching networks use attention mechanisms to weigh the importance of different parts of the input when making a decision. They are trained on a support set of labeled examples and can then generalize to new examples with few labels.</li><li>Meta-learning: Meta-learning, also known as &ldquo;learning to learn,&rdquo; involves training a model to learn how to learn from a few examples. This can be done using a variety of techniques, such as gradient-based meta-learning and metric-based meta-learning.</li><li>Model-agnostic Meta-learning (MAML): MAML is a meta-learning approach that aims to learn a good initialization for a neural network such that it can quickly adapt to new tasks with only a few examples. MAML is model-agnostic, meaning it can be applied to a wide range of deep-learning models.</li></ul><h3 id=18-active-learning>18. Active learning<a class=td-heading-self-link href=#18-active-learning aria-label="Heading self-link"></a></h3><p>Active learning is a machine learning technique where an algorithm iteratively selects a subset of the available data to be labeled by an expert or a human annotator, with the goal of maximizing the performance of the model while minimizing the cost of annotation. Suppose we want to build a model that can classify customer support tickets into one of several categories (e.g., billing, technical support, product issues). We have a large set of unlabeled support tickets, but we can only afford to label a small fraction of them due to the cost of annotation.</p><p>In an active learning approach, we start by randomly selecting a small subset of support tickets and annotating them with their corresponding category labels. We then train a classification model on this labeled subset, and use it to make predictions on the remaining unlabeled data.</p><p>Next, instead of randomly selecting additional examples to label, the active learning algorithm selects the examples that it is most uncertain about, based on the model&rsquo;s current prediction probabilities. These uncertain examples are then sent to a human annotator to be labeled, and added to the training set. The model is retrained on the expanded labeled dataset, and the process repeats until performance reaches a satisfactory level or the available budget for annotation is exhausted.</p><p>The key idea behind active learning is that by selectively choosing the most informative examples to be labeled, the algorithm can achieve better performance with fewer labeled examples than would be required by a random sampling approach.</p><p>Examples of algorithms for Active Learning are.</p><ul><li>Support Vector Machines (SVM): SVM is a popular model used for binary classification tasks in active learning. It is based on finding the hyperplane that separates the two classes with maximum margin.</li><li>Decision Trees: Decision trees can be used for classification tasks in active learning scenarios. They can be useful for selecting the most informative data points to label next.</li><li>Neural Networks: Neural networks have been used in active learning for both classification and regression tasks. They can be trained using various active learning strategies.</li><li>Bayesian Models: Bayesian models are probabilistic models that can be used in active learning to model the uncertainty of the model&rsquo;s predictions.</li><li>Random Forest: Random forests can be used for classification tasks in active learning scenarios. They can be useful for selecting the most informative data points to label next.</li></ul><h3 id=19-multi-task-learning>19. Multi-task learning<a class=td-heading-self-link href=#19-multi-task-learning aria-label="Heading self-link"></a></h3><p>It is a type of machine learning where the model learns to perform multiple tasks simultaneously. The idea is that by sharing some of the learned representations across multiple tasks, the model can achieve better performance on each individual task. Multi-task learning is used in applications where the tasks are related, and the data is scarce. Suppose we want to build a machine learning model that can perform two related NLP tasks: sentiment analysis and topic classification. Sentiment analysis involves determining whether a given text expresses a positive or negative sentiment, while topic classification involves identifying the topic or subject of the text. Multi-task learning can be thought of as a type of multi-label classification (since it involves predicting multiple labels for each example), the focus of multi-label classification is the classification task itself, while the focus of multi-task learning is shared representations or knowledge transfer between tasks.</p><p>Models used for multi-task learning are:</p><ul><li>Hard Parameter Sharing: In this model, the input features are fed into a shared hidden layer, and then the outputs of the hidden layer are connected to task-specific output layers. All the tasks share the same parameters in the hidden layer.</li><li>Soft Parameter Sharing: This model is similar to hard parameter sharing, but instead of using the same parameters for all the tasks, it allows some degree of flexibility in the parameters by adding a regularization term.</li><li>Neural Task Graph: In this model, the tasks are represented as nodes in a graph, and the edges between the nodes represent the relationships between the tasks. The input features are fed into the graph, and the output of each task is computed based on its relationship with the other tasks.</li><li>Hierarchical Multi-Task Learning: This model uses a hierarchy of tasks, where the lower-level tasks provide features to the higher-level tasks. The lower-level tasks are typically simpler and more general, while the higher-level tasks are more specific and complex.</li><li>Adversarial Multi-Task Learning: This model uses adversarial training to learn multiple tasks simultaneously. It involves training a generator network to generate examples that are similar to the training data, and a discriminator network to distinguish between the generated examples and the real examples. The tasks are learned by training the generator to generate examples that are specific to each task, while the discriminator is trained to recognize the task-specific examples.</li></ul><h3 id=20-active-transfer-learning>20. Active transfer learning<a class=td-heading-self-link href=#20-active-transfer-learning aria-label="Heading self-link"></a></h3><p>It is a type of machine learning where the model can transfer knowledge from &ldquo;related different tasks&rdquo; to improve its performance on the current task. Active transfer learning involves using knowledge acquired during the training of one model to improve the performance of a different, but related, model. This knowledge transfer typically involves fine-tuning the parameters of the new model using data from the old model, or using the old model as a source of features for the new model. Suppose we have a neural network that has been trained to classify images of dogs and cats. We now want to use this network to classify images of different breeds of dogs, but we only have a limited amount of labeled data for these new breeds. Active transfer learning is used in applications such as natural language processing and image recognition.</p><p>Some examples of models for active transfer learning include:</p><ul><li>Adaptive SVM: This model uses a transfer-learning framework to adjust SVM decision boundaries for the target task by incorporating information from the source domain. Active learning is also used to select informative samples from the source domain.</li><li>Multi-task Multi-view Gaussian Process (MT-MVGP): This model uses Gaussian processes to model multiple tasks and views simultaneously. It employs active transfer learning by selecting informative samples from the source domain to improve the accuracy of the target domain.</li><li>Transfer active learning with oracle-guided instance selection (TAOIS): This model uses a two-stage approach. In the first stage, it selects informative samples from the source domain using active learning. In the second stage, it selects instances from the target domain that are most similar to the informative samples from the source domain to reduce the transfer error.</li><li>Transfer Active Learning via Unsupervised Domain Adaptation (TALUDA): This model uses unsupervised domain adaptation to align the source and target domains. It then uses active learning to select the most informative samples from the source domain to adapt to the target domain.</li></ul><h3 id=21-meta-learning>21. Meta-learning<a class=td-heading-self-link href=#21-meta-learning aria-label="Heading self-link"></a></h3><p>It is a type of machine learning where the model learns how to learn. Meta-learning is used in applications such as few-shot learning and reinforcement learning. Let&rsquo;s say the meta-learner has been trained on a set of classification tasks, such as classifying medical images, kitchen tools, and whether the given text is offensive or not. After training, the meta-learner has learned to recognize patterns and make accurate predictions for each of these tasks. Now, suppose a new classification task is presented to the system. For example, the task might involve classifying different species of animals based on images of their footprints. This is a new task because it involves a different type of data (footprint images), and a different set of categories to classify (animal species). The meta-learner analyzes the characteristics of the new task with a few samples and then uses its learned knowledge to generate a new model that is tailored to this specific task.</p><p>Meta-learning can be approached through different types of models, some of which include:</p><ul><li>Metric-based models: These models learn a distance metric between examples and use it to classify new examples. One example of a metric-based model is Prototypical Networks.</li><li>Model-based models: These models learn how to optimize parameters for a set of tasks, and use these optimized parameters to perform well on new tasks. An example of a model-based meta-learning algorithm is Model-Agnostic Meta-Learning (MAML).</li><li>Memory-based models: These models use a memory bank of examples to perform few-shot learning. Examples of memory-based models include Matching Networks and Relation Networks.</li><li>Optimization-based models: These models learn a set of optimization rules that can be applied to different tasks. An example of an optimization-based model is Learning to Learn by Gradient Descent by Gradient Descent (L2L-GD).</li></ul><h3 id=22-deep-reinforcement-learning>22. Deep reinforcement learning<a class=td-heading-self-link href=#22-deep-reinforcement-learning aria-label="Heading self-link"></a></h3><p>It is a subset of reinforcement learning that uses deep neural networks to learn from the environment. Deep reinforcement learning is used in applications such as game playing and robotics.</p><p>Some popular models for deep reinforcement learning include:</p><ul><li>Deep Q-Networks (DQN): DQN is a model that uses a deep neural network to approximate the Q-value function, which represents the expected total reward of taking an action in a given state and following the optimal policy thereafter. DQN was introduced by Google DeepMind in 2015 and was able to achieve superhuman performance in several Atari games.</li><li>Actor-Critic Models: Actor-critic models combine a policy network (the actor) that selects actions and a value network (the critic) that evaluates the value of state-action pairs. The two networks are trained together using gradient descent to maximize the expected reward. Examples of actor-critic models include Asynchronous Advantage Actor-Critic (A3C) and Trust Region Policy Optimization (TRPO).</li><li>Deep Deterministic Policy Gradient (DDPG): DDPG is an actor-critic model that is specifically designed for continuous action spaces. It uses a deterministic policy, meaning that given a state, it outputs a specific action rather than a probability distribution over actions. DDPG has been applied successfully in a variety of domains, including robotics and autonomous driving.</li><li>Proximal Policy Optimization (PPO): PPO is a family of policy optimization methods that use a trust region approach to update the policy parameters. It is a simpler and more stable alternative to TRPO, and has been shown to achieve state-of-the-art performance on a variety of benchmark environments.</li><li>Dueling Network Architectures (Dueling DQN): Dueling DQN is a modification of the DQN architecture that separates the Q-value function into two parts: one that estimates the value of being in a particular state and another that estimates the advantage of taking a particular action in that state. This separation allows the agent to better generalize across actions and states and has been shown to improve performance on a number of benchmark tasks.</li></ul><h3 id=23-adversarial-learning>23. Adversarial learning<a class=td-heading-self-link href=#23-adversarial-learning aria-label="Heading self-link"></a></h3><p>It is a type of machine learning where the model is trained to defend against adversarial attacks. Adversarial learning is used in applications such as computer security and image recognition. If an image of &ldquo;apple&rdquo; is changed by adverserial attack but visually it looks the the same. In this situation, machine are bound to get confuse and identify this as some different object. We don&rsquo;t want machine should get confused by adverserial attack. We want machine should identify this is an &ldquo;apple&rdquo;, as human can see. At the same time machine should tell this has been changed, which human cannot see.</p><p>Some models commonly used in adversarial learning:</p><ul><li>Generative Adversarial Networks (GANs): GANs are a type of generative model that consists of a generator and a discriminator. The generator learns to generate realistic examples that fool the discriminator, while the discriminator learns to distinguish between real and generated examples.</li><li>Adversarial Autoencoders (AAEs): AAEs are a type of generative model that consist of an encoder, a decoder, and a discriminator. The encoder maps the input data to a latent space, the decoder maps the latent space back to the input data, and the discriminator learns to distinguish between real and reconstructed examples.</li><li>Adversarial Training: Adversarial training involves training a model on adversarial examples as well as regular examples. This can improve the model&rsquo;s robustness to adversarial attacks.</li><li>Adversarial Examples Detection: Adversarial examples detection involves training a separate model to detect adversarial examples, which can be used to identify and reject such examples during inference.</li><li>Adversarial Regularization: Adversarial regularization involves adding a regularization term to the loss function that encourages the model to produce outputs that are robust to adversarial examples.</li></ul><h3 id=24-contrastive-learning>24. Contrastive learning<a class=td-heading-self-link href=#24-contrastive-learning aria-label="Heading self-link"></a></h3><p>It is a type of machine learning where the model learns to distinguish between similar and dissimilar instances in a dataset. Contrastive learning is used in applications such as representation learning and image recognition. An image of an &ldquo;apple&rdquo;, a sound of word &ldquo;apple&rdquo;, and text &ldquo;apple&rdquo; written on piece of paper, &ldquo;apple&rdquo; written on the image of apple, all are representing an idea &ldquo;apple&rdquo;. If machine shows an image of apple and we type &ldquo;orange&rdquo; the system can say they both are different and your answer is wrong.</p><p>Some popular models for contrastive learning are:</p><ul><li>SimCLR (Simple Contrastive Learning of Representations) - A simple framework for contrastive learning that achieves state-of-the-art performance on a wide range of downstream tasks.</li><li>MoCo (Momentum Contrast) - A contrastive learning framework that uses a momentum encoder to update the representation of the query image.</li><li>BYOL (Bootstrap Your Own Latent) - A self-supervised learning method that learns representations by predicting the representation of an augmented view of the same image.</li><li>SwAV (Swapping Assignments between Modalities) - A method for contrastive learning that jointly learns representations from multiple modalities (e.g., images and text).</li></ul><h3 id=25-adaptive-learning>25. Adaptive Learning<a class=td-heading-self-link href=#25-adaptive-learning aria-label="Heading self-link"></a></h3><p>While buying online, or consuming online content, recommendation of new product or content is not based only on the past data about the product and buyer but what action buyer has taken recently. Adaptive learning gives more importance to the data points of current or present actions.</p><p>There are many models and techniques that fall under adaptive learning. Here are a few examples:</p><ul><li>Online gradient descent: This is a type of stochastic gradient descent (SGD) that updates the model parameters after every training example. It is commonly used in online learning, where data is streamed in real-time.</li><li>Actor-critic methods: These are reinforcement learning algorithms that have two components: an actor that selects actions, and a critic that evaluates the value of the selected actions. The actor-critic approach is widely used in robotics, gaming, and other applications.</li><li>Transfer learning: Transfer learning allows a model to leverage knowledge learned from a related task and apply it to a new task. This approach is commonly used in computer vision, natural language processing, and other fields.</li><li>Deep residual networks: These are deep neural networks that use residual connections to learn the difference between the input and the output, allowing for easier training and better performance.</li><li>Adaptive boosting (AdaBoost): This is an ensemble learning method that combines several weak learners to create a strong learner. The algorithm adapts the weight of the weak learners based on their performance. AdaBoost is commonly used in computer vision, speech recognition, and other applications.</li></ul><h2 id=26-federated-learning>26. Federated learning<a class=td-heading-self-link href=#26-federated-learning aria-label="Heading self-link"></a></h2><p>Federated learning is a machine learning approach that allows multiple devices or entities to collaboratively train a model without sharing their local data with each other or a central server. Instead, each entity trains the model on its local data and sends only the model parameters to a central server, which aggregates them to update the global model. This approach allows for privacy-preserving machine learning, as the raw data remains on the local devices and is not shared with any other parties.</p><p>Federated learning has applications in various domains, including healthcare, finance, and Internet of Things (IoT) devices. For example, in healthcare, federated learning can be used to train a predictive model on patient data without compromising their privacy. Similarly, in finance, federated learning can be used to train models on transaction data from multiple banks without sharing sensitive information between them</p><h2 id=conclusion>Conclusion<a class=td-heading-self-link href=#conclusion aria-label="Heading self-link"></a></h2><p>Machine learning techniques are essential for solving many real-world problems, from image recognition and natural language processing to self-driving cars and medical diagnosis. Each method has its strengths and limitations, and the choice of technique depends on the specific problem and the available data. By understanding the different types of machine learning techniques, you can select the most appropriate method for your project and achieve better results. As the field of machine learning continues to evolve, we can expect to see new and more powerful techniques emerge, making it an exciting time for anyone interested in artificial intelligence.</p><p><strong>Author</strong><br>Dr Hari Thapliyaal<br>dasarpai.com<br>linkedin.com/in/harithapliyal</p><div class=category-section><h4 class=category-section__title>Categories:</h4><div class=category-badges><a href=../../categories/dsblog class=category-badge>dsblog</a></div></div><div class=td-tags><h4 class=td-tags__title>Tags:</h4><div class=category-badges><a href=../../tags/machine-learning class=category-badge>Machine Learning</a><a href=../../tags/deep-learning class=category-badge>Deep Learning</a><a href=../../tags/artificial-intelligence class=category-badge>Artificial Intelligence</a><a href=../../tags/neural-networks class=category-badge>Neural Networks</a><a href=../../tags/data-science class=category-badge>Data Science</a><a href=../../tags/supervised-learning class=category-badge>Supervised Learning</a><a href=../../tags/unsupervised-learning class=category-badge>Unsupervised Learning</a><a href=../../tags/reinforcement-learning class=category-badge>Reinforcement Learning</a><a href=../../tags/ai-algorithms class=category-badge>AI Algorithms</a></div></div><div class=td-author-box><div class=td-author-box__info><h4 class=td-author-box__name>Blowfish</h4><p class=td-author-box__bio>A powerful, lightweight theme for Hugo.</p><div class=td-author-box__links><a href target=_blank rel=noopener aria-label></a><a href target=_blank rel=noopener aria-label></a></div></div></div><div class=td-social-share><h4 class=td-social-share__title>Share this article:</h4><ul class=td-social-share__list><div class=social-share><a href="https://twitter.com/intent/tweet?text=Types%20of%20Machine%20Learning&url=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fTypes-of-Machine-Learning%2f" target=_blank rel=noopener aria-label="Share on Twitter"><i class="fab fa-twitter"></i>
</a><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fTypes-of-Machine-Learning%2f" target=_blank rel=noopener aria-label="Share on Facebook"><i class="fab fa-facebook"></i>
</a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fTypes-of-Machine-Learning%2f&title=Types%20of%20Machine%20Learning" target=_blank rel=noopener aria-label="Share on LinkedIn"><i class="fab fa-linkedin"></i>
</a><a href="https://www.reddit.com/submit?url=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fTypes-of-Machine-Learning%2f&title=Types%20of%20Machine%20Learning" target=_blank rel=noopener aria-label="Share on Reddit"><i class="fab fa-reddit"></i>
</a><a href="mailto:?subject=Types%20of%20Machine%20Learning&body=https%3a%2f%2fdasarpai.github.io%2fdsblog%2fTypes-of-Machine-Learning%2f" aria-label="Share via Email"><i class="fas fa-envelope"></i></a></div></ul></div><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href=../../dsblog/Linux-OS-Directories/ aria-label="Previous page"><div class=td-pager__meta><i class="fa-solid fa-angle-left"></i>
<span class=td-pager__meta-label><b>Previous:</b></span>
<span class=td-pager__meta-title>Linux OS Directories</span></div></a><a class="td-pager__link td-pager__link--next" href=../../dsblog/File-Formats-in-Machine-Learning/ aria-label="Next page"><div class=td-pager__meta><span class=td-pager__meta-label><b>Next:</b></span>
<span class=td-pager__meta-title>The Interconnectedness of Life and Data</span>
<i class="fa-solid fa-angle-right"></i></div></a></ul></div><div class=col-md-2></div></div></main><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class=col-2><a href=https://dasarpai.com target=_blank rel=noopener><img src=https://dasarpai.github.io/assets/images/site-logo.png alt=dasarpAI width=100 style=border-radius:12px></a></div><div class=col-8><div class=row></div><div class=row></div></div><div class=col-2></div><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"></div></div></div></footer></div><script src=../../js/main.min.4e728f6d1777a74bef81c238fd7b573f42f1848a910503e07f877f49f1f442cb.js integrity="sha256-TnKPbRd3p0vvgcI4/XtXP0LxhIqRBQPgf4d/SfH0Qss=" crossorigin=anonymous></script><script defer src=../../js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=../../js/tabpane-persist.js></script><script src=https://dasarpai.github.io/js/asciinema-player.js></script><script>(function(){var e=document.querySelector("#td-section-nav");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></body></html>