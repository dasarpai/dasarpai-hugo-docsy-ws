<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.147.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href="https://localhost:1313/favicons/favicon.ico?v=1"><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-16x16.png?v=1" sizes=16x16><link rel=icon type=image/png href="https://localhost:1313/favicons/favicon-32x32.png?v=1" sizes=32x32><link rel=apple-touch-icon href="https://localhost:1313/favicons/apple-touch-icon-180x180.png?v=1" sizes=180x180><title>Ollama Setup and Running Models | Blowfish</title><meta property="og:url" content="https://localhost:1313/dsblog/Ollama-Setup-and-Running-Models/">
<meta property="og:site_name" content="Blowfish"><meta property="og:title" content="Ollama Setup and Running Models"><meta property="og:description" content="Ollama: Running Large Language Models Locally The landscape of Artificial Intelligence (AI) and Large Language Models (LLMs) has traditionally been dominated by cloud-based services. While powerful, these often come with costs, privacy concerns, and require constant internet connectivity. Ollama emerges as a compelling open-source solution, designed to simplify the process of downloading, managing, and running LLMs directly on your local machine. This approach offers significant advantages, including enhanced privacy, cost savings, offline capability, and greater control over the models you use."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="dsblog"><meta property="article:published_time" content="2025-04-19T00:00:00+00:00"><meta property="article:modified_time" content="2025-04-19T00:00:00+00:00"><meta property="article:tag" content="Ollama"><meta property="article:tag" content="Large Language Models"><meta property="article:tag" content="AI and NLP"><meta property="article:tag" content="Local Models"><meta property="article:tag" content="Cost Savings"><meta property="article:tag" content="Privacy"><meta itemprop=name content="Ollama Setup and Running Models"><meta itemprop=description content="Ollama: Running Large Language Models Locally The landscape of Artificial Intelligence (AI) and Large Language Models (LLMs) has traditionally been dominated by cloud-based services. While powerful, these often come with costs, privacy concerns, and require constant internet connectivity. Ollama emerges as a compelling open-source solution, designed to simplify the process of downloading, managing, and running LLMs directly on your local machine. This approach offers significant advantages, including enhanced privacy, cost savings, offline capability, and greater control over the models you use."><meta itemprop=datePublished content="2025-04-19T00:00:00+00:00"><meta itemprop=dateModified content="2025-04-19T00:00:00+00:00"><meta itemprop=wordCount content="1741"><meta itemprop=keywords content="ollama setup,running large language models locally,cost savings with ollama,enhanced privacy with ollama,offline large language models"><meta name=twitter:card content="summary"><meta name=twitter:title content="Ollama Setup and Running Models"><meta name=twitter:description content="Ollama: Running Large Language Models Locally The landscape of Artificial Intelligence (AI) and Large Language Models (LLMs) has traditionally been dominated by cloud-based services. While powerful, these often come with costs, privacy concerns, and require constant internet connectivity. Ollama emerges as a compelling open-source solution, designed to simplify the process of downloading, managing, and running LLMs directly on your local machine. This approach offers significant advantages, including enhanced privacy, cost savings, offline capability, and greater control over the models you use."><link rel=preload href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css as=style integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link href=https://localhost:1313/scss/main.min.8d5e9853aa86c416850fe7ccb39d048388bfe7fbefb7922f796f886c3c8084d8.css rel=stylesheet integrity="sha256-jV6YU6qGxBaFD+fMs50Eg4i/5/vvt5IveW+IbDyAhNg=" crossorigin=anonymous><link rel=stylesheet type=text/css href=https://localhost:1313/css/asciinema-player.css><script src=https://code.jquery.com/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><link rel=stylesheet href=https://localhost:1313/css/custom.css><script src=https://localhost:1313/js/lunr.js></script></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-light nav-shadow flex-column flex-md-row td-navbar"><a id=agones-top class=navbar-brand href=https://localhost:1313/><svg id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg> <span class="text-uppercase fw-bold">Blowfish</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=https://localhost:1313/><span class=active>Data Science</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/><span>Docs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/community/><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/docs/shortcodes/><span>Short Codes</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://localhost:1313/template/blog/><span>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://github.com/nunocoracao/blowfish><span></span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href>GitHub</a></li><li class="nav-item dropdown d-none d-lg-block"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Release</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=https://development.agones.dev>Development</a>
<a class=dropdown-item href=https://agones.dev></a><a class=dropdown-item href=https://1-47-0.agones.dev>1.47.0</a>
<a class=dropdown-item href=https://1-46-0.agones.dev>1.46.0</a>
<a class=dropdown-item href=https://1-45-0.agones.dev>1.45.0</a>
<a class=dropdown-item href=https://1-44-0.agones.dev>1.44.0</a>
<a class=dropdown-item href=https://1-43-0.agones.dev>1.43.0</a>
<a class=dropdown-item href=https://1-42-0.agones.dev>1.42.0</a>
<a class=dropdown-item href=https://1-41-0.agones.dev>1.41.0</a>
<a class=dropdown-item href=https://1-40-0.agones.dev>1.40.0</a>
<a class=dropdown-item href=https://1-39-0.agones.dev>1.39.0</a>
<a class=dropdown-item href=https://1-38-0.agones.dev>1.38.0</a>
<a class=dropdown-item href=https://1-37-0.agones.dev>1.37.0</a>
<a class=dropdown-item href=https://1-36-0.agones.dev>1.36.0</a>
<a class=dropdown-item href=https://1-35-0.agones.dev>1.35.0</a>
<a class=dropdown-item href=https://1-34-0.agones.dev>1.34.0</a>
<a class=dropdown-item href=https://1-33-0.agones.dev>1.33.0</a>
<a class=dropdown-item href=https://1-32-0.agones.dev>1.32.0</a>
<a class=dropdown-item href=https://1-31-0.agones.dev>1.31.0</a></div></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><ul class=dropdown-menu><li><a class=dropdown-item href=https://localhost:1313/it/>Italiano</a></li></ul></div></li></ul></div><div class="navbar-nav mx-lg-2 d-none d-lg-block"></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><div class=row><div class=col-md-2></div><div class=col-md-8><p><img src=https://localhost:1313/assets/images/dspost/dsp6262-Ollama-Setup-and-Running-Models.jpg alt="Ollama Setup and Running Models"></p><h1 id=ollama-running-large-language-models-locally>Ollama: Running Large Language Models Locally<a class=td-heading-self-link href=#ollama-running-large-language-models-locally aria-label="Heading self-link"></a></h1><p>The landscape of Artificial Intelligence (AI) and Large Language Models (LLMs) has traditionally been dominated by cloud-based services. While powerful, these often come with costs, privacy concerns, and require constant internet connectivity. Ollama emerges as a compelling open-source solution, designed to simplify the process of downloading, managing, and running LLMs directly on your local machine. This approach offers significant advantages, including enhanced privacy, cost savings, offline capability, and greater control over the models you use.</p><h2 id=why-choose-local-llms-with-ollama>Why Choose Local LLMs with Ollama?<a class=td-heading-self-link href=#why-choose-local-llms-with-ollama aria-label="Heading self-link"></a></h2><p>Running LLMs locally addresses several key challenges associated with cloud services:</p><ol><li><strong>Privacy and Security:</strong> When using local models via Ollama, your data doesn&rsquo;t need to leave your machine. This is crucial for handling sensitive information or for applications in sectors like healthcare and finance where data privacy is paramount.</li><li><strong>Cost Efficiency:</strong> Cloud-based LLM services often involve ongoing costs related to API calls or server usage. Ollama eliminates these costs, as you leverage your own hardware resources. Once a model is downloaded, running it incurs no additional expense.</li><li><strong>Reduced Latency:</strong> Local execution significantly reduces the network latency inherent in communicating with remote servers. This results in faster response times, which is beneficial for interactive applications.</li><li><strong>Offline Capability:</strong> Since the models run on your machine, you can use them even without an active internet connection (after the initial download).</li><li><strong>Customization and Flexibility:</strong> Ollama provides greater flexibility in customizing and fine-tuning models to suit specific needs, without the limitations imposed by third-party service providers.</li><li><strong>Accessibility:</strong> It simplifies the technically challenging process of setting up LLMs, making advanced language processing accessible to a broader audience, including developers, researchers, and hobbyists, without deep knowledge of machine learning frameworks or complex hardware configurations.</li></ol><h2 id=getting-started-with-ollama>Getting Started with Ollama<a class=td-heading-self-link href=#getting-started-with-ollama aria-label="Heading self-link"></a></h2><p><strong>Installation:</strong></p><p>Setting up Ollama is straightforward:</p><ol><li>Navigate to the official Ollama website (<code>ollama.com</code>).</li><li>Click the &ldquo;Download&rdquo; button.</li><li>Select your operating system (macOS, Windows, or Linux).<ul><li><strong>macOS/Windows:</strong> Download the installer application and run it. Follow the on-screen prompts. The application will set up the necessary command-line tools and potentially start a background service.</li><li><strong>Linux:</strong> Copy the provided <code>curl</code> command and execute it in your terminal to install Ollama.</li></ul></li><li><strong>Verification:</strong> Once installed, open your terminal or command prompt and type <code>ollama</code>. If the installation was successful, you should see a list of available commands and options.</li></ol><p>The Ollama application often runs as a background service, managing the models and handling requests. On macOS and Windows, you might see an Ollama icon in your system tray or menu bar.</p><h2 id=core-concepts-and-usage>Core Concepts and Usage<a class=td-heading-self-link href=#core-concepts-and-usage aria-label="Heading self-link"></a></h2><p><strong>1. Running Models:</strong></p><p>The primary command to interact with models is <code>ollama run</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ollama run &lt;model_name&gt;
</span></span></code></pre></div><p>Replace <code>&lt;model_name></code> with the identifier of the model you wish to use (e.g., <code>llama3.1</code>, <code>mistral</code>, <code>codegemma</code>, <code>llava</code>).</p><ul><li>If the specified model is not already present on your system, Ollama will automatically download it first. Model sizes can vary significantly (from a few gigabytes to hundreds), so ensure you have sufficient disk space and a stable internet connection for the download.</li><li>Once the model is ready (either downloaded or already local), Ollama will launch an interactive chat prompt in your terminal, allowing you to start conversing with the LLM immediately.</li></ul><p>Example:
<code>ollama run mistral</code></p><p>You can exit the interactive chat prompt by typing <code>/bye</code>.</p><p><strong>2. Model Management:</strong></p><ul><li><p><strong>Listing Installed Models:</strong> To see which models you have downloaded locally, use:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ollama list
</span></span></code></pre></div><p>This command displays the model name, ID, size, and modification date.</p></li><li><p><strong>Removing Models:</strong> If you need to free up disk space or no longer need a specific model, use:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ollama rm &lt;model_name&gt;
</span></span></code></pre></div><p>This will delete the specified model and its associated data from your system.</p></li><li><p><strong>Pulling Models:</strong> You can download models without immediately running them using:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ollama pull &lt;model_name&gt;
</span></span></code></pre></div></li></ul><p><strong>3. Understanding Models:</strong></p><p>Ollama provides access to a wide variety of open-source models. When choosing a model, consider these factors:</p><ul><li><strong>Parameters:</strong> Often denoted with &lsquo;B&rsquo; (billions), like 7B, 13B, 70B, or even 405B. This reflects the model&rsquo;s complexity and capacity. More parameters generally mean better performance but require more computational resources (RAM and processing power).</li><li><strong>Size:</strong> The disk space required to store the model. This is directly related to the number of parameters and quantization.</li><li><strong>RAM Requirements:</strong> Running a model requires loading it into your computer&rsquo;s RAM. Ollama&rsquo;s documentation often provides guidance on how much RAM is needed based on the model&rsquo;s parameter count (e.g., a 7B model might need 8GB+ RAM, while a 70B model could require 64GB+ RAM).</li><li><strong>Quantization:</strong> A technique used to reduce the model&rsquo;s size and computational requirements by reducing the precision of its weights (e.g., 4-bit quantization). This makes larger models feasible to run on consumer hardware, sometimes with a slight trade-off in performance.</li><li><strong>Model Types:</strong> Ollama supports various model types tailored for different tasks:<ul><li><strong>Language Models:</strong> For text generation, conversation, instruction following, summarization (e.g., Llama series, Mistral, Gemma).</li><li><strong>Multimodal Models:</strong> Capable of processing multiple types of input, such as text and images (e.g., Llava). You can provide an image file path along with your text prompt.</li><li><strong>Embedding Models:</strong> Used to convert text into numerical vector representations, essential for Retrieval-Augmented Generation (RAG) systems and semantic search (e.g., <code>nomic-embed-text</code>, <code>mxbai-embed-large</code>).</li><li><strong>Tool Calling Models:</strong> Fine-tuned models designed to interact with external tools, functions, or APIs in an agentic manner.</li></ul></li></ul><p><strong>4. Finding Models:</strong></p><p>The Ollama website features a model library (<code>ollama.com/library</code>) where you can browse, search, and filter available models. Each model page provides details about its size, parameters, use cases, and how to run it. Common popular choices include models from the Llama series, Mistral, CodeGemma (for coding tasks), and Llava (for multimodal tasks).</p><h2 id=advanced-usage>Advanced Usage<a class=td-heading-self-link href=#advanced-usage aria-label="Heading self-link"></a></h2><h3 id=1-customizing-models-with><strong>1. Customizing Models with <code>Modelfile</code>:</strong><a class=td-heading-self-link href=#1-customizing-models-with aria-label="Heading self-link"></a></h3><p>Similar to how Docker uses a <code>Dockerfile</code> to define container images, Ollama uses a <code>Modelfile</code> to create customized model variations. This plain text file allows you to:</p><ul><li>Start from a base model (<code>FROM &lt;base_model_name></code>).</li><li>Set parameters like <code>temperature</code> (controls creativity vs. factuality), <code>top_k</code>, <code>top_p</code>, etc.</li><li>Define a <code>SYSTEM</code> prompt to give the model specific instructions, persona, or context for its responses.</li><li>Include adapter weights (e.g., for LoRA fine-tuning).</li></ul><p><strong>Example <code>Modelfile</code>:</strong></p><pre tabindex=0><code>FROM llama3.1:8b
PARAMETER temperature 0.7
PARAMETER top_k 50
SYSTEM &#34;&#34;&#34;
You are a helpful assistant specializing in explaining complex scientific concepts in simple terms.
Always be concise and clear.
&#34;&#34;&#34;
</code></pre><p>To create a new custom model from this file:</p><ol><li>Save the content above into a file named <code>Modelfile</code> (no extension).</li><li>Run the command in your terminal, in the same directory as the file:<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ollama create &lt;your_custom_model_name&gt; -f Modelfile
</span></span></code></pre></div></li><li>Run your custom model:<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ollama run &lt;your_custom_model_name&gt;
</span></span></code></pre></div></li></ol><h3 id=2-the-ollama-server-and-rest-api><strong>2. The Ollama Server and REST API:</strong><a class=td-heading-self-link href=#2-the-ollama-server-and-rest-api aria-label="Heading self-link"></a></h3><p>Under the hood, Ollama runs a local HTTP server, typically on <code>http://localhost:11434</code>. This server exposes a REST API that handles requests to the LLMs. This is fundamental because it allows <em>any</em> application capable of making HTTP requests to interact with your local models.</p><ul><li><strong>Automatic Start:</strong> Usually, the server starts automatically when the Ollama desktop application is running or when you use commands like <code>ollama run</code>.</li><li><strong>Manual Start:</strong> You can manually start the server and view logs using:<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ollama serve
</span></span></code></pre></div>This will show incoming requests and processing details in your terminal.</li><li><strong>API Endpoints:</strong> The API provides various endpoints:<ul><li><code>/api/generate</code>: For straightforward text generation based on a prompt.</li><li><code>/api/chat</code>: For conversational interactions, maintaining context through a list of messages.</li><li>Other endpoints exist for managing models (listing, pulling, deleting), showing model info, and creating embeddings.</li></ul></li></ul><p>You can interact with this API using tools like <code>curl</code>, Postman, or directly from your code. Common parameters in API requests include <code>model</code>, <code>prompt</code> (for generate), <code>messages</code> (for chat), <code>stream</code> (true/false - whether to stream response tokens or wait for the full response), and <code>format</code> (<code>json</code> - to request JSON output).</p><h3 id=3-interacting-via-code-python-example><strong>3. Interacting via Code (Python Example):</strong><a class=td-heading-self-link href=#3-interacting-via-code-python-example aria-label="Heading self-link"></a></h3><p>The Ollama API makes it easy to integrate local LLMs into your applications. Here&rsquo;s how you might do it in Python:</p><ul><li><p><strong>Manual HTTP Requests:</strong> Using libraries like <code>requests</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>json</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>url</span> <span class=o>=</span> <span class=s2>&#34;http://localhost:11434/api/chat&#34;</span>
</span></span><span class=line><span class=cl><span class=n>payload</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;model&#34;</span><span class=p>:</span> <span class=s2>&#34;mistral&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;messages&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;Why is the sky blue?&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;stream&#34;</span><span class=p>:</span> <span class=kc>False</span> <span class=c1># Get the full response at once</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>post</span><span class=p>(</span><span class=n>url</span><span class=p>,</span> <span class=n>json</span><span class=o>=</span><span class=n>payload</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>response</span><span class=o>.</span><span class=n>raise_for_status</span><span class=p>()</span> <span class=c1># Raise an exception for bad status codes</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>json</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=s1>&#39;message&#39;</span><span class=p>][</span><span class=s1>&#39;content&#39;</span><span class=p>])</span>
</span></span></code></pre></div></li><li><p><strong>Using the Official <code>ollama</code> Python Package:</strong> Ollama provides convenient libraries for popular languages. For Python:</p><ol><li>Install the package: <code>pip install ollama</code></li><li>Use the client:<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>ollama</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>ollama</span><span class=o>.</span><span class=n>Client</span><span class=p>()</span> <span class=c1># Connects to http://localhost:11434 by default</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=s1>&#39;mistral&#39;</span><span class=p>,</span> <span class=n>messages</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;role&#39;</span><span class=p>:</span> <span class=s1>&#39;user&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;content&#39;</span><span class=p>:</span> <span class=s1>&#39;Why is the sky blue?&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=p>[</span><span class=s1>&#39;message&#39;</span><span class=p>][</span><span class=s1>&#39;content&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># For streaming responses:</span>
</span></span><span class=line><span class=cl><span class=c1># stream = client.chat(</span>
</span></span><span class=line><span class=cl><span class=c1>#     model=&#39;mistral&#39;,</span>
</span></span><span class=line><span class=cl><span class=c1>#     messages=[{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;Tell me a short story&#39;}],</span>
</span></span><span class=line><span class=cl><span class=c1>#     stream=True,</span>
</span></span><span class=line><span class=cl><span class=c1># )</span>
</span></span><span class=line><span class=cl><span class=c1># for chunk in stream:</span>
</span></span><span class=line><span class=cl><span class=c1>#   print(chunk[&#39;message&#39;][&#39;content&#39;], end=&#39;&#39;, flush=True)</span>
</span></span></code></pre></div></li></ol><p>The library handles the complexities of API calls, making integration much cleaner. Similar libraries exist for JavaScript/TypeScript.</p></li></ul><h3 id=4-using-graphical-user-interfaces-guis><strong>4. Using Graphical User Interfaces (GUIs):</strong><a class=td-heading-self-link href=#4-using-graphical-user-interfaces-guis aria-label="Heading self-link"></a></h3><p>Because Ollama exposes a standard API, various community-developed GUI applications can act as frontends. Tools like &ldquo;Open Web UI&rdquo; or &ldquo;Mist&rdquo; provide chat interfaces similar to commercial offerings but connect to your local Ollama models. Some even offer features for managing models, adjusting parameters, and setting up simple RAG pipelines by uploading documents directly through the UI.</p><h2 id=common-use-cases>Common Use Cases<a class=td-heading-self-link href=#common-use-cases aria-label="Heading self-link"></a></h2><p>Ollama empowers a variety of applications:</p><ul><li><strong>Development and Testing:</strong> Easily experiment with different LLMs for application features without incurring API costs or dealing with complex setups.</li><li><strong>Education and Research:</strong> Provides an accessible platform for learning about and experimenting with LLMs without the cost barriers of cloud services.</li><li><strong>Secure Applications:</strong> Build AI-powered features for applications handling sensitive data, ensuring data stays within a controlled environment.</li><li><strong>Offline AI Tools:</strong> Create tools that leverage LLMs even without internet access.</li><li><strong>Personalized Assistants:</strong> Customize models with specific instructions or knowledge using <code>Modelfile</code>.</li><li><strong>Building Local AI Applications:</strong> Create tools for tasks like:<ul><li>Text summarization</li><li>Sentiment analysis</li><li>Code generation and explanation</li><li>Retrieval-Augmented Generation (RAG) systems using local embedding models and vector stores.</li></ul></li></ul><h2 id=tree-view---everything-about-ollama>Tree View - Everything about Ollama<a class=td-heading-self-link href=#tree-view---everything-about-ollama aria-label="Heading self-link"></a></h2><div class=tree-controls><button id=expand-all>Expand All</button>
<button id=collapse-all>Collapse All</button></div><div class=tree-view>{% assign tree_data = site.data.ollama-tree %}
{% include tree-view.html nodes=tree_data depth=0 %}</div><link rel=stylesheet href=https://localhost:1313/assets/css/tree.css><script src=https://localhost:1313/assets/js/tree.js></script><h2 id=conclusion>Conclusion<a class=td-heading-self-link href=#conclusion aria-label="Heading self-link"></a></h2><p>Ollama significantly lowers the barrier to entry for working with powerful Large Language Models. By enabling local execution, it addresses key concerns around cost, privacy, and complexity. Its simple CLI, standardized API, support for model customization, and compatibility with a growing ecosystem of open-source models make it an invaluable tool for developers, researchers, and AI enthusiasts looking to harness the power of LLMs on their own terms and hardware. Whether you&rsquo;re building sophisticated AI applications or simply exploring the capabilities of modern AI, Ollama provides a robust, free, and private foundation.</p><div class=category-section><h4 class=category-section__title>Categories:</h4><div class=category-badges><a href=https://localhost:1313/categories/dsblog class=category-badge>dsblog</a><a href=https://localhost:1313/categories/ai-and-nlp class=category-badge>ai-and-nlp</a></div></div><div class=td-tags><h4 class=td-tags__title>Tags:</h4><div class=category-badges><a href=https://localhost:1313/tags/ollama class=category-badge>Ollama</a><a href=https://localhost:1313/tags/large-language-models class=category-badge>Large Language Models</a><a href=https://localhost:1313/tags/ai-and-nlp class=category-badge>AI and NLP</a><a href=https://localhost:1313/tags/local-models class=category-badge>Local Models</a><a href=https://localhost:1313/tags/cost-savings class=category-badge>Cost Savings</a><a href=https://localhost:1313/tags/privacy class=category-badge>Privacy</a><a href=https://localhost:1313/tags/offline class=category-badge>Offline</a></div></div><div class=td-author-box><div class=td-author-box__info><h4 class=td-author-box__name>Blowfish</h4><p class=td-author-box__bio>A powerful, lightweight theme for Hugo.</p><div class=td-author-box__links><a href target=_blank rel=noopener aria-label></a><a href target=_blank rel=noopener aria-label></a></div></div></div><div class=td-social-share><h4 class=td-social-share__title>Share this article:</h4><ul class=td-social-share__list><div class=social-share><a href="https://twitter.com/intent/tweet?text=Ollama%20Setup%20and%20Running%20Models&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fOllama-Setup-and-Running-Models%2f" target=_blank rel=noopener aria-label="Share on Twitter"><i class="fab fa-twitter"></i>
</a><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fOllama-Setup-and-Running-Models%2f" target=_blank rel=noopener aria-label="Share on Facebook"><i class="fab fa-facebook"></i>
</a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fOllama-Setup-and-Running-Models%2f&title=Ollama%20Setup%20and%20Running%20Models" target=_blank rel=noopener aria-label="Share on LinkedIn"><i class="fab fa-linkedin"></i>
</a><a href="https://www.reddit.com/submit?url=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fOllama-Setup-and-Running-Models%2f&title=Ollama%20Setup%20and%20Running%20Models" target=_blank rel=noopener aria-label="Share on Reddit"><i class="fab fa-reddit"></i>
</a><a href="mailto:?subject=Ollama%20Setup%20and%20Running%20Models&body=https%3a%2f%2flocalhost%3a1313%2fdsblog%2fOllama-Setup-and-Running-Models%2f" aria-label="Share via Email"><i class="fas fa-envelope"></i></a></div></ul></div><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href=https://localhost:1313/dsblog/ps-Retrieval-Augmented-Generation-with-Conflicting-Evidence/ aria-label="Previous page"><div class=td-pager__meta><i class="fa-solid fa-angle-left"></i>
<span class=td-pager__meta-label><b>Previous:</b></span>
<span class=td-pager__meta-title>Retrieval-Augmented Generation with Conflicting Evidence</span></div></a><a class="td-pager__link td-pager__link--next" href=https://localhost:1313/dsblog/BitNet-b1-58-2B4T-for-efficient-ai-processing/ aria-label="Next page"><div class=td-pager__meta><span class=td-pager__meta-label><b>Next:</b></span>
<span class=td-pager__meta-title>BitNet b1.58-2B4T: Revolutionary Binary Neural Network for Efficient AI</span>
<i class="fa-solid fa-angle-right"></i></div></a></ul></div><div class=col-md-2></div></div></main><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class=col-2><a href=https://dasarpai.com target=_blank rel=noopener><img src=https://localhost:1313/assets/images/site-logo.png alt=dasarpAI width=100 style=border-radius:12px></a></div><div class=col-8><div class=row></div><div class=row></div></div><div class=col-2></div><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"></div></div></div></footer></div><script src=https://localhost:1313/js/main.min.7c31f98e62c36b0c9c834ce3f8260a0e21895dd6aa0773e81a64b104eae3b2e8.js integrity="sha256-fDH5jmLDawycg0zj+CYKDiGJXdaqB3PoGmSxBOrjsug=" crossorigin=anonymous></script><script defer src=https://localhost:1313/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=https://localhost:1313/js/tabpane-persist.js></script><script src=https://localhost:1313/js/asciinema-player.js></script><script>(function(){var e=document.querySelector("#td-section-nav");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></body></html>