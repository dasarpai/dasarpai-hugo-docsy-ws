<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Transformers on Blowfish</title><link>https://localhost:1313/tags/transformers/</link><description>Recent content in Transformers on Blowfish</description><generator>Hugo</generator><language>en</language><managingEditor>nuno@n9o.xyz (Blowfish)</managingEditor><webMaster>nuno@n9o.xyz (Blowfish)</webMaster><lastBuildDate>Fri, 28 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://localhost:1313/tags/transformers/index.xml" rel="self" type="application/rss+xml"/><item><title>AI News - February 2025</title><link>https://localhost:1313/news/AI-News-Feb-2025/</link><pubDate>Fri, 28 Feb 2025 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/news/AI-News-Feb-2025/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/news/8005-AI-News-Feb-2025.jpg" alt="">&lt;/p>
&lt;h1 id="ai-news---february-2025">AI News - February 2025&lt;a class="td-heading-self-link" href="#ai-news---february-2025" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="1-elon-musk-announces-grok-3">1. &lt;strong>Elon Musk Announces Grok 3&lt;/strong>&lt;a class="td-heading-self-link" href="#1-elon-musk-announces-grok-3" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Elon Musk revealed plans to release Grok 3, a new AI chatbot from his startup xAI, which he claims outperforms existing AI chatbots like OpenAI&amp;rsquo;s ChatGPT[1].&lt;/p></description></item><item><title>Understanding Contextual Embedding in Transformers</title><link>https://localhost:1313/dsblog/understanding-contextual-embedding-in-transformers/</link><pubDate>Thu, 30 Jan 2025 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/understanding-contextual-embedding-in-transformers/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6214-Understanding-Contextual-Embedding-in-Transformer.jpg" alt="Understanding Contextual Embedding in Transformers">&lt;/p>
&lt;h2 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Embedding can be confusing for many people, and contextual embedding performed by transformers can be even more perplexing. Even after gaining an understanding, many questions remain. In this article, we aim to address the following questions.&lt;/p></description></item><item><title>AI Imperialism: Western Dominance and the Future of Global Technology</title><link>https://localhost:1313/dsblog/AI-Imperialism/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/AI-Imperialism/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6191-AI-Imperialism.jpg" alt="AI Imperialism">&lt;/p>
&lt;h1 id="ai-imperialism-western-dominance-and-the-future-of-global-technology">AI Imperialism: Western Dominance and the Future of Global Technology&lt;a class="td-heading-self-link" href="#ai-imperialism-western-dominance-and-the-future-of-global-technology" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>In the rapidly evolving landscape of artificial intelligence (AI), the emergence of transformer models has marked a significant milestone. Among these, OpenAI&amp;rsquo;s GPT-3 stands out as a groundbreaking achievement, yet its dominance raises critical questions about the concentration of power, legal ambiguities, and global technological equity. This article delves into the phenomenon of AI imperialism, exploring how Western dominance shapes the future of global technology and the implications for developing nations.&lt;/p></description></item><item><title>Visualizing Transformers and Attention</title><link>https://localhost:1313/dsblog/Visualizing-transformers-and-attention/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/Visualizing-transformers-and-attention/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6189-Visualizing-transformers-and-attention.jpg" alt="Visualizing transformers and attention">&lt;/p>
&lt;h1 id="visualizing-transformers-and-attention">Visualizing Transformers and Attention&lt;a class="td-heading-self-link" href="#visualizing-transformers-and-attention" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>This is the summary note from Grant Sanderson&amp;rsquo;s talk at TNG Big Tech 2024. My earlir article on transformers can be found &lt;a href="https://localhost:1313/dsblog/transformers-demystified-a-step-by-step-guide">here&lt;/a>&lt;/p></description></item><item><title>Understanding LLM GAN and Transformers</title><link>https://localhost:1313/dsblog/Understanding-LLM-GAN-and-Transformers/</link><pubDate>Fri, 26 Jul 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/Understanding-LLM-GAN-and-Transformers/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6127-Understanding-LLM-GAN-and-Transformers.jpg" alt="Understanding-LLM-GAN-Transformers">&lt;/p>
&lt;h1 id="understanding-llm-gan-and-transformers">Understanding LLM, GAN and Transformers&lt;a class="td-heading-self-link" href="#understanding-llm-gan-and-transformers" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="llm-layers">LLM Layers&lt;a class="td-heading-self-link" href="#llm-layers" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Large Language Models (LLMs) are typically based on Transformer architectures, which consist of several types of layers that work together to process and generate text. Here are the primary kinds of layers found in an LLM:&lt;/p></description></item><item><title>Transformers Demystified A Step-by-Step Guide</title><link>https://localhost:1313/dsblog/transformers-demystified-a-step-by-step-guide/</link><pubDate>Thu, 25 Jul 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/transformers-demystified-a-step-by-step-guide/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6113-transformers-demystified-a-step-by-step-guide.jpg" alt="Transformers Demystified A Step-by-Step Guide">&lt;/p>
&lt;h1 id="transformers-demystified-a-step-by-step-guide">Transformers Demystified A Step-by-Step Guide&lt;a class="td-heading-self-link" href="#transformers-demystified-a-step-by-step-guide" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>All modern Transformers are based on a paper &amp;ldquo;Attention is all you need&amp;rdquo;&lt;/p>
&lt;h2 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>This was the mother paper of all the transformer architectures we see today around NLP, Multimodal, Deep Learning. It was presented by Ashish Vaswani et al from Deep Learning / Google in 2017. We will discuss following and anything whatever question/observation/idea I have.&lt;/p></description></item></channel></rss>