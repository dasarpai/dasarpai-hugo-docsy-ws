<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Large Language Models on </title>
    <link>https://localhost:1313/tags/large-language-models/</link>
    <description>Recent content in Large Language Models on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>hari@dasarpai.com (Hari Thapliyaal)</managingEditor>
    <webMaster>hari@dasarpai.com (Hari Thapliyaal)</webMaster>
    <copyright>© 2025 Hari Thapliyaal</copyright>
    <lastBuildDate>Thu, 17 Apr 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://localhost:1313/tags/large-language-models/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Retrieval-Augmented Generation with Conflicting Evidence</title>
      <link>https://localhost:1313/dsblog/ps-Retrieval-Augmented-Generation-with-Conflicting-Evidence/</link>
      <pubDate>Thu, 17 Apr 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>https://localhost:1313/dsblog/ps-Retrieval-Augmented-Generation-with-Conflicting-Evidence/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;https://localhost:1313/assets/images/dspost/dsp6261-Retrieval-Augmented-Generation-with-Conflicting-Evidence.jpg&#34; alt=&#34;&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Paper Summary: Retrieval-Augmented Generation with Conflicting Evidence 
    &lt;div id=&#34;paper-summary-retrieval-augmented-generation-with-conflicting-evidence&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#paper-summary-retrieval-augmented-generation-with-conflicting-evidence&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2504.13079&#34; target=&#34;_blank&#34;&gt;arXiv Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The hypothesis of this paper is that &lt;strong&gt;real-world retrieval-augmented generation (RAG) systems must simultaneously handle various sources of conflicting information, including ambiguity in user queries and contradictory information arising from misinformation and noise in retrieved documents&lt;/strong&gt;. The authors argue that prior work has largely addressed these challenges in isolation.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>LLM Internal Encoding of Truthfulness and Hallucinations</title>
      <link>https://localhost:1313/dsblog/ps-LLM-Internal-Encoding-of-Truthfulness-and-Hallucinations/</link>
      <pubDate>Tue, 15 Apr 2025 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>https://localhost:1313/dsblog/ps-LLM-Internal-Encoding-of-Truthfulness-and-Hallucinations/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;https://localhost:1313/assets/images/dspost/dsp6260-LLM-Internal-Encoding-of-Truthfulness-and-Hallucinations.jpg&#34; alt=&#34;LLM Internal Encoding of Truthfulness and Hallucinations&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Paper Summary: LLM Internal Encoding of Truthfulness and Hallucinations 
    &lt;div id=&#34;paper-summary-llm-internal-encoding-of-truthfulness-and-hallucinations&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#paper-summary-llm-internal-encoding-of-truthfulness-and-hallucinations&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;The objective of this paper is to gain a deeper understanding of errors produced by large language models (LLMs) by examining their internal representations. The authors aim to reveal how information about the truthfulness of LLM outputs is encoded internally, going beyond extrinsic, behavioral analysis. They also seek to investigate the relationship between these internal representations and the external behavior of LLMs, including their tendency to produce inaccuracies or &amp;ldquo;hallucinations&amp;rdquo;. Furthermore, the paper intends to explore whether internal representations can be used to predict the types of errors LLMs make and to detect the correct answer even when the model generates an incorrect one.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Panel Discussion: Global India AI Summit 2024: Large Language Models by IndiaAI</title>
      <link>https://localhost:1313/booksummary/panel-discussion-global-indiaai-summit-2024-llm-by-indiaai/</link>
      <pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>https://localhost:1313/booksummary/panel-discussion-global-indiaai-summit-2024-llm-by-indiaai/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;https://localhost:1313/assets/images/booksummary/7534-Global-IndiaAI-Summit-2024-LLM-by-IndiaAI.jpg&#34; alt=&#34;Global India AI Summit 2024: Panel Discussion on Large Language Models by IndiaAI&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Summary of the Panel Discussion: India AI Large Language Models 
    &lt;div id=&#34;summary-of-the-panel-discussion-india-ai-large-language-models&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#summary-of-the-panel-discussion-india-ai-large-language-models&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;p&gt;The session was about&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Types of Large Language Models (LLM)</title>
      <link>https://localhost:1313/dsblog/Types-of-LLM/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>https://localhost:1313/dsblog/Types-of-LLM/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;https://localhost:1313/assets/images/dspost/dsp6171-Types-of-LLM.jpg&#34; alt=&#34;&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;&lt;strong&gt;Introduction:&lt;/strong&gt; 
    &lt;div id=&#34;introduction&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#introduction&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;The world of Generative AI (GenAI) is expanding at an astonishing rate, with new models emerging almost daily, each sporting unique names, capabilities, versions, and sizes. For AI professionals, keeping track of these models can feel like a full-time job. But for business users, IT professionals, and software developers trying to make the right choice, understanding the model’s name and what it represents can seem overwhelming. Wouldn’t it be helpful if we could decode the meaning behind these names to know if a model fits our needs and is worth the investment? In this article, we’ll break down how the names of GenAI models can reveal clues about their functionality and suitability for specific tasks, helping you make informed decisions with confidence.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Understanding LLM GAN and Transformers</title>
      <link>https://localhost:1313/dsblog/Understanding-LLM-GAN-and-Transformers/</link>
      <pubDate>Fri, 26 Jul 2024 00:00:00 +0000</pubDate>
      <author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>https://localhost:1313/dsblog/Understanding-LLM-GAN-and-Transformers/</guid>
      <description>&lt;p&gt;
    &lt;figure&gt;
      &lt;img class=&#34;my-0 rounded-md&#34; loading=&#34;lazy&#34; src=&#34;https://localhost:1313/assets/images/dspost/dsp6127-Understanding-LLM-GAN-and-Transformers.jpg&#34; alt=&#34;Understanding-LLM-GAN-Transformers&#34; /&gt;
      
    &lt;/figure&gt;
&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;Understanding LLM, GAN and Transformers 
    &lt;div id=&#34;understanding-llm-gan-and-transformers&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#understanding-llm-gan-and-transformers&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;


&lt;h2 class=&#34;relative group&#34;&gt;LLM Layers 
    &lt;div id=&#34;llm-layers&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#llm-layers&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;p&gt;Large Language Models (LLMs) are typically based on Transformer architectures, which consist of several types of layers that work together to process and generate text. Here are the primary kinds of layers found in an LLM:&lt;/p&gt;</description>
      
    </item>
    
  </channel>
</rss>
