<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI Architecture on Blowfish</title><link>https://dasarpai.github.io/tags/ai-architecture/</link><description>Recent content in AI Architecture on Blowfish</description><generator>Hugo</generator><language>en</language><managingEditor>nuno@n9o.xyz (Blowfish)</managingEditor><webMaster>nuno@n9o.xyz (Blowfish)</webMaster><lastBuildDate>Fri, 26 Jul 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://dasarpai.github.io/tags/ai-architecture/index.xml" rel="self" type="application/rss+xml"/><item><title>Understanding LLM GAN and Transformers</title><link>https://dasarpai.github.io/dsblog/Understanding-LLM-GAN-and-Transformers/</link><pubDate>Fri, 26 Jul 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/Understanding-LLM-GAN-and-Transformers/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6127-Understanding-LLM-GAN-and-Transformers.jpg" alt="Understanding-LLM-GAN-Transformers">&lt;/p>
&lt;h1 id="understanding-llm-gan-and-transformers">Understanding LLM, GAN and Transformers&lt;a class="td-heading-self-link" href="#understanding-llm-gan-and-transformers" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="llm-layers">LLM Layers&lt;a class="td-heading-self-link" href="#llm-layers" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Large Language Models (LLMs) are typically based on Transformer architectures, which consist of several types of layers that work together to process and generate text. Here are the primary kinds of layers found in an LLM:&lt;/p></description></item><item><title>Transformers Demystified A Step-by-Step Guide</title><link>https://dasarpai.github.io/dsblog/transformers-demystified-a-step-by-step-guide/</link><pubDate>Thu, 25 Jul 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/transformers-demystified-a-step-by-step-guide/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6113-transformers-demystified-a-step-by-step-guide.jpg" alt="Transformers Demystified A Step-by-Step Guide">&lt;/p>
&lt;h1 id="transformers-demystified-a-step-by-step-guide">Transformers Demystified A Step-by-Step Guide&lt;a class="td-heading-self-link" href="#transformers-demystified-a-step-by-step-guide" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>All modern Transformers are based on a paper &amp;ldquo;Attention is all you need&amp;rdquo;&lt;/p>
&lt;h2 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>This was the mother paper of all the transformer architectures we see today around NLP, Multimodal, Deep Learning. It was presented by Ashish Vaswani et al from Deep Learning / Google in 2017. We will discuss following and anything whatever question/observation/idea I have.&lt;/p></description></item></channel></rss>