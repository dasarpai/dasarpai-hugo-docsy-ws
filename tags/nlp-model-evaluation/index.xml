<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NLP Model Evaluation on Blowfish</title><link>https://localhost:1313/tags/nlp-model-evaluation/</link><description>Recent content in NLP Model Evaluation on Blowfish</description><generator>Hugo</generator><language>en</language><managingEditor>nuno@n9o.xyz (Blowfish)</managingEditor><webMaster>nuno@n9o.xyz (Blowfish)</webMaster><lastBuildDate>Fri, 14 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://localhost:1313/tags/nlp-model-evaluation/index.xml" rel="self" type="application/rss+xml"/><item><title>A Comprehensive Guide to Evaluate Generative Models</title><link>https://localhost:1313/dsblog/guide-to-evaluate-generative-models/</link><pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/guide-to-evaluate-generative-models/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6240-Guide-to-Evaluate-Generative-Models.jpg" alt="Generative Models">&lt;/p>
&lt;h1 id="a-comprehensive-guide-to-evaluate-generative-models">A Comprehensive Guide to Evaluate Generative Models&lt;a class="td-heading-self-link" href="#a-comprehensive-guide-to-evaluate-generative-models" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>In the rapidly evolving landscape of artificial intelligence, generative models have emerged as powerful tools capable of creating lifelike images, coherent text narratives, and even realistic audio. From large language models (LLMs) like GPT, Gemini, and Claude to image generators like Stable Diffusion, these systems are reshaping creativity, communication, and problem-solving. However, their potential is only as good as our ability to evaluate them effectively.&lt;/p></description></item><item><title>Machine Learning Tasks and Model Evaluation</title><link>https://localhost:1313/dsblog/ml-tasks-and-model-evaluation/</link><pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/ml-tasks-and-model-evaluation/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dsresources/dsr114-ml-tasks-and-model-evaluation.jpg" alt="Deep Learning Tasks and Models">&lt;/p>
&lt;h1 id="machine-learning-tasks-and-model-evaluation">Machine Learning Tasks and Model Evaluation&lt;a class="td-heading-self-link" href="#machine-learning-tasks-and-model-evaluation" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Machine learning is a subject where we study how to create &amp;amp; evaluate machine learning models. To create these models, we need different types of data. We build models which can help us do various kinds of tasks. There are hundreds of model building techniques and researchers keep adding new techniques, and architectures as when need arises. But, the question is how do you evaluate these models which are output of the model trainings? To evaluate the performance of a model on structured data, or classification/regression/clustering models, we require one kind of metrics. But this becomes complicated when we are dealing with voice, text and audio data. How do you evaluate ten models which are responsible for translation, or locating an object in the image, transcribing voice into text, captioning an image? To solve this problem, standard databases are created and everyone needs to demonstrate the performance of their model, architecture, or approach against that dataset. But, even if you have a baseline dataset, how will you evaluate various NLP or deep learning tasks? For that GLUE, SuperGLUE benchmarks are created.&lt;/p></description></item></channel></rss>