<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning Tasks on Blowfish</title><link>https://dasarpai.github.io/tags/machine-learning-tasks/</link><description>Recent content in Machine Learning Tasks on Blowfish</description><generator>Hugo</generator><language>en</language><managingEditor>nuno@n9o.xyz (Blowfish)</managingEditor><webMaster>nuno@n9o.xyz (Blowfish)</webMaster><lastBuildDate>Wed, 14 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://dasarpai.github.io/tags/machine-learning-tasks/index.xml" rel="self" type="application/rss+xml"/><item><title>Machine Learning Tasks and Model Evaluation</title><link>https://dasarpai.github.io/dsblog/ml-tasks-and-model-evaluation/</link><pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/ml-tasks-and-model-evaluation/</guid><description>&lt;p>&lt;img src="../../assets/images/dsresources/dsr114-ml-tasks-and-model-evaluation.jpg" alt="Deep Learning Tasks and Models">&lt;/p>
&lt;h1 id="machine-learning-tasks-and-model-evaluation">Machine Learning Tasks and Model Evaluation&lt;a class="td-heading-self-link" href="#machine-learning-tasks-and-model-evaluation" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Machine learning is a subject where we study how to create &amp;amp; evaluate machine learning models. To create these models, we need different types of data. We build models which can help us do various kinds of tasks. There are hundreds of model building techniques and researchers keep adding new techniques, and architectures as when need arises. But, the question is how do you evaluate these models which are output of the model trainings? To evaluate the performance of a model on structured data, or classification/regression/clustering models, we require one kind of metrics. But this becomes complicated when we are dealing with voice, text and audio data. How do you evaluate ten models which are responsible for translation, or locating an object in the image, transcribing voice into text, captioning an image? To solve this problem, standard databases are created and everyone needs to demonstrate the performance of their model, architecture, or approach against that dataset. But, even if you have a baseline dataset, how will you evaluate various NLP or deep learning tasks? For that GLUE, SuperGLUE benchmarks are created.&lt;/p></description></item><item><title>Machine Learning Tasks and Model Evaluation</title><link>https://dasarpai.github.io/dsresources/ml-tasks-and-model-evaluation/</link><pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsresources/ml-tasks-and-model-evaluation/</guid><description>&lt;p>&lt;img src="../../assets/images/dsresources/dsr114-ml-tasks-and-model-evaluation.jpg" alt="Deep Learning Tasks and Models">&lt;/p>
&lt;h1 id="machine-learning-tasks-and-model-evaluation">Machine Learning Tasks and Model Evaluation&lt;a class="td-heading-self-link" href="#machine-learning-tasks-and-model-evaluation" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>I am sorry, this page has moved to different location. &lt;a href="../../dsblog/ml-tasks-and-model-evaluation">Click me to go there&lt;/a>&lt;/p></description></item></channel></rss>