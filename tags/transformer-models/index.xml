<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Transformer Models on Blowfish</title><link>https://dasarpai.github.io/tags/transformer-models/</link><description>Recent content in Transformer Models on Blowfish</description><generator>Hugo</generator><language>en</language><managingEditor>nuno@n9o.xyz (Blowfish)</managingEditor><webMaster>nuno@n9o.xyz (Blowfish)</webMaster><lastBuildDate>Sun, 04 Aug 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://dasarpai.github.io/tags/transformer-models/index.xml" rel="self" type="application/rss+xml"/><item><title>LLM Architecture and Training</title><link>https://dasarpai.github.io/dsblog/LLM-Architecture-and-Training/</link><pubDate>Sun, 04 Aug 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/LLM-Architecture-and-Training/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6129-LLM-Architecture-and-Training.jpg" alt="LLM-Architecture-and-Training">&lt;/p>
&lt;h1 id="understanding-llm-architectures-and-model-training">&lt;strong>Understanding LLM Architectures and Model Training&lt;/strong>&lt;a class="td-heading-self-link" href="#understanding-llm-architectures-and-model-training" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>Large Language Models (LLMs) are transforming the field of artificial intelligence by enabling machines to understand and generate human language with unprecedented accuracy. This article delves into the architecture, training methods, and practical applications of LLMs. Weâ€™ll explore the core components that make these models so powerful and explain how they are trained and fine-tuned for real-world use cases.&lt;/p></description></item></channel></rss>