<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Model Architecture on Blowfish</title><link>https://localhost:1313/tags/model-architecture/</link><description>Recent content in Model Architecture on Blowfish</description><generator>Hugo</generator><language>en</language><managingEditor>nuno@n9o.xyz (Blowfish)</managingEditor><webMaster>nuno@n9o.xyz (Blowfish)</webMaster><lastBuildDate>Thu, 22 Aug 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://localhost:1313/tags/model-architecture/index.xml" rel="self" type="application/rss+xml"/><item><title>Variations of Language Model in Huggingface</title><link>https://localhost:1313/dsblog/Variations-of-Language-Model-in-Huggingface/</link><pubDate>Thu, 22 Aug 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/Variations-of-Language-Model-in-Huggingface/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6138-Variations-of-Language-Model-in-Huggingface.jpg" alt="Variations-of-LanguageModel">&lt;/p>
&lt;h1 id="variations-of-language-model-in-huggingface">Variations of Language Model in Huggingface&lt;a class="td-heading-self-link" href="#variations-of-language-model-in-huggingface" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="what-the-model-variable-in-huggingface">What the Model variable in Huggingface?&lt;a class="td-heading-self-link" href="#what-the-model-variable-in-huggingface" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>We know base moels like BERT, T5, GPT2, GPT3 etc are developed by researchers working with different companies. But when we look into huggingface model repository we see other models like GPT2LMHeadModel, GPT2ForSequenceClassification, etc what are these?&lt;/p></description></item><item><title>Compressing Large Language Model</title><link>https://localhost:1313/dsblog/compressing-llm/</link><pubDate>Tue, 07 Nov 2023 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/compressing-llm/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6099-Compressing-LLM.jpg" alt="Compressing Large Language Model">&lt;/p>
&lt;h1 id="compressing-large-language-model">Compressing Large Language Model&lt;a class="td-heading-self-link" href="#compressing-large-language-model" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="is-this-article-for-me">Is this article for me?&lt;a class="td-heading-self-link" href="#is-this-article-for-me" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>If you are looking answers to following question then &amp;ldquo;Yes&amp;rdquo;&lt;/p>
&lt;ul>
&lt;li>What is LLM compression?&lt;/li>
&lt;li>Why is LLM compression necessary?&lt;/li>
&lt;li>What are the different techniques for LLM compression?&lt;/li>
&lt;li>How does quantization work in LLM compression?&lt;/li>
&lt;li>What is pruning, and how does it help in compressing LLMs?&lt;/li>
&lt;li>Can you explain knowledge distillation in the context of LLMs?&lt;/li>
&lt;li>What is low-rank factorization and its role in LLM compression?&lt;/li>
&lt;li>How effective are weight sharing techniques in compressing LLMs?&lt;/li>
&lt;li>What are the trade-offs involved in LLM compression?&lt;/li>
&lt;li>How does fine-tuning work in the context of compressed LLMs?&lt;/li>
&lt;li>What are the benefits of fine-tuning in compressed LLMs?&lt;/li>
&lt;li>What role does hardware play in LLM compression?&lt;/li>
&lt;li>What are the ethical considerations in LLM compression?&lt;/li>
&lt;li>What are the future directions in LLM compression?&lt;/li>
&lt;/ul>
&lt;h2 id="1-what-is-llm-compression">1. &lt;strong>What is LLM Compression?&lt;/strong>&lt;a class="td-heading-self-link" href="#1-what-is-llm-compression" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>LLM (Large Language Model) compression refers to a set of techniques and methodologies aimed at reducing the size of large language models while maintaining their performance as much as possible. Large language models, such as GPT, BERT, and their variants, often contain hundreds of millions to billions of parameters, making them resource-intensive to deploy and run. The sheer size of these models poses challenges in terms of storage, computation, and real-time inference, especially when deploying on devices with limited hardware resources like mobile phones or edge devices.&lt;/p></description></item><item><title>ML Model Development Framework</title><link>https://localhost:1313/dsblog/ML-Model-Development-Framework/</link><pubDate>Sat, 02 Sep 2023 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/ML-Model-Development-Framework/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6096-ML-Model-Development-Framework.jpg" alt="ML Model Development Framework">&lt;/p>
&lt;h1 id="ml-model-development-framework--model-repositories">ML Model Development Framework &amp;amp; Model Repositories&lt;a class="td-heading-self-link" href="#ml-model-development-framework--model-repositories" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>There are hundreds of &lt;a href="https://localhost:1313/dsblog/nlp-tasks">machine learning tasks&lt;/a>. To do these tasks there are &lt;a href="https://localhost:1313/dsblog/Type-of-Databases">thousands of datasets&lt;/a> created by individuals, governments, and corporations. We need to develop AI models using these datasets. There are thousands of models &lt;a href="dsblog/ML-Model-Repository-from-Pinto0309">1&lt;/a>, &lt;a href="https://localhost:1313/dsblog/paperwithcode-resources">2&lt;/a>, &lt;a href="https://localhost:1313/dsblog/What-Are-Transformers-in-AI">3&lt;/a> and many model development frameworks. It is practically mind-blowing to track this whole body of work and understand all this work in its entirety. But if you dive deeper into the following frameworks you will get a fair idea about the overall direction of the work. These frameworks are used to maintain pre-trained model repositories and download pre-trained models. You can develop your own finetuned model using those pre-trained models.&lt;/p></description></item><item><title>Introduction to Neural Network</title><link>https://localhost:1313/dsblog/Introduction-to-Neural-Network/</link><pubDate>Tue, 17 Jan 2023 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/Introduction-to-Neural-Network/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6034-Introduction-to-Neural-Network.jpg" alt="Introduction to Neural Network">&lt;/p>
&lt;h1 id="introduction-to-neural-network">Introduction to Neural Network&lt;a class="td-heading-self-link" href="#introduction-to-neural-network" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="introduction-to-a-perceptron">Introduction to a Perceptron&lt;a class="td-heading-self-link" href="#introduction-to-a-perceptron" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>A perceptron is a type of artificial neural network that can be used for binary classification. It is a simple model that consists of a single layer of artificial neurons and is used to classify input data into one of two categories. The perceptron algorithm learns the weights of the artificial neurons by adjusting them based on the input data and the desired output. The perceptron is considered a basic building block for more complex neural networks.&lt;/p></description></item><item><title>What Are Transformers in AI</title><link>https://localhost:1313/dsblog/What-Are-Transformers-in-AI/</link><pubDate>Tue, 03 Aug 2021 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/What-Are-Transformers-in-AI/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6031-What-are-Transformers-in-AI.jpg" alt="What-are-Transformers-in-AI">&lt;/p>
&lt;h1 id="what-are-transformers-in-ai">What Are Transformers in AI&lt;a class="td-heading-self-link" href="#what-are-transformers-in-ai" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="transformer-architecture">Transformer Architecture&lt;a class="td-heading-self-link" href="#transformer-architecture" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/transformer/transformer-arch.jpg" alt="Transformer">&lt;/p>
&lt;h2 id="background">Background&lt;a class="td-heading-self-link" href="#background" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Whether GPT, ChatGPT, DALL-E, Whisper, Satablity AI or whatever significant you see in the AI worlds nowdays it is because of Transformer Architecture. Transformers are a type of neural network architecture that have several properties that make them effective for modeling data with long-range dependencies. They generally feature a combination of multi-headed attention mechanisms, residual connections, layer normalization, feedforward connections, and positional embeddings.&lt;/p></description></item></channel></rss>