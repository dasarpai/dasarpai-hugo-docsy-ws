<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Embeddings on Blowfish</title><link>https://dasarpai.github.io/tags/embeddings/</link><description>Recent content in Embeddings on Blowfish</description><generator>Hugo</generator><language>en</language><managingEditor>nuno@n9o.xyz (Blowfish)</managingEditor><webMaster>nuno@n9o.xyz (Blowfish)</webMaster><lastBuildDate>Fri, 31 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://dasarpai.github.io/tags/embeddings/index.xml" rel="self" type="application/rss+xml"/><item><title>Exploring Tokenization and Embedding in NLP</title><link>https://dasarpai.github.io/dsblog/exploring-tokenization-and-embedding-in-nlp/</link><pubDate>Fri, 31 Jan 2025 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/exploring-tokenization-and-embedding-in-nlp/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6215-Exploring-Tokenization-in-AI.jpg" alt="Exploring Tokenization and Embedding in NLP">&lt;/p>
&lt;h1 id="exploring-tokenization-and-embedding-in-nlp">Exploring Tokenization and Embedding in NLP&lt;a class="td-heading-self-link" href="#exploring-tokenization-and-embedding-in-nlp" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>Tokenization and embedding are key components of natural language processing (NLP) models. Sometimes people misunderstand tokenization and embedding and this article is to address those issues. This is in the question answer format and addressing following questions.&lt;/p></description></item><item><title>Understanding Contextual Embedding in Transformers</title><link>https://dasarpai.github.io/dsblog/understanding-contextual-embedding-in-transformers/</link><pubDate>Thu, 30 Jan 2025 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/understanding-contextual-embedding-in-transformers/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6214-Understanding-Contextual-Embedding-in-Transformer.jpg" alt="Understanding Contextual Embedding in Transformers">&lt;/p>
&lt;h2 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Embedding can be confusing for many people, and contextual embedding performed by transformers can be even more perplexing. Even after gaining an understanding, many questions remain. In this article, we aim to address the following questions.&lt;/p></description></item><item><title>Exploring Dense Embedding Models in AI</title><link>https://dasarpai.github.io/dsblog/Exploring-Dense-Embedding-Models-in-AI/</link><pubDate>Thu, 10 Oct 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/Exploring-Dense-Embedding-Models-in-AI/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6157-Exploring-Dense-Embedding-Models-in-AI.jpg" alt="Exploring Dense Embedding Models in AI">&lt;/p>
&lt;h2 id="what-is-dense-embedding-in-ai">What is dense embedding in AI?&lt;a class="td-heading-self-link" href="#what-is-dense-embedding-in-ai" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Dense embeddings are critical in many AI applications, particularly in deep learning, where they help reduce data complexity and enhance the modelâ€™s ability to generalize from patterns in data.&lt;/p></description></item></channel></rss>