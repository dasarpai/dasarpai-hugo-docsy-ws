<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Open Source AI on Blowfish</title><link>https://localhost:1313/tags/open-source-ai/</link><description>Recent content in Open Source AI on Blowfish</description><generator>Hugo</generator><language>en</language><managingEditor>nuno@n9o.xyz (Blowfish)</managingEditor><webMaster>nuno@n9o.xyz (Blowfish)</webMaster><lastBuildDate>Sat, 27 Jul 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://localhost:1313/tags/open-source-ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Stanford Alpaca</title><link>https://localhost:1313/dsblog/Stanford-Alpaca/</link><pubDate>Sat, 27 Jul 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/Stanford-Alpaca/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6116-Stanford-Alpaca.jpg" alt="Stanford-Alpaca">&lt;/p>
&lt;h1 id="stanford-alpaca">Stanford Alpaca&lt;a class="td-heading-self-link" href="#stanford-alpaca" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;a href="https://github.com/tatsu-lab/stanford_alpaca">Stanford Alpaca Github Report&lt;/a>&lt;/p>
&lt;ul>
&lt;li>Stanford Alpaca is An &amp;ldquo;Instruction-following&amp;rdquo; LLaMA Model&lt;/li>
&lt;li>This is the repo aims to build and share an instruction-following LLaMA model. The repo contains:
&lt;ul>
&lt;li>The 52K &lt;a href="https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json">instruction-following data&lt;/a> used for fine-tuning the model.&lt;/li>
&lt;li>The code for generating the data.&lt;/li>
&lt;li>The code for fine-tuning the model.&lt;/li>
&lt;li>The code for recovering Alpaca-7B weights from our released weight diff.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="overview">Overview&lt;a class="td-heading-self-link" href="#overview" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>The current &amp;ldquo;Alpaca 7B model&amp;rdquo; is fine-tuned from a &amp;ldquo;7B LLaMA&amp;rdquo; model on 52K instruction-following data generated by the techniques in the Self-Instruct paper.&lt;/li>
&lt;li>Alpaca 7B model behaves similarly to the text-davinci-003 model on the Self-Instruct instruction-following evaluation suite.&lt;/li>
&lt;li>Alpaca is still under development, and there are many limitations that have to be addressed.&lt;/li>
&lt;li>Alphaca is not yet fine-tuned to be safe and harmless.&lt;/li>
&lt;li>Initial release contains the data generation procedure, dataset, and training recipe.&lt;/li>
&lt;li>Model weights can be released if the creators of LLaMA gives permission.&lt;/li>
&lt;li>Live demo to help readers better understand the capabilities and limits of Alpaca is available.&lt;/li>
&lt;li>Based on followin papers:
&lt;ul>
&lt;li>LLaMA: Open and Efficient Foundation Language Models. &lt;a href="https://arxiv.org/abs/2302.13971v1">Hugo2023&lt;/a>&lt;/li>
&lt;li>Self-Instruct: Aligning Language Model with Self Generated Instructions. &lt;a href="https://arxiv.org/abs/2212.10560">Yizhong2022&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Data Release
&lt;ul>
&lt;li>alpaca_data.json contains 52K instruction-following data we used for fine-tuning the Alpaca model. This JSON file is a list of dictionaries, each dictionary contains the following fields: Instruction, input, output (text-davinci-003 geneated answer).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="highlevel-activities-of-the-alpaca-project">Highlevel Activities of the Alpaca Project&lt;a class="td-heading-self-link" href="#highlevel-activities-of-the-alpaca-project" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Highlevel Actitivies done by Stanford Alpaca team and Project Output&lt;/p></description></item></channel></rss>