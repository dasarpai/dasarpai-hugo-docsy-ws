<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NVIDIA on Blowfish</title><link>https://dasarpai.github.io/tags/nvidia/</link><description>Recent content in NVIDIA on Blowfish</description><generator>Hugo</generator><language>en</language><managingEditor>nuno@n9o.xyz (Blowfish)</managingEditor><webMaster>nuno@n9o.xyz (Blowfish)</webMaster><lastBuildDate>Sun, 09 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://dasarpai.github.io/tags/nvidia/index.xml" rel="self" type="application/rss+xml"/><item><title>Demystifying NVIDIA GPUs</title><link>https://dasarpai.github.io/dsblog/demystify-nvidia-gpus/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/demystify-nvidia-gpus/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6216-Demystify-NVIDIA-GPUs.jpg" alt="Demystifying NVIDIA GPUs">&lt;/p>
&lt;h1 id="demystifying-nvidia-gpus">Demystifying NVIDIA GPUs&lt;a class="td-heading-self-link" href="#demystifying-nvidia-gpus" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>NVIDIA has been in the GPU manufacturing business since 1993. They offer hundreds of different types of GPUs for various segments and purposes. For those not in the GPU infrastructure business, it can be confusing to understand even their naming conventions. In this article, I will do my best to help you understand the different types of NVIDIA GPUs and their naming conventions.&lt;/p></description></item><item><title>Shaping Tomorrow with AI: Nvidiaâ€™s Innovations in Graphics, Robotics, and Intelligence</title><link>https://dasarpai.github.io/dsblog/shaping-tomorrow-with-ai-nvidia/</link><pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/shaping-tomorrow-with-ai-nvidia/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6198-shaping-tomorrow-with-ai-nvidia.jpg" alt="Shaping Tomorrow with AI: Nvidiaâ€™s Innovations in Graphics, Robotics, and Intelligence">&lt;/p>
&lt;h1 id="shaping-tomorrow-with-ai-nvidias-innovations-in-graphics-robotics-and-intelligence">Shaping Tomorrow with AI: Nvidiaâ€™s Innovations in Graphics, Robotics, and Intelligence&lt;a class="td-heading-self-link" href="#shaping-tomorrow-with-ai-nvidias-innovations-in-graphics-robotics-and-intelligence" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>This article is based on various online resources, including articles and YouTube videos, but is heavily influenced by the NVIDIA CES 2025 Keynote Speech by Jensen Huang.&lt;/p></description></item><item><title>Exploring Graphics Processing Units (GPUs)</title><link>https://dasarpai.github.io/dsblog/Exploring-GPUs/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/Exploring-GPUs/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6188-Exploring-GPUs.jpg" alt="Exploring Graphics Processing Units (GPUs)">&lt;/p>
&lt;h1 id="exploring-graphics-processing-units-gpus">Exploring Graphics Processing Units (GPUs)&lt;a class="td-heading-self-link" href="#exploring-graphics-processing-units-gpus" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="overall-computational-power-of-gpus">&lt;strong>Overall Computational Power of GPUs&lt;/strong>&lt;a class="td-heading-self-link" href="#overall-computational-power-of-gpus" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>âš¡ &lt;strong>Incredible Calculation Speed:&lt;/strong> Modern GPUs can perform tens of trillions of calculations per second (e.g., 36 trillion for Cyberpunk 2077).&lt;/li>
&lt;li>ðŸŒ &lt;strong>Human Comparison:&lt;/strong> Achieving this manually would require the equivalent of over 4,400 Earths full of people, each doing one calculation every second.&lt;/li>
&lt;/ul>
&lt;h2 id="gpu-vs-cpu">&lt;strong>GPU vs. CPU&lt;/strong>&lt;a class="td-heading-self-link" href="#gpu-vs-cpu" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>ðŸš¢ &lt;strong>Cargo Ship vs. Airplane Analogy:&lt;/strong> GPUs are like cargo ships (massive capacity, slower), and CPUs are like jets (fast, versatile, fewer tasks at once).&lt;/li>
&lt;li>âš–ï¸ &lt;strong>Different Strengths:&lt;/strong> CPUs handle operating systems, flexible tasks, and fewer but more complex instructions. GPUs excel at huge amounts of simple, repetitive calculations.&lt;/li>
&lt;li>ðŸ”€ &lt;strong>Parallel vs. General Purpose:&lt;/strong> GPUs are less flexible but highly parallel, CPUs are more general-purpose and can run a wide variety of programs and instructions.&lt;/li>
&lt;/ul>
&lt;h2 id="gpu-architecture--components-ga102-example">&lt;strong>GPU Architecture &amp;amp; Components (GA102 Example)&lt;/strong>&lt;a class="td-heading-self-link" href="#gpu-architecture--components-ga102-example" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>ðŸ’½ &lt;strong>Central GPU Die (GA102):&lt;/strong> A large chip with 28.3 billion transistors organized into Graphics Processing Clusters (GPCs), Streaming Multiprocessors (SMs), and cores.&lt;/li>
&lt;li>ðŸ—ï¸ &lt;strong>Hierarchical Structure:&lt;/strong> GA102 has 7 GPCs â†’ 12 SMs per GPC â†’ 4 Warps per SM â†’ 32 CUDA Per Wrap and 4 Tensor Per Warmp and 1 Ray Tracing Per GPC.&lt;/li>
&lt;li>ðŸ”¢ &lt;strong>Types of Cores:&lt;/strong>
&lt;ul>
&lt;li>âš™ï¸ CUDA Cores: Handle basic arithmetic (addition, multiplication) most commonly used in gaming.&lt;/li>
&lt;li>ðŸ§© Tensor Cores: Perform massive matrix calculations for AI and neural networks.&lt;/li>
&lt;li>ðŸ’Ž Ray Tracing Cores: Specialized for lighting and reflection calculations in real-time graphics.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="manufacturing--binning">&lt;strong>Manufacturing &amp;amp; Binning&lt;/strong>&lt;a class="td-heading-self-link" href="#manufacturing--binning" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>ðŸ”§ &lt;strong>Shared Chip Design:&lt;/strong> Different GPU models (e.g., 3080, 3090, 3090 Ti) share the same GA102 design.&lt;/li>
&lt;li>ðŸ•³ï¸ &lt;strong>Defects &amp;amp; Binning:&lt;/strong> Manufacturing imperfections result in some cores being disabled. This leads to different â€œtiersâ€ of the same GPU architecture.&lt;/li>
&lt;/ul>
&lt;h2 id="cuda-core-internals">&lt;strong>CUDA Core Internals&lt;/strong>&lt;a class="td-heading-self-link" href="#cuda-core-internals" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>âž• &lt;strong>Simple Calculator Design:&lt;/strong> Each CUDA core is basically a tiny calculator that does fused multiply-add (FMA) and a few other operations.&lt;/li>
&lt;li>ðŸ’» &lt;strong>Common Operations:&lt;/strong> Primarily handles 32-bit floating-point and integer arithmetic. More complex math (division, trignometry) is done by fewer, special function units.&lt;/li>
&lt;/ul>
&lt;h2 id="memory-systems-gddr6x--gddr7">&lt;strong>Memory Systems: GDDR6X &amp;amp; GDDR7&lt;/strong>&lt;a class="td-heading-self-link" href="#memory-systems-gddr6x--gddr7" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>ðŸ’¾ &lt;strong>Graphics Memory:&lt;/strong> GDDR6X chips (by Micron) feed terabytes of data per second into the GPUâ€™s thousands of cores.&lt;/li>
&lt;li>ðŸš€ &lt;strong>High Bandwidth:&lt;/strong> GPU memory operates at huge bandwidths (over 1 terabyte/s) compared to typical CPU memory (~64 GB/s).&lt;/li>
&lt;li>ðŸ”¢ &lt;strong>Beyond Binary:&lt;/strong> GDDR6X and GDDR7 use multiple voltage levels (PAM-4 and PAM-3) to encode more data per signal, increasing transfer rates.&lt;/li>
&lt;li>ðŸ—ï¸ &lt;strong>Future Memory Tech:&lt;/strong> Micron also develops HBM (High Bandwidth Memory) for AI accelerators, stacking memory chips in 3D, greatly boosting capacity and speed while reducing power.&lt;/li>
&lt;/ul>
&lt;h2 id="parallel-computing-concepts-simd--simt">&lt;strong>Parallel Computing Concepts (SIMD &amp;amp; SIMT)&lt;/strong>&lt;a class="td-heading-self-link" href="#parallel-computing-concepts-simd--simt" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>â™»ï¸ &lt;strong>Embarrassingly Parallel:&lt;/strong> Tasks like graphics rendering, Bitcoin mining, or AI training are easily split into millions of independent calculations.&lt;/li>
&lt;li>ðŸ“œ &lt;strong>Single Instruction Multiple Data (SIMD):&lt;/strong> Apply the same instruction to many data points at onceâ€”perfect for transforming millions of vertices in a 3D scene.&lt;/li>
&lt;li>ðŸ”“ &lt;strong>From SIMD to SIMT:&lt;/strong> Newer GPUs use Single Instruction Multiple Threads (SIMT), allowing threads to progress independently and handle complex branching more efficiently.&lt;/li>
&lt;/ul>
&lt;h2 id="thread--warp-organization">&lt;strong>Thread &amp;amp; Warp Organization&lt;/strong>&lt;a class="td-heading-self-link" href="#thread--warp-organization" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>ðŸ“¦ &lt;strong>Thread Hierarchy:&lt;/strong> Threads â†’ Warps (groups of 32 threads) â†’ Thread Blocks â†’ Grids.&lt;/li>
&lt;li>ðŸŽ›ï¸ &lt;strong>Gigathread Engine:&lt;/strong> Manages the allocation of thread blocks to streaming multiprocessors, optimizing parallel processing.&lt;/li>
&lt;/ul>
&lt;h2 id="practical-applications">&lt;strong>Practical Applications&lt;/strong>&lt;a class="td-heading-self-link" href="#practical-applications" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>ðŸŽ® &lt;strong>Video Games:&lt;/strong> GPUs transform coordinates, apply textures, shading, and handle complex rendering pipelines. Millions of identical operations on different vertices and pixels are done in parallel.&lt;/li>
&lt;li>â‚¿ &lt;strong>Bitcoin Mining:&lt;/strong> GPUs can run the SHA-256 hashing algorithm in parallel many millions of times per second. Though now replaced by ASIC miners, GPUs were initially very efficient at this.&lt;/li>
&lt;li>ðŸ¤– &lt;strong>AI &amp;amp; Neural Networks:&lt;/strong> Tensor cores accelerate matrix multiplications critical for training neural nets and powering generative AI.&lt;/li>
&lt;li>ðŸ’¡ &lt;strong>Ray Tracing:&lt;/strong> Specialized cores handle ray tracing calculations for realistic lighting and reflections in real-time graphics.&lt;/li>
&lt;/ul>
&lt;h2 id="microns-role--advancements">&lt;strong>Micronâ€™s Role &amp;amp; Advancements&lt;/strong>&lt;a class="td-heading-self-link" href="#microns-role--advancements" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>ðŸ­ &lt;strong>Micron Memory Chips:&lt;/strong> GDDR6X and future GDDR7 designed by Micron power high-speed data transfers on GPUs.&lt;/li>
&lt;li>ðŸ”® &lt;strong>Innovations in Memory:&lt;/strong> High Bandwidth Memory (HBM) for AI chips stacks DRAM vertically, creating high-capacity, high-throughput solutions at lower energy costs.&lt;/li>
&lt;li>ðŸ“š &lt;strong>Technological Marvel:&lt;/strong> Modern graphics cards are a blend of advanced materials, clever architectures, and innovative manufacturing. They enable astonishing levels of visual realism, parallel computation, and AI capabilities.&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=h9Z4oGN89MU">How do Graphics Cards Work? Exploring GPU Architecture&lt;/a>&lt;/p></description></item><item><title>Interaction: Jenson Huang and Masayoshi Son Interaction</title><link>https://dasarpai.github.io/booksummary/Jenson-Masayoshi-Interaction-2024/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/booksummary/Jenson-Masayoshi-Interaction-2024/</guid><description>&lt;h1 id="nvidia-and-softbank-collaboration">NVIDIA and SoftBank Collaboration&lt;a class="td-heading-self-link" href="#nvidia-and-softbank-collaboration" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>&lt;img src="../../assets/images/booksummary/7536-Jenson-Huang-Masayoshi-Son.jpg" alt="Jenson Huang and Masayoshi Son Interaction">&lt;/p>
&lt;h2 id="summary-ai-collaboration-discussion-between-jensen-huang-nvidia-and-masayoshi-son-softbank">Summary: AI Collaboration Discussion between Jensen Huang (NVIDIA) and Masayoshi Son (SoftBank)&lt;a class="td-heading-self-link" href="#summary-ai-collaboration-discussion-between-jensen-huang-nvidia-and-masayoshi-son-softbank" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>This thematic breakdown captures the major points discussed, emphasizing the collaborative vision and potential of AI to transform Japan and beyond.&lt;/p></description></item><item><title>Introduction to NVIDIA and Products</title><link>https://dasarpai.github.io/dsblog/introduction-nvidia-products/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/introduction-nvidia-products/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6182-Introduction-NVIDIA-Products.jpg" alt="Introduction-NVIDIA-Products">&lt;/p>
&lt;h2 id="nvidia-timeline">NVIDIA Timeline&lt;a class="td-heading-self-link" href="#nvidia-timeline" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>NVIDIA Corporation has an illustrious history since its founding in 1993. It started as a graphics processing pioneer and has grown into a global leader in AI, gaming, data center technologies, and more. Here&amp;rsquo;s a timeline of key milestones and activities:&lt;/p></description></item><item><title>Interview with Jensen Huang @ NVIDIA at Dealbook Event 2023</title><link>https://dasarpai.github.io/booksummary/interview-jensen-huang-nvidia-dealbook-event-2023/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/booksummary/interview-jensen-huang-nvidia-dealbook-event-2023/</guid><description>&lt;p>&lt;img src="../../assets/images/booksummary/7530-Interview-Jensen-Huang-NVIDIA-Dealbook-Event-2023.jpg" alt="Interview-Jensen-Huang-NVIDIA-Dealbook-Event-2024">&lt;/p>
&lt;h1 id="insights-from-the-interview-with-jensen-huang--nvidia-at-dealbook-event-2023">Insights from the Interview with Jensen Huang @ NVIDIA at Dealbook Event 2023&lt;a class="td-heading-self-link" href="#insights-from-the-interview-with-jensen-huang--nvidia-at-dealbook-event-2023" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="theme-1-the-evolution-of-ai-computing">&lt;strong>Theme 1: The Evolution of AI Computing&lt;/strong>&lt;a class="td-heading-self-link" href="#theme-1-the-evolution-of-ai-computing" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Reinvention of Computing&lt;/strong>: Jensen emphasized that AI development isn&amp;rsquo;t just about designing new chips but about a comprehensive reinvention of computing systems, including software, hardware, and networking.&lt;/li>
&lt;li>&lt;strong>Transition to Generative Computing&lt;/strong>: Most current computing focuses on data retrieval, but the future involves combining retrieval with generative capabilities, transforming how systems operate globally.&lt;/li>
&lt;li>&lt;strong>Origins of AI Hardware&lt;/strong>: Nvidia&amp;rsquo;s first AI supercomputer, DGX, was designed for internal use but gained prominence when figures like Elon Musk requested it. It highlighted the potential of neural networks and deep learning.&lt;/li>
&lt;/ul>
&lt;h2 id="theme-2-ai-progress-and-challenges">&lt;strong>Theme 2: AI Progress and Challenges&lt;/strong>&lt;a class="td-heading-self-link" href="#theme-2-ai-progress-and-challenges" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Milestones in AI&lt;/strong>: Nvidia foresaw AI&amp;rsquo;s impact in 2012 with AlexNet, a neural network capable of unparalleled visual recognition. This breakthrough informed their investments in AI infrastructure.&lt;/li>
&lt;li>&lt;strong>Current AI Limitations&lt;/strong>: AI today excels in tasks like perception and pattern recognition but struggles with multi-step reasoning and decision trees. Addressing these gaps is a major focus for researchers.&lt;/li>
&lt;li>&lt;strong>Predictions for AGI&lt;/strong>: Within five years, AI systems might perform tasks that meet basic human intelligence standards, pushing closer to Artificial General Intelligence (AGI).&lt;/li>
&lt;/ul>
&lt;h2 id="theme-3-corporate-governance-and-ai-ethics">&lt;strong>Theme 3: Corporate Governance and AI Ethics&lt;/strong>&lt;a class="td-heading-self-link" href="#theme-3-corporate-governance-and-ai-ethics" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Lessons from OpenAI Drama&lt;/strong>: Huang underscored the importance of strong corporate governance, contrasting Nvidiaâ€™s for-profit structure with OpenAIâ€™s non-profit roots, which faced challenges balancing innovation and control.&lt;/li>
&lt;li>&lt;strong>Regulation and Safety&lt;/strong>: AI must be managed with principles similar to those governing autonomous systems like airplanes, emphasizing redundancy, stress testing, and monitoring for safety.&lt;/li>
&lt;/ul>
&lt;h2 id="theme-4-geopolitical-and-strategic-challenges">&lt;strong>Theme 4: Geopolitical and Strategic Challenges&lt;/strong>&lt;a class="td-heading-self-link" href="#theme-4-geopolitical-and-strategic-challenges" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Chip Independence&lt;/strong>: Achieving supply chain independence for semiconductors could take decades. Nvidia uses 35,000 components sourced globally, underscoring the complexity of achieving autonomy.&lt;/li>
&lt;li>&lt;strong>Export Regulations and China&lt;/strong>: Nvidia must comply with U.S. regulations limiting advanced chip exports to China. While these policies aim to safeguard national security, they risk spurring innovation in competing markets like China.&lt;/li>
&lt;li>&lt;strong>Surprise from Huawei&lt;/strong>: Chinaâ€™s progress with a 7nm chip was acknowledged, but Huang stated it reflected iterative advancements rather than a revolutionary leap.&lt;/li>
&lt;/ul>
&lt;h2 id="theme-5-nvidias-strategic-edge">&lt;strong>Theme 5: Nvidiaâ€™s Strategic Edge&lt;/strong>&lt;a class="td-heading-self-link" href="#theme-5-nvidias-strategic-edge" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Broad Vision&lt;/strong>: Nvidia sees AI as an ecosystem that integrates chips, software, and infrastructure. The companyâ€™s comprehensive approach positions it uniquely in the tech landscape.&lt;/li>
&lt;li>&lt;strong>Direct Reports Philosophy&lt;/strong>: Huang manages 50 direct reports to maintain fluid communication and reduce organizational hierarchy, ensuring swift decision-making and alignment.&lt;/li>
&lt;li>&lt;strong>Cultural Resilience&lt;/strong>: Huangâ€™s philosophy emphasizes operating with a blend of ambition and caution, drawn from Nvidiaâ€™s history of overcoming adversity.&lt;/li>
&lt;/ul>
&lt;h2 id="theme-6-the-future-of-nvidia-and-ai">&lt;strong>Theme 6: The Future of Nvidia and AI&lt;/strong>&lt;a class="td-heading-self-link" href="#theme-6-the-future-of-nvidia-and-ai" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Reinvention of Data Centers&lt;/strong>: With AI reshaping computing, existing data centers worth trillions will require retooling, heralding a new era in infrastructure.&lt;/li>
&lt;li>&lt;strong>Ignorance and Innovation&lt;/strong>: Huang highlighted how the initial ignorance of challenges allowed Nvidia to innovate fearlessly. This &amp;ldquo;teenage attitude&amp;rdquo; continues to drive their culture.&lt;/li>
&lt;li>&lt;strong>Long-Term Vision&lt;/strong>: Nvidia is doubling down on creating specialized AI systems for fields like drug discovery and chip design, ensuring the company remains at the forefront of AI-driven innovation.&lt;/li>
&lt;/ul>
&lt;h2 id="key-takeaways">&lt;strong>Key Takeaways&lt;/strong>&lt;a class="td-heading-self-link" href="#key-takeaways" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>Nvidia&amp;rsquo;s leadership in AI stems from a holistic vision that goes beyond hardware to include software and systems integration.&lt;/li>
&lt;li>Jensen Huang sees AI&amp;rsquo;s evolution as a fundamental shift comparable to the industrial revolution of computing.&lt;/li>
&lt;li>Balancing innovation with ethics, governance, and geopolitics will shape Nvidiaâ€™s future and the AI industry at large.&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=Pkj-BLHs6dE">Full Youtube Video - Jensen Huang of Nvidia on the Future of A.I. in DealBook Summit 2023&lt;/a>&lt;/p></description></item></channel></rss>