<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Model Optimization on Blowfish</title><link>https://localhost:1313/tags/model-optimization/</link><description>Recent content in Model Optimization on Blowfish</description><generator>Hugo</generator><language>en</language><managingEditor>nuno@n9o.xyz (Blowfish)</managingEditor><webMaster>nuno@n9o.xyz (Blowfish)</webMaster><lastBuildDate>Tue, 12 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://localhost:1313/tags/model-optimization/index.xml" rel="self" type="application/rss+xml"/><item><title>Exploring GGUF and Other Model Formats</title><link>https://localhost:1313/dsblog/exploring-gguf-and-other-model-formats/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/exploring-gguf-and-other-model-formats/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6180-exploring-gguf.jpg" alt="Understanding GGUF and Other Model Formats in Machine Learning">&lt;/p>
&lt;h1 id="understanding-gguf-and-other-model-formats-in-machine-learning">&lt;strong>Understanding GGUF and Other Model Formats in Machine Learning&lt;/strong>&lt;a class="td-heading-self-link" href="#understanding-gguf-and-other-model-formats-in-machine-learning" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>As machine learning models continue to grow in complexity, the need for efficient, flexible, and versatile model formats becomes more pronounced. While formats like ONNX, TensorFlow’s SavedModel, and PyTorch’s native format have been around for some time, newer formats like GGUF are gaining attention for their unique benefits. This article explores these formats, their use cases, and how they support various aspects of machine learning, including deployment, compatibility, and optimization.&lt;/p></description></item><item><title>Machine Learning Key Concepts</title><link>https://localhost:1313/dsblog/Machine-Learning-Key-Concepts/</link><pubDate>Thu, 03 Oct 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/Machine-Learning-Key-Concepts/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6152-Machine-Learning-Key-Concepts.jpg" alt="Exploring Docker and VS Code Integration">&lt;/p>
&lt;h1 id="machine-learning-key-concepts">Machine Learning Key Concepts&lt;a class="td-heading-self-link" href="#machine-learning-key-concepts" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>In this article Essential Machine Learning Techniques/Concepts are Explained, some of them are are Cross-Validation, Hyperparameter Optimization, Machine learning types and much More.&lt;/p></description></item><item><title>Compressing Large Language Model</title><link>https://localhost:1313/dsblog/compressing-llm/</link><pubDate>Tue, 07 Nov 2023 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/compressing-llm/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6099-Compressing-LLM.jpg" alt="Compressing Large Language Model">&lt;/p>
&lt;h1 id="compressing-large-language-model">Compressing Large Language Model&lt;a class="td-heading-self-link" href="#compressing-large-language-model" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="is-this-article-for-me">Is this article for me?&lt;a class="td-heading-self-link" href="#is-this-article-for-me" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>If you are looking answers to following question then &amp;ldquo;Yes&amp;rdquo;&lt;/p>
&lt;ul>
&lt;li>What is LLM compression?&lt;/li>
&lt;li>Why is LLM compression necessary?&lt;/li>
&lt;li>What are the different techniques for LLM compression?&lt;/li>
&lt;li>How does quantization work in LLM compression?&lt;/li>
&lt;li>What is pruning, and how does it help in compressing LLMs?&lt;/li>
&lt;li>Can you explain knowledge distillation in the context of LLMs?&lt;/li>
&lt;li>What is low-rank factorization and its role in LLM compression?&lt;/li>
&lt;li>How effective are weight sharing techniques in compressing LLMs?&lt;/li>
&lt;li>What are the trade-offs involved in LLM compression?&lt;/li>
&lt;li>How does fine-tuning work in the context of compressed LLMs?&lt;/li>
&lt;li>What are the benefits of fine-tuning in compressed LLMs?&lt;/li>
&lt;li>What role does hardware play in LLM compression?&lt;/li>
&lt;li>What are the ethical considerations in LLM compression?&lt;/li>
&lt;li>What are the future directions in LLM compression?&lt;/li>
&lt;/ul>
&lt;h2 id="1-what-is-llm-compression">1. &lt;strong>What is LLM Compression?&lt;/strong>&lt;a class="td-heading-self-link" href="#1-what-is-llm-compression" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>LLM (Large Language Model) compression refers to a set of techniques and methodologies aimed at reducing the size of large language models while maintaining their performance as much as possible. Large language models, such as GPT, BERT, and their variants, often contain hundreds of millions to billions of parameters, making them resource-intensive to deploy and run. The sheer size of these models poses challenges in terms of storage, computation, and real-time inference, especially when deploying on devices with limited hardware resources like mobile phones or edge devices.&lt;/p></description></item><item><title>Model Tuning with VertexAI</title><link>https://localhost:1313/dsblog/Model-Tuning-with-VertexAI/</link><pubDate>Mon, 24 Jul 2023 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/Model-Tuning-with-VertexAI/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6081-Model-Tuning-with-VertexAI.jpg" alt="Model Tuning with VertexAI">&lt;/p>
&lt;h1 id="tuning-large-language-model-with-vertexai">Tuning Large Language Model with VertexAI&lt;a class="td-heading-self-link" href="#tuning-large-language-model-with-vertexai" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="why-model-tuning">Why Model Tuning?&lt;a class="td-heading-self-link" href="#why-model-tuning" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Tuning is required when you want the model to learn something niche or specific that deviates from general language patterns.&lt;/p></description></item><item><title>Cost Functions and Optimizers in Machine Learning</title><link>https://localhost:1313/dsblog/Cost-Functions-and-Optimizers-in-Machine-Learning/</link><pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/Cost-Functions-and-Optimizers-in-Machine-Learning/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6045-Cost-Functions-and-Optimizers-in-Machine-Learning.jpg" alt="Cost-Functions-and-Optimizers-in-Machine-Learning">&lt;/p>
&lt;h1 id="cost-functions-and-optimizers-in-machine-learning">Cost-Functions-and-Optimizers-in-Machine-Learning&lt;a class="td-heading-self-link" href="#cost-functions-and-optimizers-in-machine-learning" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="what-is-machine-learning">What is machine learning?&lt;a class="td-heading-self-link" href="#what-is-machine-learning" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Machine learning is a subfield of artificial intelligence that focuses on the &lt;strong>development of algorithms and statistical models&lt;/strong> that enable computers to improve their performance on a specific task through experience.&lt;/p></description></item></channel></rss>