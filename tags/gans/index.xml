<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GANs on Blowfish</title><link>https://dasarpai.github.io/tags/gans/</link><description>Recent content in GANs on Blowfish</description><generator>Hugo</generator><language>en</language><managingEditor>nuno@n9o.xyz (Blowfish)</managingEditor><webMaster>nuno@n9o.xyz (Blowfish)</webMaster><lastBuildDate>Sun, 16 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://dasarpai.github.io/tags/gans/index.xml" rel="self" type="application/rss+xml"/><item><title>State of the Art Image Generation Models in Computer Vision: A Comprehensive Overview</title><link>https://dasarpai.github.io/dsblog/state-of-the-art-image-generation-models-in-computer-vision/</link><pubDate>Sun, 16 Feb 2025 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/state-of-the-art-image-generation-models-in-computer-vision/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6219-State-of-the-Art-Computer-Vision-Models.jpg" alt="State-of-the-Art 3D Image Generation Models">&lt;/p>
&lt;h1 id="state-of-the-art-computer-vision-models">State of the Art Computer Vision Models&lt;a class="td-heading-self-link" href="#state-of-the-art-computer-vision-models" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="what-are-the-different-methods-of-image-generation">What are the different methods of Image generation?&lt;a class="td-heading-self-link" href="#what-are-the-different-methods-of-image-generation" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>There are several methods for image generation. Diffusion models are currently the state-of-the-art due to their balance of quality, flexibility, and scalability. However, other methods like GANs and autoregressive models remain relevant for specific use cases. Let&amp;rsquo;s see them one by one.&lt;/p></description></item><item><title>Understanding LLM GAN and Transformers</title><link>https://dasarpai.github.io/dsblog/Understanding-LLM-GAN-and-Transformers/</link><pubDate>Fri, 26 Jul 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/Understanding-LLM-GAN-and-Transformers/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6127-Understanding-LLM-GAN-and-Transformers.jpg" alt="Understanding-LLM-GAN-Transformers">&lt;/p>
&lt;h1 id="understanding-llm-gan-and-transformers">Understanding LLM, GAN and Transformers&lt;a class="td-heading-self-link" href="#understanding-llm-gan-and-transformers" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="llm-layers">LLM Layers&lt;a class="td-heading-self-link" href="#llm-layers" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Large Language Models (LLMs) are typically based on Transformer architectures, which consist of several types of layers that work together to process and generate text. Here are the primary kinds of layers found in an LLM:&lt;/p></description></item></channel></rss>