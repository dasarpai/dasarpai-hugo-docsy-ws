<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Local Models on Blowfish</title><link>https://localhost:1313/tags/local-models/</link><description>Recent content in Local Models on Blowfish</description><generator>Hugo</generator><language>en</language><managingEditor>nuno@n9o.xyz (Blowfish)</managingEditor><webMaster>nuno@n9o.xyz (Blowfish)</webMaster><lastBuildDate>Sat, 19 Apr 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://localhost:1313/tags/local-models/index.xml" rel="self" type="application/rss+xml"/><item><title>Ollama Setup and Running Models</title><link>https://localhost:1313/dsblog/Ollama-Setup-and-Running-Models/</link><pubDate>Sat, 19 Apr 2025 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://localhost:1313/dsblog/Ollama-Setup-and-Running-Models/</guid><description>&lt;p>&lt;img src="https://localhost:1313/assets/images/dspost/dsp6262-Ollama-Setup-and-Running-Models.jpg" alt="Ollama Setup and Running Models">&lt;/p>
&lt;h1 id="ollama-running-large-language-models-locally">Ollama: Running Large Language Models Locally&lt;a class="td-heading-self-link" href="#ollama-running-large-language-models-locally" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>The landscape of Artificial Intelligence (AI) and Large Language Models (LLMs) has traditionally been dominated by cloud-based services. While powerful, these often come with costs, privacy concerns, and require constant internet connectivity. Ollama emerges as a compelling open-source solution, designed to simplify the process of downloading, managing, and running LLMs directly on your local machine. This approach offers significant advantages, including enhanced privacy, cost savings, offline capability, and greater control over the models you use.&lt;/p></description></item></channel></rss>