<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Model Training on Blowfish</title><link>https://dasarpai.github.io/tags/model-training/</link><description>Recent content in Model Training on Blowfish</description><generator>Hugo</generator><language>en</language><managingEditor>nuno@n9o.xyz (Blowfish)</managingEditor><webMaster>nuno@n9o.xyz (Blowfish)</webMaster><lastBuildDate>Sun, 04 Aug 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://dasarpai.github.io/tags/model-training/index.xml" rel="self" type="application/rss+xml"/><item><title>LLM Architecture and Training</title><link>https://dasarpai.github.io/dsblog/LLM-Architecture-and-Training/</link><pubDate>Sun, 04 Aug 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/LLM-Architecture-and-Training/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6129-LLM-Architecture-and-Training.jpg" alt="LLM-Architecture-and-Training">&lt;/p>
&lt;h1 id="understanding-llm-architectures-and-model-training">&lt;strong>Understanding LLM Architectures and Model Training&lt;/strong>&lt;a class="td-heading-self-link" href="#understanding-llm-architectures-and-model-training" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;p>Large Language Models (LLMs) are transforming the field of artificial intelligence by enabling machines to understand and generate human language with unprecedented accuracy. This article delves into the architecture, training methods, and practical applications of LLMs. Weâ€™ll explore the core components that make these models so powerful and explain how they are trained and fine-tuned for real-world use cases.&lt;/p></description></item><item><title>Why to Finetune LLM?</title><link>https://dasarpai.github.io/dsblog/why-to-finetune-llm/</link><pubDate>Sun, 28 Jul 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/why-to-finetune-llm/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6115-why-to-finetune-llm.jpg" alt="Why to Finetune LLM?">&lt;/p>
&lt;h1 id="finetuning-fewshot-learning-why-and-how">Finetuning, Fewshot Learning, Why and How?&lt;a class="td-heading-self-link" href="#finetuning-fewshot-learning-why-and-how" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="why-to-finetune-a-llm">Why to finetune a LLM?&lt;a class="td-heading-self-link" href="#why-to-finetune-a-llm" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Fine-tuning a large language model (LLM) can provide several benefits, depending on your specific needs and objectives. Here are some key reasons to consider fine-tuning an LLM:&lt;/p></description></item><item><title>Stanford Alpaca</title><link>https://dasarpai.github.io/dsblog/Stanford-Alpaca/</link><pubDate>Sat, 27 Jul 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/Stanford-Alpaca/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6116-Stanford-Alpaca.jpg" alt="Stanford-Alpaca">&lt;/p>
&lt;h1 id="stanford-alpaca">Stanford Alpaca&lt;a class="td-heading-self-link" href="#stanford-alpaca" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;a href="https://github.com/tatsu-lab/stanford_alpaca">Stanford Alpaca Github Report&lt;/a>&lt;/p>
&lt;ul>
&lt;li>Stanford Alpaca is An &amp;ldquo;Instruction-following&amp;rdquo; LLaMA Model&lt;/li>
&lt;li>This is the repo aims to build and share an instruction-following LLaMA model. The repo contains:
&lt;ul>
&lt;li>The 52K &lt;a href="https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json">instruction-following data&lt;/a> used for fine-tuning the model.&lt;/li>
&lt;li>The code for generating the data.&lt;/li>
&lt;li>The code for fine-tuning the model.&lt;/li>
&lt;li>The code for recovering Alpaca-7B weights from our released weight diff.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="overview">Overview&lt;a class="td-heading-self-link" href="#overview" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>The current &amp;ldquo;Alpaca 7B model&amp;rdquo; is fine-tuned from a &amp;ldquo;7B LLaMA&amp;rdquo; model on 52K instruction-following data generated by the techniques in the Self-Instruct paper.&lt;/li>
&lt;li>Alpaca 7B model behaves similarly to the text-davinci-003 model on the Self-Instruct instruction-following evaluation suite.&lt;/li>
&lt;li>Alpaca is still under development, and there are many limitations that have to be addressed.&lt;/li>
&lt;li>Alphaca is not yet fine-tuned to be safe and harmless.&lt;/li>
&lt;li>Initial release contains the data generation procedure, dataset, and training recipe.&lt;/li>
&lt;li>Model weights can be released if the creators of LLaMA gives permission.&lt;/li>
&lt;li>Live demo to help readers better understand the capabilities and limits of Alpaca is available.&lt;/li>
&lt;li>Based on followin papers:
&lt;ul>
&lt;li>LLaMA: Open and Efficient Foundation Language Models. &lt;a href="https://arxiv.org/abs/2302.13971v1">Hugo2023&lt;/a>&lt;/li>
&lt;li>Self-Instruct: Aligning Language Model with Self Generated Instructions. &lt;a href="https://arxiv.org/abs/2212.10560">Yizhong2022&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Data Release
&lt;ul>
&lt;li>alpaca_data.json contains 52K instruction-following data we used for fine-tuning the Alpaca model. This JSON file is a list of dictionaries, each dictionary contains the following fields: Instruction, input, output (text-davinci-003 geneated answer).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="highlevel-activities-of-the-alpaca-project">Highlevel Activities of the Alpaca Project&lt;a class="td-heading-self-link" href="#highlevel-activities-of-the-alpaca-project" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Highlevel Actitivies done by Stanford Alpaca team and Project Output&lt;/p></description></item><item><title>Cost Functions and Optimizers in Machine Learning</title><link>https://dasarpai.github.io/dsblog/Cost-Functions-and-Optimizers-in-Machine-Learning/</link><pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/Cost-Functions-and-Optimizers-in-Machine-Learning/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6045-Cost-Functions-and-Optimizers-in-Machine-Learning.jpg" alt="Cost-Functions-and-Optimizers-in-Machine-Learning">&lt;/p>
&lt;h1 id="cost-functions-and-optimizers-in-machine-learning">Cost-Functions-and-Optimizers-in-Machine-Learning&lt;a class="td-heading-self-link" href="#cost-functions-and-optimizers-in-machine-learning" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="what-is-machine-learning">What is machine learning?&lt;a class="td-heading-self-link" href="#what-is-machine-learning" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Machine learning is a subfield of artificial intelligence that focuses on the &lt;strong>development of algorithms and statistical models&lt;/strong> that enable computers to improve their performance on a specific task through experience.&lt;/p></description></item><item><title>Introduction to Neural Network</title><link>https://dasarpai.github.io/dsblog/Introduction-to-Neural-Network/</link><pubDate>Tue, 17 Jan 2023 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/Introduction-to-Neural-Network/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6034-Introduction-to-Neural-Network.jpg" alt="Introduction to Neural Network">&lt;/p>
&lt;h1 id="introduction-to-neural-network">Introduction to Neural Network&lt;a class="td-heading-self-link" href="#introduction-to-neural-network" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="introduction-to-a-perceptron">Introduction to a Perceptron&lt;a class="td-heading-self-link" href="#introduction-to-a-perceptron" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>A perceptron is a type of artificial neural network that can be used for binary classification. It is a simple model that consists of a single layer of artificial neurons and is used to classify input data into one of two categories. The perceptron algorithm learns the weights of the artificial neurons by adjusting them based on the input data and the desired output. The perceptron is considered a basic building block for more complex neural networks.&lt;/p></description></item></channel></rss>