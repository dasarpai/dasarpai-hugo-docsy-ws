<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI Models on Blowfish</title><link>https://dasarpai.github.io/tags/ai-models/</link><description>Recent content in AI Models on Blowfish</description><generator>Hugo</generator><language>en</language><managingEditor>nuno@n9o.xyz (Blowfish)</managingEditor><webMaster>nuno@n9o.xyz (Blowfish)</webMaster><lastBuildDate>Sun, 23 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://dasarpai.github.io/tags/ai-models/index.xml" rel="self" type="application/rss+xml"/><item><title>Unlocking the Power of Prompts: A Comprehensive Guide to Prompt Engineering</title><link>https://dasarpai.github.io/dsblog/unlocking-the-power-of-prompts/</link><pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/unlocking-the-power-of-prompts/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6226-Unlocking-the-Power-of-Prompting.jpg" alt="Unlocking the Power of Prompts">&lt;/p>
&lt;h1 id="unlocking-the-power-of-prompts">Unlocking the Power of Prompts&lt;a class="td-heading-self-link" href="#unlocking-the-power-of-prompts" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>What can you do with Art of Prompting and powerful AI Models? You can use prompts to accomplish almost any task that requires human intelligence. However, prompting is just one part of the process—you also need a powerful model to execute these prompts effectively.&lt;/p></description></item><item><title>Power of Chinese AI Models</title><link>https://dasarpai.github.io/dsblog/power-of-chinese-ai-models/</link><pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/power-of-chinese-ai-models/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6212-Power-of-Chinese-AI-Models.jpg" alt="Power of Chinese AI Models">&lt;/p>
&lt;h1 id="power-of-chinese-ai-models">Power of Chinese AI Models&lt;a class="td-heading-self-link" href="#power-of-chinese-ai-models" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>After the Deepseek R1 turmoil in the market, there has been a shift in attention towards China. The West is now looking towards the East, and even those in the East are turning their gaze northward.&lt;/p></description></item><item><title>Types of Large Language Models (LLM)</title><link>https://dasarpai.github.io/dsblog/Types-of-LLM/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/Types-of-LLM/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6171-Types-of-LLM.jpg" alt="">&lt;/p>
&lt;h2 id="introduction">&lt;strong>Introduction:&lt;/strong>&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>The world of Generative AI (GenAI) is expanding at an astonishing rate, with new models emerging almost daily, each sporting unique names, capabilities, versions, and sizes. For AI professionals, keeping track of these models can feel like a full-time job. But for business users, IT professionals, and software developers trying to make the right choice, understanding the model’s name and what it represents can seem overwhelming. Wouldn’t it be helpful if we could decode the meaning behind these names to know if a model fits our needs and is worth the investment? In this article, we’ll break down how the names of GenAI models can reveal clues about their functionality and suitability for specific tasks, helping you make informed decisions with confidence.&lt;/p></description></item><item><title>Stanford Alpaca</title><link>https://dasarpai.github.io/dsblog/Stanford-Alpaca/</link><pubDate>Sat, 27 Jul 2024 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/Stanford-Alpaca/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6116-Stanford-Alpaca.jpg" alt="Stanford-Alpaca">&lt;/p>
&lt;h1 id="stanford-alpaca">Stanford Alpaca&lt;a class="td-heading-self-link" href="#stanford-alpaca" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;a href="https://github.com/tatsu-lab/stanford_alpaca">Stanford Alpaca Github Report&lt;/a>&lt;/p>
&lt;ul>
&lt;li>Stanford Alpaca is An &amp;ldquo;Instruction-following&amp;rdquo; LLaMA Model&lt;/li>
&lt;li>This is the repo aims to build and share an instruction-following LLaMA model. The repo contains:
&lt;ul>
&lt;li>The 52K &lt;a href="https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json">instruction-following data&lt;/a> used for fine-tuning the model.&lt;/li>
&lt;li>The code for generating the data.&lt;/li>
&lt;li>The code for fine-tuning the model.&lt;/li>
&lt;li>The code for recovering Alpaca-7B weights from our released weight diff.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="overview">Overview&lt;a class="td-heading-self-link" href="#overview" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>The current &amp;ldquo;Alpaca 7B model&amp;rdquo; is fine-tuned from a &amp;ldquo;7B LLaMA&amp;rdquo; model on 52K instruction-following data generated by the techniques in the Self-Instruct paper.&lt;/li>
&lt;li>Alpaca 7B model behaves similarly to the text-davinci-003 model on the Self-Instruct instruction-following evaluation suite.&lt;/li>
&lt;li>Alpaca is still under development, and there are many limitations that have to be addressed.&lt;/li>
&lt;li>Alphaca is not yet fine-tuned to be safe and harmless.&lt;/li>
&lt;li>Initial release contains the data generation procedure, dataset, and training recipe.&lt;/li>
&lt;li>Model weights can be released if the creators of LLaMA gives permission.&lt;/li>
&lt;li>Live demo to help readers better understand the capabilities and limits of Alpaca is available.&lt;/li>
&lt;li>Based on followin papers:
&lt;ul>
&lt;li>LLaMA: Open and Efficient Foundation Language Models. &lt;a href="https://arxiv.org/abs/2302.13971v1">Hugo2023&lt;/a>&lt;/li>
&lt;li>Self-Instruct: Aligning Language Model with Self Generated Instructions. &lt;a href="https://arxiv.org/abs/2212.10560">Yizhong2022&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Data Release
&lt;ul>
&lt;li>alpaca_data.json contains 52K instruction-following data we used for fine-tuning the Alpaca model. This JSON file is a list of dictionaries, each dictionary contains the following fields: Instruction, input, output (text-davinci-003 geneated answer).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="highlevel-activities-of-the-alpaca-project">Highlevel Activities of the Alpaca Project&lt;a class="td-heading-self-link" href="#highlevel-activities-of-the-alpaca-project" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Highlevel Actitivies done by Stanford Alpaca team and Project Output&lt;/p></description></item><item><title>What is LLM</title><link>https://dasarpai.github.io/dsblog/what-is-llm/</link><pubDate>Fri, 18 Aug 2023 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/what-is-llm/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6087-What-is-LLM.jpg" alt="What is LLM">&lt;/p>
&lt;h1 id="what-is-large-language-model">What is Large Language Model&lt;a class="td-heading-self-link" href="#what-is-large-language-model" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>LLM stands for &lt;strong>Large Language Model&lt;/strong>. It is a type of artificial intelligence (AI) model that is trained on a massive dataset of text and code. This allows LLMs to learn the statistical relationships between words and phrases, and to generate text that is similar to the text that they were trained on.&lt;/p></description></item><item><title>Model Garden of VertexAI</title><link>https://dasarpai.github.io/dsblog/Model-Garden-of-VertexAI/</link><pubDate>Wed, 21 Jun 2023 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/Model-Garden-of-VertexAI/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6065-Model-Garden-of-VertexAI.jpg" alt="All Resources to Learn Data Science">&lt;/p>
&lt;h1 id="model-garden-of-vertexai">Model Garden of VertexAI:&lt;a class="td-heading-self-link" href="#model-garden-of-vertexai" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="unlocking-the-power-of-googles-vertexai-exploring-the-world-of-pre-built-models-for-ai-tasks">Unlocking the Power of Google&amp;rsquo;s VertexAI: Exploring the World of Pre-Built Models for AI Tasks&lt;a class="td-heading-self-link" href="#unlocking-the-power-of-googles-vertexai-exploring-the-world-of-pre-built-models-for-ai-tasks" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h2 id="introduction">Introduction:&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Artificial Intelligence (AI) has transformed numerous industries, from healthcare and finance to e-commerce, logistic, eduction and entertainment. But the complexity of developing machine learning models often poses a challenge. As the demand for AI-powered solutions continues to rise, data scientists seek efficient ways to leverage pre-trained models or build custom models to address specific tasks. In this regard, Google&amp;rsquo;s VertexAI emerges as a robust platform that offers an extensive selection of pre-built models for a wide range of AI tasks. VertexAI platform has revolutionized the landscape by seamlessly leveraging LLM (Large Language Models) and Prompt Engineering techniques to perform complex machine learning tasks effortlessly. With VertexAI, data scientists can harness the power of state-of-the-art language models, such as LLM, to accelerate their ML development process. Additionally, the innovative concept of Prompt Engineering enables users to effectively communicate with the models, guiding them to deliver precise and accurate results. From computer vision and natural language processing to speech processing and structured tabular data analysis, Vertex AI&amp;rsquo;s repertoire includes over 100 models catering to diverse application domains. This article explores how Vertex AI, through its integration of LLM and Prompt Engineering, empowers users to effortlessly tackle intricate machine learning tasks across diverse domains, revolutionizing the AI development experience.&lt;/p></description></item><item><title>What is GAN?</title><link>https://dasarpai.github.io/dsblog/What-is-GAN/</link><pubDate>Tue, 17 Jan 2023 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/What-is-GAN/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6043-gan.jpg" alt="Partial Dependence Plots">&lt;/p>
&lt;h1 id="what-is-gan">What is GAN?&lt;a class="td-heading-self-link" href="#what-is-gan" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="what-is-gan-generative-adversarial-network">What is GAN (Generative Adversarial Network)?&lt;a class="td-heading-self-link" href="#what-is-gan-generative-adversarial-network" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Generative adversarial networks (GANs) are besing used to generate images, videos, text, audio and music. GAN is a class of machine-learning models introduced by Ian Goodfellow and his colleagues in 2014. The GANs became popular among researchers quickly because of their property to generate new data with the same statistics as the input training set. It can be applied to images, videos, textual data, tabular data and more, proving useful for semi-supervised, fully supervised, and reinforcement learning.&lt;/p></description></item><item><title>What Are Transformers in AI</title><link>https://dasarpai.github.io/dsblog/What-Are-Transformers-in-AI/</link><pubDate>Tue, 03 Aug 2021 00:00:00 +0000</pubDate><author>nuno@n9o.xyz (Blowfish)</author><guid>https://dasarpai.github.io/dsblog/What-Are-Transformers-in-AI/</guid><description>&lt;p>&lt;img src="../../assets/images/dspost/dsp6031-What-are-Transformers-in-AI.jpg" alt="What-are-Transformers-in-AI">&lt;/p>
&lt;h1 id="what-are-transformers-in-ai">What Are Transformers in AI&lt;a class="td-heading-self-link" href="#what-are-transformers-in-ai" aria-label="Heading self-link">&lt;/a>&lt;/h1>
&lt;h2 id="transformer-architecture">Transformer Architecture&lt;a class="td-heading-self-link" href="#transformer-architecture" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;img src="../../assets/images/dspost/transformer/transformer-arch.jpg" alt="Transformer">&lt;/p>
&lt;h2 id="background">Background&lt;a class="td-heading-self-link" href="#background" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Whether GPT, ChatGPT, DALL-E, Whisper, Satablity AI or whatever significant you see in the AI worlds nowdays it is because of Transformer Architecture. Transformers are a type of neural network architecture that have several properties that make them effective for modeling data with long-range dependencies. They generally feature a combination of multi-headed attention mechanisms, residual connections, layer normalization, feedforward connections, and positional embeddings.&lt;/p></description></item></channel></rss>